### General Terms

- 描述一个神经网络:
    - **参数量**: 模型中所有可训练参数的数量, 包括 weight 和 bias.
        - DeepSeek-R1 有 $671\text{ B}$ 可训练参数 ($6710$ 亿).
    - **Activation 激活值**: 神经网络各个层神经元上的数值 (注意与参数区分开!).
    - **Hyperparameter 超参数**: 模型训练前需要设置的参数, 如学习率、batch size、层数, etc.

- **ANN (Artificial Neural Network)**: 就是传统意义上的神经网络.

- **MSE (Mean Squared Error)**: 可用作 Loss function.

- **AI 幻觉**: AI 编造事实的现象.

- **DAG (Directed Acyclic Graph) 计算图**: 有向无环图, 用来可视化一次计算过程 (哪些数据先算, 后面的数据依赖哪些数据), 由张量和算子组成.

- **NAS (Neural Architecture Search)**: 神经网络架构搜索, 自动化地搜索神经网络的最佳架构 (而不是人工设计) @mellor2021neuralarchitecturesearchtraining.

- **BIC (Brain-inspired Computing) / NM (Neuromorphic) Computing**: 比如 SNN (Spiking Neural Network, 与 ANN 是同层概念).
    - **ANN2SNN methods**: 将 ANN 转换为 SNN 的方法 @deng2025edgeintelligencespikingneural.
    - **EdgeSNN**: "Edge Intelligence based on SNNs" @deng2025edgeintelligencespikingneural. 

- **Federated Learning**: 为了避免公司不愿分享数据、用户隐私泄漏等问题, 将训练数据交付 cloud 不再可能, 所以我们先在边缘设备上训练模型, 然后用某种方法将各个边缘设备上学到的 "知识" 汇总到 cloud 上, 这就叫 **FL**. [@deng2025edgeintelligencespikingneural; @somvanshi2025tinymachinelearningtiny]

![ANN vs SNN @deng2025edgeintelligencespikingneural](ann-vs-snn.png){#fig-ann-vs-snn}


- **FM (Foundation Model)**: 基础模型. 一般为了解决一个问题往往会训练一个专门的模型 (task-specific model). 但 FM 是一种通用的模型, 在大规模、广泛、多样的数据 (多模态的, 文字/图像/音频) 上进行预训练 (训练成本极高 @tcheyan_2025_decentralized); 然后可以通过微调 (fine-tuning) 来适应各种下游任务 (如文本生成、图像识别、语音识别等). 比如 Meta 的 Llama 模型.
    - **Edge-native foundation models**: 边缘化的 FM, 涉及到对大模型的 knowledge distillation, pruning, quantization 等技术, 以适应边缘设备的计算和存储限制 @somvanshi2025tinymachinelearningtiny.

- **Pretraining 预训练**: 用维基百科、书籍等未标注的大规模数据集对模型进行训练 (自监督的), 方法包括:
    - **Masked Language Modeling (MLM)**: 随机挖空, 然后预测挖空的词语.
    - **Next Sentence Prediction (NSP)**: 以句子为单位的 MLM.

- **Post-training/Fine-tuning 后训练/微调**: 在预训练模型的基础上, 用少量标注数据对模型进行训练, 以适应特定任务 (比如电影评论->情感标签).
    - **Full fine-tuning 全参数微调**: 会更新模型的**所有**参数. 性能好, 但成本极高 (700 亿参数的 LLM 需要 1TB 显存).
    - **Parameter-efficient fine-tuning (PEFT) 参数高效微调**: 主流, 冻结 $99\%$ 以上的模型参数, 只更新少量参数 (E.g., LoRA).
    - **Instruction tuning/Supervised fine-tuning 指令微调/有监督微调 (SFT)**: 用**指令-回答对**微调 @tcheyan_2025_decentralized, 如 "翻译成中文: How are you?" -> "你好吗?", 让模型会回答而只是文本补全.
    - **Alignment tuning/Reinforcement Learning with Human Feedback (RLHF) 对齐微调/基于人类反馈的强化学习**: SFT 微调后的模型不是最有帮助的/有害的/不符合价值观的, RLHF 不是喂给模型更多数据, 而是让模型给出多个回答, 然后让**人类**打分 (Reward), 用强化学习 (如 PPO) 进行微调来最大化这个 Reward @tcheyan_2025_decentralized.

- **VLA (Vision Language Action)**: 智能驾驶/机器人领域内的一种先进的多模态机器学习模型, 它结合了视觉、语言和动作三种能力, 旨在实现从感知输入直接映射到机器人控制动作的完整闭环能力.

- **VLM (Vision Language Model)**: 视觉语言模型, 可处理图片和自然语言两种模态进行理解和生成任务的模型.

- **ViT (Vision Transformer)**: 一种基于 Transformer 架构的计算机视觉模型.

- **AIGC (AI-Generated Content)**: 生成式 AI.

### CV 计算机视觉相关 {#sec-cv-terms}

- **Tensor**: 多维数组.
    - 名字来源: 由于数学里 $(0,k)$-tensor $T: V^k \to \mathbb{F}$ 在选取一组 basis $\{\mathbf{e}_i\}_{i=1}^k$ 后的 representation 刚好是一个 $k$ 维数组 $T_{i_1i_2 \cdots i_k}$ (正如 linear functional 可以被一个 covector 描述, bilinear functional 可以被一个 matrix 描述).
    - 机器学习里 **training data**, **kernel**, **feature map** 等都用 tensor 来描述, Python 自带的 `list` 和 `np.array()`, 统统都要转化为 `torch.tensor`:

        ```python
        import torch
        import numpy as np
        # python list to tensor
        data_list = [[1, 2, 3], [4, 5, 6]]
        list2tensor = torch.tensor(data_list)
        # numpy array to tensor
        data_array = np.array([[1, 2, 3], [4, 5, 6]])
        array2tensor = torch.from_numpy(data_array)
        ```
    - **Tensor Shape H, W, C(D), N**: Height, Width, Channel(Depth), Batch size, 含义见 @fig-data-layout. 描述有一定顺序, 默认有两种:
        - **Batch 在后**: <u>HWC</u>**N**.
        - **Batch 在前**: **N**<u>HWC</u>.


- **Data Layout Formats 张量在内存中的布局**
    - **NCHW**: `pytorch`
    - **NHWC**: `numpy`
    - TODO

    ![三种 Data Layout Formats @a2023_understanding.](data-layout.png){#fig-data-layout width=80%}

- **Feature Map 特征图**: CNN 中间层的一个 channel, 代表机器学习到的一种特征.
    - 注意不是 kernel! 
    - *Abuse of Terms:* 有时 Input 的每个 channel 也叫 feature map.

    ![每个 kernel (filter) 可以产生一张 Feature map @anello_2022_visualizing.](feature-map.png){#fig-feature-map width=70%}

- 一些图像处理工具:
    - **Pillow (`from PIL import Image`)**: 一般用来对静态图像做基础操作 (裁剪、缩放、旋转、颜色变换等).
    - **OpenCV (`import cv2`)**: 一般用来做视频流 (实时图像) 处理, 性能较高.
    - **FFmpeg (`import ffmpeg`)**: 开源的命令行工具和 Python 库, 用来做音视频格式转换, 压缩等等还有很多, yyds.

### Transformer 相关

- **Causal Self-attention**: 给 attention 加上 mask (softmax 之前的 score 给 $-\infty$), 使得每个 token 只能 attend 它前面的 token (包括它自己).

### Agent 相关

- **Agent Framework**:
    - **Agent 智能体**: 在 AI 模型和系统 Agent Tools (一些 API 比如 `read_files()`) 之间传话的程序.
    - User-agent 端: User 给出自然语言要求, Agent 理解并产生调用 API 的指令.
        - **User Prompt 用户提示词**: 你问的问题 (自然语言).
        - **System Prompt 系统提示词**: 系统给模型的隐藏指令, 一般用来改变 AI 的「人设」, 比如 "You are my girlfriend.".
            - **Function Calling**: 只是规范格式后的 System Prompt (比如用 `json`, 毕竟概率模型输出的指令可能不符合 API 要求).
    - Agent-API 端: Agent 调用 API (比如查找文件路径或者调用搜索引擎), 然后把结果返回给 Agent.
        - **MCP (Model Context Protocol)**: 在 Agent 和 Agent Tools (API) 之间的一层, 方便整理不同类型的 Agent Tools.

    ![Agent Framework.](agent-arch.png){#fig-agent-arch width=80%}


### Computing in ML

- **Training/Inference 训练/推理**: Training 是用数据集来更新模型参数的过程 (一般用 Backpropagation); Inference 是用训练好的模型来进行 Forward propagation 的过程 @tcheyan_2025_decentralized.
    - Inference 比 Training 计算量小很多 (但是对大模型来说依然很大).
    - 训练成本: 主要来自 GPU, 比如 NVIDIA H100/B200 costs \$30K per unit @tcheyan_2025_decentralized, OpenAI 计划 2025 年底部署 100w 台 GPU! Altman 说 GPT-4 训练成本一亿美元.
    - **Decentralized/Distributed Training 分布式训练**: Blockchain 的成功说明不同地方的算力可以 contribute to the same thing. 分布式训练目标是设计一个 blockchain-like system, 鼓励所有能联网的闲置设备贡献算力来训练大模型, 实现 $0$ GPU 成本. [@tcheyan_2025_decentralized; @dong2025singleaiclustersurvey].
        - [**Prime Intellect's "Protocol"**](https://www.primeintellect.ai/blog/protocol) @_2025_introducing.

        ![分布式训练: The Third Epoch of AI @_2025_chakra.](third-epoch.png){#fig-third-epoch}

- **数据类型**:
    - **FP32**: 32 位浮点数, 1 位符号位, 8 位指数位, 23 位尾数位, 精度高, 计算速度慢.
    - **BF16**: Brain Floating Point 16, 1 位符号位, 8 位指数位, 7 位尾数位, 精度低, 计算速度快.


- **FLOP (Floating-Point OPeration)**: 一次浮点运算包括一次加/减/乘/除.
    - **FLOPs (Floating-point Operations)**: 用来表达**计算量**.
    - **FLOPS (Floating-point Operations Per Second)**: 用来表达**算力**
        - **OPS (Operations Per Second)**: 一般是整数运算的算力单位.
        - 比如 NVIDIA A10 FP32: $31.2$ TF (TeraFLOPS), INT8 有 $250$ TOPS | 500 TOPS* (稀疏模式下).

- **MAC (Multiply‑ACcumulate)**: $a \leftarrow a + (b \times c)$ 这种运算, 神经网络中有大量的 MAC 运算. 1 MAC = 2 FLOPs.
    - `torchprofile` 库可以统计模型中的 FLOPs 和 MACs.

- **Spase DNN**: 稀疏神经网络, 指网络中有大量的权重为零 (即不参与计算) 的神经网络.
    - 根据稀疏的结构不同, 可分为三类 (见 @fig-sparsity). 其中 Semi-structured Sparsity 的意思是比如权重矩阵每 $4$ 个权重就有 $2$ 个权重为零 (记做 $2:4$ sparsity) @sabih2025hardwaresoftwarecodesignriscvextensions.

        ![用权重矩阵展示 (a) Structured (b) unstructured (c) semi-structured sparsity @sabih2025hardwaresoftwarecodesignriscvextensions, 蓝色的格子代表权重为 $0$](sparsity.png){#fig-sparsity}

    - **Pruning**: 剪枝, 将神经网络中不重要的权重 (如接近零的权重) 设置为零.
        - **Unstructured Pruning**: 非结构化剪枝, 移除个别权重, 硬件控制复杂度大.
        - **Structured Pruning**: 结构化剪枝, 移除整个通道/滤波器/神经元等 @sabih2025hardwaresoftwarecodesignriscvextensions, 更适合硬件加速. (比如 @fig-sparsity (a) 就可通过 pruning 移除输入的绿色神经元和输出的红色神经元).
        - **Semi-structured Pruning**: 半结构化剪枝, 介于上述两者之间.

- **Quantization 量化**: 跟模数转换一样, 将模拟 (高分辨率, 如 FP32) 的数据转换为低分辨率 (如 INT8) 的过程.
    - **量化方法** (注意下面的概念不是严格并列的, 比如 Adaptive 量化可以是 Uniform 也可以是 Non-uniform) @skki_2025_quantization:
        - **Uniform 量化**
        - **Non-uniform 量化**
        - **Weight Clustering 权重聚类量化**
        - **Integer-only 纯整数量化**: 将所有参数全部转化为整数 (而不是短的浮点如 DeepSeek 的 UE8M0 FP8), 如果有专门针对整数运算优化的硬件加速器就很适合.
        - **Hybrid 混合量化**
        - **Adaptive 自适应量化**
    - **量化范式** @skki_2025_quantization:
        - **PTQ (PQ, Post-Training Quantization) 训练后量化**: 先完全不管量化将神经网络训练完成 (比如在 FP32 下训练), 然后在将其参数量化为低精度格式 (比如 INT8), 后续也不再训练了. 常见方法:
            - **ADPQ (ADaptive PQ)**: 通过自适应 LASSO 回归识别敏感权重, 无需 Caliberation Dataset.
            - **GPTQ (GPT Quantization)**: 给 GPT 量化的, 基于 Hessian 矩阵优化量化误差来压缩模型.
            - **SmoothQuant**: 通过平滑激活值分布来减少量化误差.
        - **QAT (Quantization-Aware Training) 量化感知训练**: 在模型训练阶段就加入量化的训练让它学习到如何避免将来量化后精度损失 (比如前向传播时加入 fake quantization nodes 伪量化节点来模拟低精度的量化误差), 显然他肯定会比 PTQ 好, 但是训练成本很高. 常见方法:
            - **QLoRA**: 结合了 $4$-bit 量化和 LoRA 微调.
            - **AWQ (Activation-aware Weight Quantization)**: 关注低精度量化 (INT4/INT3) 分析激活值分布, 优先保护对模型输出影响较大的权重, 适合边缘设备.

- **GEMM (GEneral Matrix-Matrix Multiplication)**: 通用矩阵乘法.

- **CIM (Compute In Memory)**: 存内计算.

- **Tiling 分块/瓦片**: 在 CUDA 编程中, 由于 global memory 访问延迟高, 比如计算两个矩阵相加, 可以将上半和下半部分分别交给两个 block 里进行计算, 开辟两个 shared memory 来存储各自的半部分 (注意 shared memory 不在 block 间共享, 矩阵上下两半部分的相加刚好也是无依赖的! 如果是 GEMM 就不能这样分配!). shared memory 访问效率高.

### Optimizer in ML

- **Gradient Descent (GD) 梯度下降[^gd]**: 所有优化方法都从这个基础方法改进而来.
    - **Batch Gradient Descent (BGD) 全批量梯度下降**: 用整个训练集计算梯度并更新参数.
    - **Stochastic Gradient Descent (SGD) 随机梯度下降**: 每次用一个样本计算梯度并更新参数.
    - **Mini-batch Gradient Descent 小批量梯度下降**: 每次用一批样本 (mini-batch, 如 $32$ 个样本) 前向传播然后算这批样本的平均 Loss 来进行反向传播更新参数.
        - 前两个太极端了, BGD 计算量太大, SGD 不稳定, 基本不会用!
    - **Batch**: 每次用多少样本进行一次参数更新.
        - *Abuse of Terms:* 有时候卷积核的个数也称为 batch (这是因为 image batch 和 kernel 在内存中的布局方法可以一样, 但真是离大谱!)

[^gd]: *Abuse of Terms:* 现在说的 GD/SGD 就是指 Mini-batch 版本! 实际训练中不用 BGD (计算量太大) 和纯 SGD (不稳定)!

- **Optimizer 优化器**: 更新模型参数的算法.
    - **Momentum-based 动量优化器**: 引入历史项的加权平均 (等价于指数加权), 相当于给参数中的点赋予质量和惯性 (而不是没有质量), 不易受噪声影响, 可**加速**和**平滑**收敛、避免**陷入局部最优**[^momentum]. 也有一些基于次改良的版本:
        - **Nesterov Accelerated Gradient (NAG)**: 将未来的参数点的梯度也参与计算, 避免 overshoot. 开启这个功能无需设置额外的超参数, 以 `torch` 为例:

            ```python
            optimizer_nag = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)
            ```

    - **Adaptive Gradient (Adagrad) 自适应梯度优化器**: 有利于 Sparse Features 的学习 (通过给每个参数分配不同的学习率! 或者理解为对参数进行归一化):

    ![Adagrad 给梯度平缓方向对应的参数 ($Y$) 更大的学习率 ($\alpha_Y = 0.05$) 来加快这个方向的下降 @tg_2020_why. 也可以用 "Normalize" 的角度理解.](adagrad.png){#fig-adagrad}


[^momentum]: "SGD is a walking man downhill, slowly but steady. Momentum is a heavy ball running downhill, smooth and fast." @maciejbalawejder_2022_optimizers


### ML Frameworks

- **Tensorflow, JAX, PyTorch**: 机器学习框架 @fig-mlsys. 其实就是一些 Python 库.

    ```python
    import tensorflow as tf
    import torch
    import jax.numpy as jnp
    ```
    - Pytorch 是最主流的, Tensorflow 快死了好像.
    - 如何看待公司面试要求手写 transformer? 首先会背 Pytorch 函数并不 cool, Pytorch 只是复刻了主流论文里的框架, 如果你能写一个架构并且顶替 transformer, 你的库函数不久后也会出现在 Pytorch 里.
    - RISCV 上有 TfLM (TensorFlow Lite for Microcontrollers), 
    - **ONNX (Open Neural Network Exchange)**: 一种统一的描述神经网络结构的格式, 以上三种框架都支持导出为 ONNX 格式.

- **TVM, XLA (Accelerated Linear Algebra)**: 机器学习编译器 @fig-mlsys, 在以上三个框架内都有 python 的接口函数.
    - **OpenXLA**: 中间表示 **StableHLO (Stable High-Level Optimizer)**, **XLA**, **PJRT** 的实现工程.

![AI 编译栈和编程体系](mlsys.png){#fig-mlsys width=80%}

### Benchmarks in ML

- **AUC (Area Under Curve)**: 二分类模型的性能评估指标, 越大越好.

- **F-score**: 二分类 (正类、负类) 模型的性能评估指标.
    - **TP (True Positive)**: 正类被正确分类为正类.
    - **FP (False Positive)**: 负类被错误分类为正类.
    - **FN (False Negative)**: 正类被错误分类为负类.
    - **Recall 召回率**: $TP/TP+FP$
    - **Precision 精确率**: $TP/TP+FN$
    - **F1-score**: Recall 和 Precision 的调和平均 (F-$\beta$ score 的特例)
    - **F-$\beta$ score**: 仅仅是给 recall 加了权重.

        ![](F-score.png)

- **A/B Test**: 类似双盲实验, 比如研究修改按钮颜色能否提升点击率? 新模型是否真的比旧模型好? 可以用这种方法进行对比实验.

- **QE (Quantization Error) 量化误差**
    - **AQE (Average Quantization Error) 平均量化误差**
    - **MQE (Maximum Quantization Error) 最大量化误差**
    - **OQE (Output Quantization Error) 输出层量化误差**

### Related Philosophy

- **Symbol Grounded Problem**: 符号嵌入 (接地) 问题. 探讨的是符号 (或词语) 是如何在一个系统中获得意义的. 比如 "猫" 是一个符号, 但它不仅仅是一个符号, 它还与其它符号有所关联 (Grounded "嵌入"), 这种关联是 "猫" 的意义. 关于符号是如何嵌入的, 有以下几种观点:
    - **具身认知**: 意义必须通过一种感官、具身的方式与世界互动, 才能真正理解并嵌入符号. @The_Human_Developers_2025_cnblogs
    - **联结主义**: 符号的意义只取决于它与其它符号的关系 (有点范畴论的感觉哈哈). 符号可以通过网络中激活模式来进行嵌入. 这些模型并不明确地定义符号的意义, 而是通过训练大量数据, 学习感官输入与概念之间的关联. @The_Human_Developers_2025_cnblogs

- **Inductive Bias (归纳偏见)**: 人类通过先验知识和经验 (归纳) 来引导学习过程, 这个过程体现在: 给神经网络设计特定的结构、将自由度更大的模型的某些参数置 $0$ (将函数空间/参数空间减小, 比如 CNN 可以看作 FCNN 的子集 @fig-cnn-fcnn), etc. 这样可以, 但是可能人为地丢弃了结构很好的函数空间 (bias) @battaglia2018relationalinductivebiasesdeep.
    - Weak Inductive Bias 的网络 (比如 ViT, compared with CNN) 需要的训练数据更多 (data-hungry).
    - Advantage: 如果归纳地合适, 可以让模型更快地收敛到合理的解. 比如 @fig-different-ib 中我们用 $5$ 个参数的神经网络拟合两个**斜抛运动**的数据点. 如果事先告诉模型这是一个斜抛运动 (by setting weights $w_1=w_2=w_5=0$), 而不是 exponential decay, 模型很容易就能拟合出很准确的结果 (棕色).
    - Disadvantage: 如果归纳地不合适, 比如 @fig-different-ib 的红色和紫色, 显然离斜抛运动差得远.
        - 再拿 CNN 举例, 由于 CNN 在**几乎每一层**使用卷积核, 它隐含的 bias 就是: 一张图片的**语义可以由局部的信息层次化拼凑出来**、图片**元素的位置不影响图片的语义 (平移不变性)**. 这样训练出来的模型对于手写数字识别非常准确和高效, 但是会先入为主地重视局部纹理而不是全局特征 (@fig-colored-cat 是一个很经典的例子, 说明 ViT 很好地解决了 CNN 很难学习到全局信息的问题).
        - 但 ViT 也有自己的 bias: 比如**切分成 patch 的时候引入了局部性**、**所有 patch 用同一个线性映射层 (shared embedding matrix) 也引入了平移不变性**. 但是这只是在第一步编码时引入的, 后面位置编码和 transformer 就能很好地学习到全局信息了.

::: {.column-margin}
![Inductive Bias Demo. 一张彩色猫, ResNet 将其分类为 “金刚鹦鹉”, 而 ViT 分类为 “埃及猫” @juliaturc_2025_why.](colored-cat.png){#fig-colored-cat}
:::

::: {layout = "[50,50]"}
![CNN 是有归纳偏见的 FCNN (原本 $64$ 个参数减小到了 $4$ 个参数).](cnn-fcnn.png){#fig-cnn-fcnn}

![不同的 Inductive Bias 带来完全不同的拟合效果 @gerstnerlab_2023_rl36.](different-ib.png){#fig-different-ib}
:::

@misc{a2021_deep,
  title = {Deep Learning Architecture},
  url = {https://gitee.com/hitsz-cslab/dla#https://gitee.com/hitsz-cslab/dla/tree/master/stupkt},
  urldate = {2025-07-15},
  year = {2021},
  organization = {Gitee.com}
}

@ARTICLE{2018arXiv180503648K,
  author = {{Kastner}, R. and {Matai}, J. and {Neuendorffer}, S.},
  title = "{Parallel Programming for FPGAs}",
  journal = {ArXiv e-prints},
  archivePrefix = "arXiv",
  eprint = {1805.03648},
  keywords = {Computer Science - Hardware Architecture},
  year = 2018,
  month = may
}

@misc{redmon2016lookonceunifiedrealtime,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.02640}, 
}


@article{Er_2025_bilibili,
	author	={上下求索电子Er},
	title	={[YOLO V1] 数据标注和输出张量_哔哩哔哩_bilibili},
	year	={2025},
	url	={https://www.bilibili.com/video/BV1gKwAeWEo4/?spm_id_from=333.788.player.switch&vd_source=42579e22289b6144ba0b2bdcf99834e3&p=3}
}

@article{Hack_2025_CSDN,
	author	={Hack 电子},
	title	={哈工大深度学习体系结构课程 | 实验2：YOLO算法量化加速-CSDN博客},
	year	={2025},
	url	={https://blog.csdn.net/HackEle/article/details/135944887}
}

@book{patt_2020_introduction,
  author = {Patt, Yale N and Patel, Sanjay J},
  publisher = {Mcgraw-Hill},
  title = {Introduction to Computing Systems : from Bits and Gates to c/c++ & beyond},
  year = {2020}
}

@article{Slchoi_2025_velog,
	author	={Slchoi},
	title	={Multi-Cycle Implementation & Pipelining},
	year	={2025},
	url	={https://velog.io/@taegon1998/4.2-4.3-Multi-Cycle-Implementation-Pipelining}
}

@book{murphy_2012_machine,
  author = {Murphy, Kevin P},
  publisher = {Mit Press},
  title = {Machine Learning : a Probabilistic Perspective},
  year = {2012}
}

// The fantastic animation
@article{The_Cpu_Is_2025_youtube,
	author	={QBayLogic},
	title	={CPU vs FPGA explained in a short animation - YouTube},
	year	={2025},
	url	={https://www.youtube.com/watch?v=BML1YHZpx2o}
}

@article{Simulation_Time._Putting_2025_oscc,
	author	={YSYX},
	title	={E5 从 RTL 代码到可流片版图 | 官方文档},
	year	={2025},
	url	={https://ysyx.oscc.cc/docs/2407/e/5.html}
}

@article{Lazyparser_2025_bilibili,
	author	={Lazyparser},
	title	={Compilation and Linker},
	year	={2025},
	url	={https://www.bilibili.com/video/BV1Q5411w7z5?spm_id_from=333.788.videopod.episodes&vd_source=42579e22289b6144ba0b2bdcf99834e3&p=5}
}

@article{Voice_2025_youtube,
	author	={MIT HAN Lab},
	title	={Lecture 2 - Basics of Neural Networks (MIT 6.5940, Fall 2023)},
	year	={2025},
	url	={https://www.youtube.com/watch?v=ieg0RJb7TeI&list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB&index=4}
}

@article{emory,
	author	={Shun Yan Cheung},
	title	={Lecture notes on Computer Science courses: Computer Architecture (CS355)},
	year	={2025},
	url	={https://www.cs.emory.edu/~cheung/Courses/355/Syllabus/syl.html#CURRENT}
}

@article{li_2023_design,
  author = {Li, Zonghao and Carusone, Anthony Chan},
  month = {10},
  pages = {01-09},
  title = {Design and Optimization of Low-Dropout Voltage Regulator Using Relational Graph Neural Network and Reinforcement Learning in Open-Source SKY130 Process},
  doi = {10.1109/iccad57390.2023.10323720},
  year = {2023},
  journal = {2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}
}
@misc{Intel_2025_intel,
	author = {Intel},
	title = {What Is Edge Computing? – Intel},
	url = {https://www.intel.com/content/www/us/en/learn/what-is-edge-computing.html},
	year = {2025},
	organization = {Intel}
}

@misc{Asking_Questions_2025_youtube,
	author = {Systems Group at ETH Zürich},
	month = {11},
	title = {Introduction to FPGAs and ML Inference with hls4ml (Benjamin Ramhorst, 8 November 2024)},
	url = {https://www.youtube.com/watch?v=2y3GNY4tf7A},
	urldate = {2025-11-04},
	year = {2024},
	organization = {YouTube}
}

@article{Duarte:2018ite,
    author = "Duarte, Javier and others",
    title = "{Fast inference of deep neural networks in FPGAs for particle physics}",
    eprint = "1804.06913",
    archivePrefix = "arXiv",
    primaryClass = "physics.ins-det",
    reportNumber = "FERMILAB-PUB-18-089-E",
    doi = "10.1088/1748-0221/13/07/P07027",
    journal = "JINST",
    volume = "13",
    number = "07",
    pages = "P07027",
    year = "2018"
}

@misc{Fusion_Energy_Sciences_2025_energy,
  author = {US Department of Energy},
  title = {AI Tackles Disruptive Tearing Instability in Fusion Plasma},
  url = {https://www.energy.gov/science/fes/articles/ai-tackles-disruptive-tearing-instability-fusion-plasma},
  year = {2025},
  organization = {Energy.gov}
}

@misc{sali2025realtimefpgabased,
      title={Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations}, 
      author={Safa Mohammed Sali and Mahmoud Meribout and Ashiyana Abdul Majeed},
      year={2025},
      eprint={2509.04153},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2509.04153}, 
}


@misc{sabih2025hardwaresoftwarecodesignriscvextensions,
    title={Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs}, 
    author={Muhammad Sabih and Abrarul Karim and Jakob Wittmann and Frank Hannig and Jürgen Teich},
    year={2025},
    eprint={2504.19659},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2504.19659}, 
}

@misc{mellor2021neuralarchitecturesearchtraining,
	title={Neural Architecture Search without Training}, 
	author={Joseph Mellor and Jack Turner and Amos Storkey and Elliot J. Crowley},
	year={2021},
	eprint={2006.04647},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2006.04647}, 
}

@misc{somvanshi2025tinymachinelearningtiny,
	title={From Tiny Machine Learning to Tiny Deep Learning: A Survey}, 
	author={Shriyank Somvanshi and Md Monzurul Islam and Gaurab Chhetri and Rohit Chakraborty and Mahmuda Sultana Mimi and Sawgat Ahmed Shuvo and Kazi Sifatul Islam and Syed Aaqib Javed and Sharif Ahmed Rafat and Anandi Dutta and Subasish Das},
	year={2025},
	eprint={2506.18927},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2506.18927}, 
}

@misc{jung2024optimizingdeploymenttinytransformers,
	title={Optimizing the Deployment of Tiny Transformers on Low-Power MCUs}, 
	author={Victor J. B. Jung and Alessio Burrello and Moritz Scherer and Francesco Conti and Luca Benini},
	year={2024},
	eprint={2404.02945},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2404.02945}, 
}

@article{Muir2025,
	author    = {Dylan Richard Muir and Sadique Sheik},
	title     = {The road to commercial success for neuromorphic technologies},
	journal   = {Nature Communications},
	year      = {2025},
	volume    = {16},
	number    = {1},
	pages     = {3586},
	doi       = {10.1038/s41467-025-57352-1},
	url       = {https://doi.org/10.1038/s41467-025-57352-1},
	abstract  = {Neuromorphic technologies adapt biological neural principles to synthesise high-efficiency computational devices, characterised by continuous real-time operation and sparse event-based communication. After several false starts, a confluence of advances now promises widespread commercial adoption. Gradient-based training of deep spiking neural networks is now an off-the-shelf technique for building general-purpose Neuromorphic applications, with open-source tools underwritten by theoretical results. Analog and mixed-signal Neuromorphic circuit designs are being replaced by digital equivalents in newer devices, simplifying application deployment while maintaining computational benefits. Designs for in-memory computing are also approaching commercial maturity. Solving two key problems—how to program general Neuromorphic applications; and how to deploy them at scale—clears the way to commercial success of Neuromorphic processors. Ultra-low-power Neuromorphic technology will find a home in battery-powered systems, local compute for internet-of-things devices, and consumer wearables. Inspiration from uptake of tensor processors and GPUs can help the field overcome remaining hurdles.},
	issn      = {2041-1723}
}

@misc{deng2025edgeintelligencespikingneural,
	title={Edge Intelligence with Spiking Neural Networks}, 
	author={Shuiguang Deng and Di Yu and Changze Lv and Xin Du and Linshan Jiang and Xiaofan Zhao and Wentao Tong and Xiaoqing Zheng and Weijia Fang and Peng Zhao and Gang Pan and Schahram Dustdar and Albert Y. Zomaya},
	year={2025},
	eprint={2507.14069},
	archivePrefix={arXiv},
	primaryClass={cs.DC},
	url={https://arxiv.org/abs/2507.14069}, 
}

@article{Jin2025,
	author    = {Miao Jin and Xiaohong Wang and Ce Guo and Shufan Yang},
	title     = {Research on target detection for autonomous driving based on ECS-spiking neural networks},
	journal   = {Scientific Reports},
	year      = {2025},
	volume    = {15},
	number    = {1},
	pages     = {13725},
	doi       = {10.1038/s41598-025-97913-4},
	url       = {https://doi.org/10.1038/s41598-025-97913-4},
	abstract  = {In response to the increasing demands for improved model performance and reduced energy consumption in object detection tasks relevant to autonomous driving, this research presents an advanced YOLO model, designated as ECSLIF-YOLO, which is based on the Leaky Integrate-and-Fire with Extracellular Space (ECS-LIF) framework. The primary aim of this model is to tackle the issues associated with the high energy consumption of traditional artificial neural networks (ANNs) and the suboptimal performance of existing spiking neural networks (SNNs). Empirical findings demonstrate that ECSLIF-YOLO achieves a peak mean Average Precision (mAP) of 0.917 on the BDD100K and KITTI datasets, thereby aligning with the accuracy levels of conventional ANNs while exceeding the performance of current direct-training SNN approaches without incurring additional energy costs. These findings suggest that ECSLIF-YOLO is particularly well-suited to assist the development of efficient and reliable systems for autonomous driving.},
	issn      = {2045-2322}
}

@misc{ashraf2025spact18spikinghumanaction,
	title={SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities}, 
	author={Yasser Ashraf and Ahmed Sharshar and Velibor Bojkovic and Bin Gu},
	year={2025},
	eprint={2507.16151},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2507.16151}, 
}

@misc{kim2019spikingyolospikingneuralnetwork,
	title={Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection}, 
	author={Seijoon Kim and Seongsik Park and Byunggook Na and Sungroh Yoon},
	year={2019},
	eprint={1903.06530},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1903.06530}, 
}

@misc{su2023deepdirectlytrainedspikingneural,
	title={Deep Directly-Trained Spiking Neural Networks for Object Detection}, 
	author={Qiaoyi Su and Yuhong Chou and Yifan Hu and Jianing Li and Shijie Mei and Ziyang Zhang and Guoqi Li},
	year={2023},
	eprint={2307.11411},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2307.11411}, 
}

@misc{lv2023detrs,
	title={DETRs Beat YOLOs on Real-time Object Detection},
	author={Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},
	year={2023},
	eprint={2304.08069},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{zou2025spikevideoformerefficientspikedrivenvideo,
	title={SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity}, 
	author={Shihao Zou and Qingfeng Li and Wei Ji and Jingjing Li and Yongkui Yang and Guoqi Li and Chao Dong},
	year={2025},
	eprint={2505.10352},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2505.10352}, 
}

@article{The_Human_Developers_2025_cnblogs,
	author	={Sida Dai},
	title	={Symbol Grounding Problem in Artificial Intelligence},
	year	={2025},
	url	={https://www.cnblogs.com/sddai/p/18670953}
}

@misc{mumuni2025largelanguagemodelsartificial,
	title={Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches}, 
	author={Alhassan Mumuni and Fuseini Mumuni},
	year={2025},
	eprint={2501.03151},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2501.03151}, 
}

@misc{krakauer2025largelanguagemodelsemergence,
	title={Large Language Models and Emergence: A Complex Systems Perspective}, 
	author={David C. Krakauer and John W. Krakauer and Melanie Mitchell},
	year={2025},
	eprint={2506.11135},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2506.11135}, 
}

@article{Aryaroop_Majumder_2025_medium,
	author	={Aryaroop Majumder},
	title	={Debunking the LLM-to-AGI Misconception: Why Current Large Language Models (LLMs) Cannot Achieve Artificial General Intelligence (AGI)},
	year	={2025},
	url	={https://medium.com/@aryaroop04/debunking-the-llm-to-agi-misconception-why-current-large-language-models-llms-cannot-achieve-9e6202d3ae5a}
}

@misc{mccrum2024conwaysgamelifeanalogue,
	title={Conways game of life as an analogue to a habitable world Livingness beyond the biological}, 
	author={James McCrum and Terence P Kee},
	year={2024},
	eprint={2410.22389},
	archivePrefix={arXiv},
	primaryClass={nlin.CG},
	url={https://arxiv.org/abs/2410.22389}, 
}

@misc{Continuing_To_Use_2025_popularmechanics,
	author = {Orf, Darren},
	month = {07},
	title = {Quantum Entanglement in Your Brain Is What Generates Consciousness, Radical Study Suggests},
	url = {https://www.popularmechanics.com/science/a65368553/quantum-entanglement-in-brain-consciousness/},
	year = {2025},
	organization = {Popular Mechanics}
}

@misc{silva2025leveragingquantumsuperpositioninfer,
	title={Leveraging Quantum Superposition to Infer the Dynamic Behavior of a Spatial-Temporal Neural Network Signaling Model}, 
	author={Gabriel A. Silva},
	year={2025},
	eprint={2403.18963},
	archivePrefix={arXiv},
	primaryClass={quant-ph},
	url={https://arxiv.org/abs/2403.18963}, 
}

@article{Fu2024,
	author    = {Tingzhao Fu and Jianfa Zhang and Run Sun and Yuyao Huang and Wei Xu and Sigang Yang and Zhihong Zhu and Hongwei Chen},
	title     = {Optical neural networks: progress and challenges},
	journal   = {Light: Science \& Applications},
	year      = {2024},
	volume    = {13},
	number    = {1},
	pages     = {263},
	doi       = {10.1038/s41377-024-01590-3},
	url       = {https://doi.org/10.1038/s41377-024-01590-3},
	abstract  = {Artificial intelligence has prevailed in all trades and professions due to the assistance of big data resources, advanced algorithms, and high-performance electronic hardware. However, conventional computing hardware is inefficient at implementing complex tasks, in large part because the memory and processor in its computing architecture are separated, performing insufficiently in computing speed and energy consumption. In recent years, optical neural networks (ONNs) have made a range of research progress in optical computing due to advantages such as sub-nanosecond latency, low heat dissipation, and high parallelism. ONNs are in prospect to provide support regarding computing speed and energy consumption for the further development of artificial intelligence with a novel computing paradigm. Herein, we first introduce the design method and principle of ONNs based on various optical elements. Then, we successively review the non-integrated ONNs consisting of volume optical components and the integrated ONNs composed of on-chip components. Finally, we summarize and discuss the computational density, nonlinearity, scalability, and practical applications of ONNs, and comment on the challenges and perspectives of the ONNs in the future development trends.},
	issn      = {2047-7538}
}

@article{OI2023,
	author = {Smirnova, Lena and Caffo, Brian and Gracias, David and Huang, Qi and Morales Pantoja, Itzy Erin and Tang, Bohao and Berlinicke, Cynthia and Boyd, J. and Harris, Timothy and Johnson, Erik and Kagan, Brett and Kahn, Jeffrey and Muotri, Alysson and Paulhamus, Barton and Schwamborn, Jens and Plotkin, Jesse and Szalay, Alexander and Vogelstein, Joshua and Hartung, Thomas},
	year = {2023},
	month = {02},
	pages = {1017235},
	title = {Organoid intelligence (OI): the new frontier in biocomputing and intelligence-in-a-dish},
	volume = {1},
	journal = {Frontiers in Science},
	doi = {10.3389/fsci.2023.1017235}
}

@misc{Timothy_Oh_2025_medium,
	author = {Oh, Timothy},
	month = {07},
	title = {The Future of Artificial Intelligence — Wetware},
	url = {https://medium.com/@timothyohoc/the-future-of-artificial-intelligence-wetware-da2e396b3c55},
	urldate = {2025-11-04},
	year = {2023},
	organization = {Medium}
}

@article{Carpegna_2025,
	title={Spiker+: A Framework for the Generation of Efficient Spiking Neural Networks FPGA Accelerators for Inference at the Edge},
	volume={13},
	ISSN={2376-4562},
	url={http://dx.doi.org/10.1109/TETC.2024.3511676},
	DOI={10.1109/tetc.2024.3511676},
	number={3},
	journal={IEEE Transactions on Emerging Topics in Computing},
	publisher={Institute of Electrical and Electronics Engineers (IEEE)},
	author={Carpegna, Alessio and Savino, Alessandro and Carlo, Stefano Di},
	year={2025},
	month=jul, pages={784–798} 
}

@ARTICLE{10636118,
	author={Li, Guoqi and Deng, Lei and Tang, Huajin and Pan, Gang and Tian, Yonghong and Roy, Kaushik and Maass, Wolfgang},
	journal={Proceedings of the IEEE}, 
	title={Brain-Inspired Computing: A Systematic Survey and Future Trends}, 
	year={2024},
	volume={112},
	number={6},
	pages={544-584},
	keywords={Artificial intelligence;Computational modeling;Brain modeling;Biological system modeling;Surveys;Computer architecture;Benchmark testing;Bio-inspired computing;Neuromorphic engineering;Sensors;Neural networks;Software tools;Benchmark datasets;brain-inspired computing (BIC);computing architecture;neuromorphic chips;neuromorphic sensors;software tool;spiking neural networks (SNNs)},
	doi={10.1109/JPROC.2024.3429360}
}

@misc{luo2025integervaluedtrainingspikedriveninference,
	title={Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection}, 
	author={Xinhao Luo and Man Yao and Yuhong Chou and Bo Xu and Guoqi Li},
	year={2025},
	eprint={2407.20708},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2407.20708}, 
}

@article{Zheng2024,
	author    = {Hanle Zheng and Zhong Zheng and Rui Hu and Bo Xiao and Yujie Wu and Fangwen Yu and Xue Liu and Guoqi Li and Lei Deng},
	title     = {Temporal dendritic heterogeneity incorporated with spiking neural networks for learning multi-timescale dynamics},
	journal   = {Nature Communications},
	year      = {2024},
	volume    = {15},
	number    = {1},
	pages     = {277},
	doi       = {10.1038/s41467-023-44614-z},
	url       = {https://doi.org/10.1038/s41467-023-44614-z},
	abstract  = {It is widely believed the brain-inspired spiking neural networks have the capability of processing temporal information owing to their dynamic attributes. However, how to understand what kind of mechanisms contributing to the learning ability and exploit the rich dynamic properties of spiking neural networks to satisfactorily solve complex temporal computing tasks in practice still remains to be explored. In this article, we identify the importance of capturing the multi-timescale components, based on which a multi-compartment spiking neural model with temporal dendritic heterogeneity, is proposed. The model enables multi-timescale dynamics by automatically learning heterogeneous timing factors on different dendritic branches. Two breakthroughs are made through extensive experiments: the working mechanism of the proposed model is revealed via an elaborated temporal spiking XOR problem to analyze the temporal feature integration at different levels; comprehensive performance benefits of the model over ordinary spiking neural networks are achieved on several temporal computing benchmarks for speech recognition, visual recognition, electroencephalogram signal recognition, and robot place recognition, which shows the best-reported accuracy and model compactness, promising robustness and generalization, and high execution efficiency on neuromorphic hardware. This work moves neuromorphic computing a significant step toward real-world applications by appropriately exploiting biological observations.},
	issn      = {2041-1723}
}

@article{He2024,
	author    = {Linxuan He and Yunhui Xu and Weihua He and Yihan Lin and Yang Tian and Yujie Wu and Wenhui Wang and Ziyang Zhang and Junwei Han and Yonghong Tian and Bo Xu and Guoqi Li},
	title     = {Network model with internal complexity bridges artificial intelligence and neuroscience},
	journal   = {Nature Computational Science},
	year      = {2024},
	volume    = {4},
	number    = {8},
	pages     = {584--599},
	doi       = {10.1038/s43588-024-00674-9},
	url       = {https://doi.org/10.1038/s43588-024-00674-9},
	abstract  = {Artificial intelligence (AI) researchers currently believe that the main approach to building more general model problems is the big AI model, where existing neural networks are becoming deeper, larger and wider. We term this the big model with external complexity approach. In this work we argue that there is another approach called small model with internal complexity, which can be used to find a suitable path of incorporating rich properties into neurons to construct larger and more efficient AI models. We uncover that one has to increase the scale of the network externally to stimulate the same dynamical properties. To illustrate this, we build a Hodgkin–Huxley (HH) network with rich internal complexity, where each neuron is an HH model, and prove that the dynamical properties and performance of the HH network can be equivalent to a bigger leaky integrate-and-fire (LIF) network, where each neuron is a LIF neuron with simple internal complexity.},
	issn      = {2662-8457}
}

@inproceedings{Prakash_2023,
	title={CFU Playground: Full-Stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs},
	url={http://dx.doi.org/10.1109/ISPASS57527.2023.00024},
	DOI={10.1109/ispass57527.2023.00024},
	booktitle={2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
	publisher={IEEE},
	author={Prakash, Shvetank and Callahan, Tim and Bushagour, Joseph and Banbury, Colby and Green, Alan V. and Warden, Pete and Ansell, Tim and Reddi, Vijay Janapa},
	year={2023},
	month=apr, pages={157–167} 
}

@article{Hirose_2020,
	author = {Hirose, Takayuki and Sawaragi, Tetsuo},
	year = {2020},
	month = {03},
	pages = {104556},
	title = {Extended FRAM model based on cellular automaton to clarify complexity of socio-technical systems and improve their safety},
	volume = {123},
	journal = {Safety Science},
	doi = {10.1016/j.ssci.2019.104556}
}

@misc{Datawrapper_2025_datawrapper,
	author = {Lokhov, Ivan},
	month = {06},
	title = {Game of Life | Datawrapper Blog},
	url = {https://www.datawrapper.de/blog/game-of-life},
	year = {2021},
	organization = {Datawrapper}
}

@misc{wikipediacontributors_2023_gun,
	author = {Wikipedia Contributors},
	month = {08},
	publisher = {Wikimedia Foundation},
	title = {Gun (cellular automaton)},
	url = {https://en.wikipedia.org/wiki/Gun_(cellular_automaton)},
	year = {2023},
	organization = {Wikipedia}
}

@misc{a2025_chapter,
	publisher = {Artificial Intelligence Index Report 2025},
	title = {CHAPTER 1: Research and Development},
	url = {https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter1_final.pdf},
	urldate = {2025-11-03},
	year = {2025}
}

@misc{yan2024surveyfpgabasedacceleratorml,
	title={A survey on FPGA-based accelerator for ML models}, 
	author={Feng Yan and Andreas Koch and Oliver Sinnen},
	year={2024},
	eprint={2412.15666},
	archivePrefix={arXiv},
	primaryClass={cs.AR},
	url={https://arxiv.org/abs/2412.15666}, 
}

@article{Pedersen2024,
	author    = {Jens E. Pedersen and Steven Abreu and Matthias Jobst and Gregor Lenz and Vittorio Fra and Felix Christian Bauer and Dylan Richard Muir and Peng Zhou and Bernhard Vogginger and Kade Heckel and Gianvito Urgese and Sadasivan Shankar and Terrence C. Stewart and Sadique Sheik and Jason K. Eshraghian},
	title     = {Neuromorphic Intermediate Representation: A Unified Instruction Set for Interoperable Brain-Inspired Computing},
	journal   = {Nature Communications},
	year      = {2024},
	volume    = {15},
	number    = {1},
	pages     = {8122},
	doi       = {10.1038/s41467-024-52259-9},
	url       = {https://doi.org/10.1038/s41467-024-52259-9},
	abstract  = {Spiking neural networks and neuromorphic hardware platforms that simulate neuronal dynamics are getting wide attention and are being applied to many relevant problems using Machine Learning. Despite a well-established mathematical foundation for neural dynamics, there exists numerous software and hardware solutions and stacks whose variability makes it difficult to reproduce findings. Here, we establish a common reference frame for computations in digital neuromorphic systems, titled Neuromorphic Intermediate Representation (NIR). NIR defines a set of computational and composable model primitives as hybrid systems combining continuous-time dynamics and discrete events. By abstracting away assumptions around discretization and hardware constraints, NIR faithfully captures the computational model, while bridging differences between the evaluated implementation and the underlying mathematical formalism. NIR supports an unprecedented number of neuromorphic systems, which we demonstrate by reproducing three spiking neural network models of different complexity across 7 neuromorphic simulators and 4 digital hardware platforms. NIR decouples the development of neuromorphic hardware and software, enabling interoperability between platforms and improving accessibility to multiple neuromorphic technologies. We believe that NIR is a key next step in brain-inspired hardware-software co-evolution, enabling research towards the implementation of energy efficient computational principles of nervous systems. NIR is available at neuroir.org.},
	issn      = {2041-1723}
}

@misc{berti2025emergentabilitieslargelanguage,
	title={Emergent Abilities in Large Language Models: A Survey}, 
	author={Leonardo Berti and Flavio Giorgi and Gjergji Kasneci},
	year={2025},
	eprint={2503.05788},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2503.05788}, 
}

@misc{filho_2024_what,
	author = {Filho, Roberto Piacentini},
	month = {08},
	title = {What Is a Chiplet, and Why Should You Care?},
	url = {https://www.keysight.com/blogs/en/tech/sim-des/what-is-a-chiplet-and-why-should-you-care},
	urldate = {2025-11-16},
	year = {2024},
	organization = {Keysight.com}
}

@misc{zhao2024analyticalheterogeneousdietodie3d,
	title={Analytical Heterogeneous Die-to-Die 3D Placement with Macros}, 
	author={Yuxuan Zhao and Peiyu Liao and Siting Liu and Jiaxi Jiang and Yibo Lin and Bei Yu},
	year={2024},
	eprint={2403.09070},
	archivePrefix={arXiv},
	primaryClass={cs.AR},
	url={https://arxiv.org/abs/2403.09070}, 
}

@INPROCEEDINGS{10454441,
	author={Smith, Alan and Chapman, Eric and Patel, Chintan and Swaminathan, Raja and Wuu, John and Huang, Tyrone and Jung, Wonjun and Kaganov, Alexander and McIntyre, Hugh and Mangaser, Ramon},
	booktitle={2024 IEEE International Solid-State Circuits Conference (ISSCC)}, 
	title={11.1 AMD InstinctTM MI300 Series Modular Chiplet Package – HPC and AI Accelerator for Exa-Class Systems}, 
	year={2024},
	volume={67},
	number={},
	pages={490-492},
	keywords={Data centers;Graphics processing units;AI accelerators;Bandwidth;Packaging;Silicon;Central Processing Unit},
	doi={10.1109/ISSCC49657.2024.10454441}
}

@article{liao_2023_dreamplace,
	author = {Liao, Peiyu and Guo, Dawei and Guo, Zizheng and Liu, Siting and Lin, Yibo and Yu, Bei},
	month = {01},
	pages = {3374-3387},
	publisher = {Institute of Electrical and Electronics Engineers},
	title = {DREAMPlace 4.0: Timing-Driven Placement with Momentum-Based Net Weighting and Lagrangian-Based Refinement},
	doi = {10.1109/tcad.2023.3240132},
	urldate = {2025-11-22},
	volume = {42},
	year = {2023},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}
}

@misc{brancheducation_2024_how,
	author = {Branch Education},
	month = {05},
	title = {How Are Microchips Made? CPU Manufacturing Process Steps},
	url = {https://www.youtube.com/watch?v=dX9CGRZwD-w},
	urldate = {2024-06-14},
	year = {2024},
	organization = {YouTube}
}

@INPROCEEDINGS{10323747,
	author={Hu, Kai-Shun and Chi, Hao-Yu and Lin, I-Jye and Wu, Yi-Hsuan and Chen, Wei-Hsu and Hsieh, Yi-Ting},
	booktitle={2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)}, 
	title={Invited Paper: 2023 ICCAD CAD Contest Problem B: 3D Placement with Macros}, 
	year={2023},
	volume={},
	number={},
	pages={1-6},
	keywords={Measurement;Three-dimensional displays;Design automation;Runtime;Microprocessors;Computer architecture;Benchmark testing;Physical design;placement;3DIC;chiplet},
	doi={10.1109/ICCAD57390.2023.10323747}
}

@article{eplace2015,
	author = {Lu, Jingwei and Chen, Pengwen and Chang, Chin-Chih and Sha, Lu and Huang, Dennis Jen-Hsin and Teng, Chin-Chi and Cheng, Chung-Kuan},
	title = {ePlace: Electrostatics-Based Placement Using Fast Fourier Transform and Nesterov's Method},
	year = {2015},
	issue_date = {February 2015},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {20},
	number = {2},
	issn = {1084-4309},
	url = {https://doi.org/10.1145/2699873},
	doi = {10.1145/2699873},
	abstract = {We develop a flat, analytic, and nonlinear placement algorithm, ePlace, which is more effective, generalized, simpler, and faster than previous works. Based on the analogy between placement instance and electrostatic system, we develop a novel placement density function eDensity, which models every object as positive charge and the density cost as the potential energy of the electrostatic system. The electric potential and field distribution are coupled with density using a well-defined Poisson's equation, which is numerically solved by spectral methods based on fast Fourier transform (FFT). Instead of using the conjugate gradient (CG) nonlinear solver in previous placers, we propose to use Nesterov's method which achieves faster convergence. The efficiency bottleneck on line search is resolved by predicting the steplength using a closed-form equation of Lipschitz constant. The placement performance is validated through experiments on the ISPD 2005 and ISPD 2006 benchmark suites, where ePlace outperforms all state-of-the-art placers (Capo10.5, FastPlace3.0, RQL, MAPLE, ComPLx, BonnPlace, POLAR, APlace3, NTUPlace3, mPL6) with much shorter wirelength and shorter or comparable runtime. On average, of all the ISPD 2005 benchmarks, ePlace outperforms the leading placer BonnPlace with 2.83\% shorter wirelength and runs 3.05\texttimes{} faster; and on average, of all the ISPD 2006 benchmarks, ePlace outperforms the leading placer MAPLE with 4.59\% shorter wirelength and runs 2.84\texttimes{} faster.},
	journal = {ACM Trans. Des. Autom. Electron. Syst.},
	month = mar,
	articleno = {17},
	numpages = {34},
	keywords = {Electrostatics, Nesterov's method, Poisson's equation, analytic placement, density function, fast Fourier transform, nonlinear optimization, preconditioning, spectral methods}
}

@inproceedings{Chen2024,
	author = {Chen, Yan-Jen and Hsieh, Cheng-Hsiu and Su, Po-Han and Chen, Shao-Hsiang and Chang, Yao-Wen},
	title = {Mixed-Size 3D Analytical Placement with Heterogeneous Technology Nodes},
	year = {2024},
	isbn = {9798400706011},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3649329.3657370},
	doi = {10.1145/3649329.3657370},
	abstract = {This paper proposes a mixed-size 3D analytical placement framework for face-to-face stacked integrated circuits fabricated with heterogeneous technology nodes and connected by hybrid bonding technology. The proposed framework efficiently partitions a given netlist into two dies and optimizes the positions of each macro, standard cell, and hybrid bonding terminal (HBT). A multi-technology objective function and a multi-technology density penalty calculation process are adopted to handle the heterogeneous-technology-node constraints during mixed-size 3D global placement. Furthermore, a 3D objective function is used to refine the placement result during HBT-cell co-optimization. Our placer achieves the best results for all contest test cases compared with the participating teams at the 2023 CAD Contest at ICCAD on 3D Placement with Macros.},
	booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
	articleno = {139},
	numpages = {6},
	keywords = {face-to-face stacked integrated circuits, heterogeneous integration, analytical placement},
	location = {San Francisco, CA, USA},
	series = {DAC '24}
}

@misc{liao2023analyticaldietodie3dplacement,
	title={Analytical Die-to-Die 3D Placement with Bistratal Wirelength Model and GPU Acceleration}, 
	author={Peiyu Liao and Yuxuan Zhao and Dawei Guo and Yibo Lin and Bei Yu},
	year={2023},
	eprint={2310.07424},
	archivePrefix={arXiv},
	primaryClass={cs.AR},
	url={https://arxiv.org/abs/2310.07424}, 
}

@misc{tg_2020_why,
	author = {TG, Javier},
	month = {09},
	title = {Why Sparse Features Should Have Bigger Learning Rates associated? and How Adagrad Achieves this?},
	url = {https://datascience.stackexchange.com/questions/82240/why-sparse-features-should-have-bigger-learning-rates-associated-and-how-adagra},
	urldate = {2025-12-03},
	year = {2020},
	organization = {Data Science Stack Exchange}
}

@phdthesis{phdthesisAlgorithms2019,
	author = {Monteiro, Jucemar},
	year = {2019},
	month = {05},
	pages = {},
	title = {Algorithms to improve area density utilization, routability and timing during detailed placement and legalization of VLSI circuits}
}

@misc{_2024_83,
	author = {iEDA},
	month = {08},
	title = {8.3 Layout Problems and Modeling},
	url = {https://ieda.oscc.cc/en/train/eda/eda-model/8_3_palcement.html},
	urldate = {2025-12-05},
	year = {2024},
	organization = {Oscc.cc}
}

@misc{maciejbalawejder_2022_optimizers,
	author = {Maciej Balawejder},
	month = {03},
	publisher = {Nerd for Tech},
	title = {Optimizers in Machine Learning - Nerd for Tech - Medium},
	url = {https://medium.com/nerd-for-tech/optimizers-in-machine-learning-f1a9c549f8b4},
	urldate = {2025-12-05},
	year = {2022},
	organization = {Medium}
}

@misc{_2024_a7vlsi,
	author = {iEDA},
	month = {08},
	title = {A7-VLSI Auto Placement for Circuit Cells},
	url = {https://ieda.oscc.cc/en/train/practice/algorithms/a7.html},
	urldate = {2025-12-05},
	year = {2024},
	organization = {Oscc.cc}
}
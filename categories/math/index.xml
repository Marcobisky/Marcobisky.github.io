<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Math on Marcobisky</title>
        <link>http://localhost:1313/categories/math/</link>
        <description>Recent content in Math on Marcobisky</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Marcobisky</copyright>
        <lastBuildDate>Mon, 30 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/categories/math/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Machine Learning Final Exam</title>
        <link>http://localhost:1313/p/machine-learning-final-exam/</link>
        <pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/machine-learning-final-exam/</guid>
        <description>&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文仅供个人的学习和参考使用&lt;/li&gt;
&lt;li&gt;本文参考了UofG尊贵的祖传ppt&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Others&amp;rdquo; 中的一般是要通过具体问题来进行学习和理解的部分&lt;/li&gt;
&lt;li&gt;ML 需要大量的实践和数学上的理解，不可能靠本文学懂 ML，这些概念的罗列也只是&lt;em&gt;某种形式教育&lt;/em&gt;之下的产物，不必沉迷。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;optimization&#34;&gt;Optimization
&lt;/h2&gt;&lt;h3 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;三个要素：decision variables, objective functions and constraints. 任何优化问题先决定这三个分别具体是什么&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Decision space&lt;/strong&gt; (决策域) $\mathbb{R}^n$ 和 &lt;strong&gt;Feasible Space&lt;/strong&gt; (可行域) $X$: 有 $n$ 个 decision variables, decision space 的维数就是 $n$; 加上工程学限制，将 decision space 进一步缩小为 feasible space $X$ (比如天线集合参数所有可能的取值集合).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feasible Solution&lt;/strong&gt; $x$: Feasible Space 中的一个点 $x \in X$, 简称 solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective functions&lt;/strong&gt; (目标函数):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;按 Objective space 分类：
&lt;ul&gt;
&lt;li&gt;单目标优化： $f(x): X \to \mathbb{R}$&lt;/li&gt;
&lt;li&gt;多目标优化 $f(x): X \to \mathbb{R}^m$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;按需求分类：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;：需要求最小化的一切函数 (最优化方法的默认约定).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;utility function&lt;/strong&gt;（效用）：需要求最大值的一切函数.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective space&lt;/strong&gt; (目标空间): $f$ 的值所在的空间，$\mathbb{R}$ 或 $\mathbb{R}^n$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constraint / Unconstraint&lt;/strong&gt;: 无约束是有约束的特例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-objective / Multi-objective&lt;/strong&gt;: 看下面&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unimodal (Convex) / Multimodal&lt;/strong&gt;: 有一个最值 / 有多个极值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;single-objective-optimization&#34;&gt;Single-objective optimization
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定义：目标函数的值是标量（而不是向量）的问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/optimization_def.png&#34;
	width=&#34;2438&#34;
	height=&#34;914&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/optimization_def_hu12949627818430624984.png 480w, http://localhost:1313/p/machine-learning-final-exam/optimization_def_hu9196538371009860556.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;266&#34;
		data-flex-basis=&#34;640px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution的质量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two feasible solutions: Loss function值越小的solution越好&lt;/li&gt;
&lt;li&gt;A feasible solution vs. an infeasible solution: 当然feasible solution更好&lt;/li&gt;
&lt;li&gt;Two infeasible solutions: 哪个超出可行域的程度更低更好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Combinational Optimization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义：决策向量 $x$ 所在的集合是有限阶（离散）的&lt;/li&gt;
&lt;li&gt;Examples:
&lt;ul&gt;
&lt;li&gt;Traveling Salesman Problem (TSP): 你要从袁记云饺门店出发去五组团、八组团、六组团、格院楼送外卖，最后要回到袁记云饺门店，怎么走最近？
&lt;ul&gt;
&lt;li&gt;应用：Robot control：应按照哪种顺序使用机械臂才能最大限度地缩短钻孔时间？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Quadratic Assignment Problem (QAP): 教学楼有三处地点（有远有近），如何分配地点作为办公室、教室、乒乓球室能使总走路成本最低？（比如老师要经常从办公室到教室，不能太远）。命名为Quadratic的原因是该问题可以用一个二次型来描述。&lt;/li&gt;
&lt;li&gt;knapsack：你的包只能装20kg的东西，你要旅游只能用包装东西，有手机、拯救者电脑、烧水壶、水杯、卫生纸、杠铃，你带哪些？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Continuous Optimization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义：决策向量 $x$ 所在的集合是连续的&lt;/li&gt;
&lt;li&gt;Examples:
&lt;ul&gt;
&lt;li&gt;天线设计：确定天线的长度、宽度、高度等几何参数（在一定范围），使得他的 $S_{11}$ （理解为浪费的能量）Response 在工作波段 5.28 GHz – 5.72 GHz 尽可能小。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;multi-objective-optimization&#34;&gt;Multi-objective optimization
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定义：多目标优化也叫 Vector Optimization，因为目标函数的取值可看作一个向量&lt;/p&gt;
&lt;p&gt;$
\min_{x\in X}(f_{1}(x),f_{2}(x),\cdots ,f_{k}(x))
$&lt;/p&gt;
&lt;p&gt;where $X$ is the feasible space (一般就是 $\mathbb{R}^n$). $x \in X$ is a feasible solution&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;但是往往会有 trade-off 情况：让哪个 $f$ 的值最大呢？但是有一种很显然的情况:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution的质量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$x_1^{*}$ &lt;strong&gt;Dominates&lt;/strong&gt; $x_2^*$: 如果两个 feasible solution $x_1^{*}$ 和 $x_2^{*}$，如果 $x_1^{*}$ 能让所有的 $f$ 都要比用 $x_2^{*}$ 的更小 ( $\le$ )，那 $x_1^{*}$ 肯定优于 $x_2^{*}$，称 $x_1^{*}$ Dominates $x_2^{*}$. 如果是 $&amp;lt;$，称 $x_1^{*}$ strictly dominates $x_2^{*}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$x_1^{*}$, $x_1^{*}$ &lt;strong&gt;Cannot Compare&lt;/strong&gt;: 如果相较于 $x_1^{*}$，$x_2^{*}$ 会让有些 $f$ 更小有些更大（而不是全部更小或者全部更大），则称 $x_1^{*}$ cannot compared with $x_2^{*}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$x_1^{*}$ 和 $x_1^{*}$ 要么不能比较，要么一者优于另一者！&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pareto set&lt;/strong&gt; 和 &lt;strong&gt;Pareto front&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/Pareto_front.png&#34;
	width=&#34;1802&#34;
	height=&#34;1350&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/Pareto_front_hu15069518061312468447.png 480w, http://localhost:1313/p/machine-learning-final-exam/Pareto_front_hu17854343704622216852.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;$A$ and $B$ dominates $C$&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/PS_PF.png&#34;
	width=&#34;888&#34;
	height=&#34;1062&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/PS_PF_hu14601109703247268178.png 480w, http://localhost:1313/p/machine-learning-final-exam/PS_PF_hu5305634759178065730.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pareto set 在可行域中，而 Pareto front 在目标空间中&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;200px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;metric-space-度量空间&#34;&gt;Metric space 度量空间
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Decision space $\mathbb{F}^n$ ($\mathbb{F}$ 是伽罗瓦域) 可被赋予一个特定的 Metric (如欧式距离、Taxi-cab 距离、Hamming 距离) 使之成为一个度量空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他概念请自行阅读 Rudin 或 Terence Tao 的实分析教材，omitted here.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;others&#34;&gt;Others
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;将Engineering问题转化为 Optimization 的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ai-部分&#34;&gt;AI 部分
&lt;/h2&gt;&lt;h3 id=&#34;machine-learning-ml-general-concepts&#34;&gt;Machine Learning (ML) General Concepts
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一句话概括ML：人先猜一个函数 $f$ 的样子，让机器算出这个函数里面的参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML 的分类&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/ML_class.png&#34;
	width=&#34;1954&#34;
	height=&#34;724&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/ML_class_hu13302043225562044500.png 480w, http://localhost:1313/p/machine-learning-final-exam/ML_class_hu2690921056094842256.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;ML的分类&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;269&#34;
		data-flex-basis=&#34;647px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;supervised-learning&#34;&gt;Supervised Learning
&lt;/h3&gt;&lt;h4 id=&#34;definitions&#34;&gt;Definitions
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt;: 数据集中有标签的学习.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data mining (preparation)&lt;/strong&gt;: 数据集的收集过程&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data cleaning&lt;/strong&gt;: 筛选错误的数据的过程&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Missing data&lt;/strong&gt;: 对于丢失的数据，丢弃(Dropping)和填补(Imputing)都不太好，因为前者可能使有用的数据损失，后者可能引入偏差 (这ppt写的，无语╰(‵□′)╯)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;(Labelled) Training set&lt;/strong&gt; $\mathcal{D}$: 一个输入-输出的集合 $\mathcal{D}= {(\mathbf{x_i},y_i)}^N_{i=1}$，$\mathbf{x_i}$ 是输入（比如图片），$y_i$ 是输出（比如类别）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feature Space&lt;/strong&gt; $\mathcal{X}$: $\mathbf{x_i}$ 的取值集合
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feature Selection&lt;/strong&gt;: 选择你觉得相关的数据形式（比如生物学研究中，基因表达数据可能包含上万个基因特征，但只有少数基因与某种疾病相关，那就别把这些不相关的数据用来训练了）
&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/feature_selection.png&#34;
	width=&#34;1110&#34;
	height=&#34;760&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/feature_selection_hu244886884879820385.png 480w, http://localhost:1313/p/machine-learning-final-exam/feature_selection_hu5238993921454423167.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Feature Section 的分类&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;146&#34;
		data-flex-basis=&#34;350px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Label Space&lt;/strong&gt; $\mathcal{Y}$: $y_i$ 的取值集合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Supervised learning 解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; 问题（or &lt;strong&gt;Pattern recognition&lt;/strong&gt;: $\mathcal{Y}$ 有限集， $\mathcal{Y} = {1, \ldots, C}$. (比如 {Male, Female})，$y_i$ 称作 &lt;strong&gt;categorical variable&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binary Classification: $C=2$&lt;/li&gt;
&lt;li&gt;Multiclass Classification: $C&amp;gt;2$&lt;/li&gt;
&lt;li&gt;Multilabel Classification: $f$ 为多值映射&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;举例：手写数字识别、图片分类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;regression&lt;/strong&gt; 问题: $y_i$ 取值于一个无限集 (比如 $\mathbb{R}$)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unsupervised-learning&#34;&gt;Unsupervised Learning
&lt;/h3&gt;&lt;h4 id=&#34;definitions-1&#34;&gt;Definitions
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt;: 数据集中没有标签的学习，机器需要自行发现 “interesting patterns” (称为 “knowledge discovery”)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;(Unlabelled) Training set&lt;/strong&gt; $\mathcal{D}$: 一个只有输入的集合 $\mathcal{D}= {(\mathbf{x_i})}^N_{i=1}$，$\mathbf{x_i}$ 是输入（比如图片）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;UnSupervised learning 解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: 没有标签的分类问题
&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/clustering.png&#34;
	width=&#34;2054&#34;
	height=&#34;712&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/clustering_hu5711183451677437300.png 480w, http://localhost:1313/p/machine-learning-final-exam/clustering_hu311243230497095402.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Clustering is Unlabelled Classification&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;288&#34;
		data-flex-basis=&#34;692px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: 将数据点降维从而发现影响数据分布的少量几个本质因素(&lt;strong&gt;latent factors&lt;/strong&gt;), 其实很像 Feature Reduction.
&lt;ul&gt;
&lt;li&gt;常见方法包括：PCA (principal components analysis), ICA (a variation of PCA)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning
&lt;/h3&gt;&lt;h4 id=&#34;definitions-2&#34;&gt;Definitions
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;: 人类也学不会，或者说不清楚怎么学的问题让机器去学的机器学习方法。但是人类知道怎么给奖励或惩罚 (punishment or reward)，使得机器在试错和奖励中学到东西。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-final-exam/reinforcement.png&#34;
	width=&#34;1568&#34;
	height=&#34;918&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-final-exam/reinforcement_hu9903096740412477591.png 480w, http://localhost:1313/p/machine-learning-final-exam/reinforcement_hu6701565579191203966.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方法：（没搞懂）
&lt;ul&gt;
&lt;li&gt;Policy-based RL uses a policy or deterministic strategy that maximises cumulative reward&lt;/li&gt;
&lt;li&gt;Value-based RL tries to maximise an arbitrary value function&lt;/li&gt;
&lt;li&gt;Model-based RL creates a virtual model for a certain environment and the
agent learns to perform within those constraint&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Machine Learning Notes (Updating)</title>
        <link>http://localhost:1313/p/machine-learning-notes-updating/</link>
        <pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/machine-learning-notes-updating/</guid>
        <description>&lt;img src="http://localhost:1313/p/machine-learning-notes-updating/cover.png" alt="Featured image of post Machine Learning Notes (Updating)" /&gt;&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;人类最伟大的成就之一就是发明了集合(set)和映射(mapping)这两个概念。他们的抽象性使得自然界的&lt;strong&gt;所有&lt;/strong&gt;存在都可以被这两个概念描述。比如分类就是图片集合到类别集合的映射，图片生成就是文字集合到图片集合的映射，信源编码就是字符集合到codewords集合的映射，等等。要注意这里的集合都是非常大的！比如图片集合是所有图片，不同大小，黑白的、彩色的等等等等，我们姑且称这种集合为“大集合”吧。比如 $\mathbb{R}$ 到 $\mathbb{R}$ 上所有函数构成的集合(denoted $\mathbb{R}^{\mathbb{R}}$)，这也是“大集合”，这种集合在实践中很难把握和分析。&lt;/p&gt;
&lt;p&gt;说回映射，他们往往都是在人脑中发生的，比如我们看到一张图片，立即认为这是这是一条狗而不是其他的东西，即把图片映成拉狗这个对象，但这个映射写不出表达式！&lt;/p&gt;
&lt;p&gt;或者说，可以？！如果我们粗略地写出了某种表达式，且图片通过这个表达式输出的结果和人脑中输出的结果很像，那是不是可以说我们找到了这种映射的显式表达？&lt;/p&gt;
&lt;p&gt;这正是机器学习干的事情，我们尝试找到一种映射，定义域是所有图片的集合(for example)，值域是物品字符串的集合。如果我们把所有图片到物品字符串的&lt;strong&gt;映射&lt;/strong&gt;也都收集在一起构成一个“大集合”（或称为大函数空间），其实我们要找的就是这个大函数空间中的一个元素！注意是元素，这个元素就是我们脑中的那个映射。&lt;/p&gt;
&lt;p&gt;但是怎么找呢？首先这个映射都没有一个固定的形式，比如它可以是 $y=ax^2$，也可以是 $y = \sin(ax)+bx+cx^2+d\log(x)$ 等等所有这样的表达式 (这里简单起见就用一维变量的来说明)。如果我们仅仅把问题就局限在某种特定的函数形式上，这个过程就叫参数化(parametrization)，这里的 $a, b, c, d$ 就叫做参数(parameters)。某一种特定的参数化，远远不能涵盖大函数空间中的所有元素！&lt;/p&gt;
&lt;p&gt;但是有一种参数化形式，它几乎能涵盖整个函数空间！！也就是说，我们只需要考虑这&lt;strong&gt;一种&lt;/strong&gt;形式的函数，就几乎相当于考虑了函数空间的&lt;strong&gt;所有&lt;/strong&gt;函数！这怎么可能呢？当然上面举例的两种参数化显然是不行的，你能想出什么样的映射形式能够“逼近”大函数空间中的任何映射？&lt;/p&gt;
&lt;h2 id=&#34;universal-approximation-theorem&#34;&gt;Universal approximation theorem
&lt;/h2&gt;&lt;p&gt;这种参数化长这样：&lt;/p&gt;
&lt;p&gt;$$
y = \sum_{i=1}^m c_i , \sigma(w_i \cdot x + b_i) &lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;其中 $\sigma(.)$ 是一种给定的具体的非线性函数，$w_i$, $b_i$, $c_i$都是参数。$x$ 是哪个集合里面的？我知道你很急，但你先别急，总之你看到了一种特殊的参数化形式，现在我跟你说它能拟合任意的函数！&lt;/p&gt;
&lt;p&gt;如果只有三个参数的话，提供的自由度只有三个，肯定不行。所以它们其实是很多很多（但有限个）参数，只是分成了三类。如果能将这种参数话形式可视化出来就好了，有什么方法呢？&lt;/p&gt;
&lt;p&gt;这种形式的函数都可以表示成这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-notes-updating/neural-network.png&#34;
	width=&#34;1364&#34;
	height=&#34;656&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-notes-updating/neural-network_hu14705155456146977583.png 480w, http://localhost:1313/p/machine-learning-notes-updating/neural-network_hu15926872759567313556.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;499px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;称为（全连接的）“神经网络”。&lt;/p&gt;
&lt;p&gt;神经网络可以拟合任意函数。&lt;/p&gt;
&lt;p&gt;这个结论使得机器学习得以安稳地存在，因为我们知道理论上，只要网络足够复杂，就可以拟合出任意的映射（而不会忙活半天最终证明压根拟合不了！）&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Least Squares as Projection 最小二乘法的投影解释</title>
        <link>http://localhost:1313/p/least-squares-as-projection-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E6%8A%95%E5%BD%B1%E8%A7%A3%E9%87%8A/</link>
        <pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/least-squares-as-projection-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E6%8A%95%E5%BD%B1%E8%A7%A3%E9%87%8A/</guid>
        <description>&lt;img src="http://localhost:1313/p/least-squares-as-projection-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E6%8A%95%E5%BD%B1%E8%A7%A3%E9%87%8A/cover.png" alt="Featured image of post Least Squares as Projection 最小二乘法的投影解释" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;The goal is to find the linear model $y = \beta_0 + \beta_1 x$ such that the sum of squared errors between the predicted values and the actual data is minimized.&lt;/p&gt;
&lt;h2 id=&#34;linear-model&#34;&gt;Linear Model
&lt;/h2&gt;&lt;p&gt;The form of the linear model is:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $y_i$ is the observed value, $x_i$ is the independent variable, $\beta_0$ is the intercept, $\beta_1$ is the slope, and $\epsilon_i$ is the error term.&lt;/p&gt;
&lt;p&gt;We wish to find $\beta_0$ and $\beta_1$ such that the predicted values $\hat{y}_i = \beta_0 + \beta_1 x_i$ minimize the sum of squared errors between $\hat{y}_i$ and the observed values $y_i$.&lt;/p&gt;
&lt;h2 id=&#34;design-matrix-and-observation-vector&#34;&gt;Design Matrix and Observation Vector
&lt;/h2&gt;&lt;p&gt;To make the problem more convenient, we represent it using vectors and matrices.&lt;/p&gt;
&lt;h3 id=&#34;design-matrix&#34;&gt;Design Matrix
&lt;/h3&gt;&lt;p&gt;Define the design matrix $\mathbf{X}$ as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{X} = \begin{bmatrix}
1 &amp;amp; 1 \newline
1 &amp;amp; 2 \newline
1 &amp;amp; 3 \newline
1 &amp;amp; 4
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;The first column contains only 1s, representing the constant term $\beta_0$, and the second column contains the values of the independent variable $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;observation-vector&#34;&gt;Observation Vector
&lt;/h3&gt;&lt;p&gt;Define the observation vector $\mathbf{y}$ as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{y} = \begin{bmatrix}
2 \newline
3 \newline
5 \newline
7
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;This vector contains all the observed values $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;parameter-vector&#34;&gt;Parameter Vector
&lt;/h3&gt;&lt;p&gt;Define the parameter vector $\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \newline \beta_1 \end{bmatrix}$.&lt;/p&gt;
&lt;h2 id=&#34;sum-of-squared-errors-objective-function&#34;&gt;Sum of Squared Errors Objective Function
&lt;/h2&gt;&lt;p&gt;In regression, our goal is to find the parameters $\boldsymbol{\beta}$ such that the predicted values $\hat{\mathbf{y}} = \mathbf{X} \boldsymbol{\beta}$ are as close as possible to the observed values $\mathbf{y}$, by minimizing the sum of squared errors (SSE):&lt;/p&gt;
&lt;p&gt;$$
S(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$&lt;/p&gt;
&lt;h2 id=&#34;deriving-the-normal-equation&#34;&gt;Deriving the Normal Equation
&lt;/h2&gt;&lt;p&gt;The key idea of least squares is to find $\boldsymbol{\beta}$ such that the residual $\mathbf{y} - \mathbf{X} \boldsymbol{\beta}$ is minimized. Geometrically, this means that the residual should be orthogonal to the column space of the design matrix $\mathbf{X}$, which leads to the normal equation:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{X}^T (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}) = 0
$$&lt;/p&gt;
&lt;p&gt;Expanding this:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{X}^T \mathbf{y} = \mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}}
$$&lt;/p&gt;
&lt;p&gt;This is the normal equation, which can be solved to find the least squares estimate $\hat{\boldsymbol{\beta}}$.&lt;/p&gt;
&lt;h2 id=&#34;solving-the-normal-equation&#34;&gt;Solving the Normal Equation
&lt;/h2&gt;&lt;p&gt;Now, let&amp;rsquo;s compute the parts of the normal equation.&lt;/p&gt;
&lt;h3 id=&#34;compute-mathbfxt-mathbfx&#34;&gt;Compute $\mathbf{X}^T \mathbf{X}$
&lt;/h3&gt;&lt;p&gt;$$
\mathbf{X}^T \mathbf{X} = \begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 1 \newline
1 &amp;amp; 2 \newline
1 &amp;amp; 3 \newline
1 &amp;amp; 4
\end{bmatrix}
= \begin{bmatrix}
4 &amp;amp; 10 \newline
10 &amp;amp; 30
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;compute-mathbfxt-mathbfy&#34;&gt;Compute $\mathbf{X}^T \mathbf{y}$
&lt;/h3&gt;&lt;p&gt;$$
\mathbf{X}^T \mathbf{y} = \begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
2 \newline
3 \newline
5 \newline
7
\end{bmatrix}
= \begin{bmatrix}
17 \newline
50
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;solve-the-normal-equation&#34;&gt;Solve the Normal Equation
&lt;/h3&gt;&lt;p&gt;Now we solve the normal equation:&lt;/p&gt;
&lt;p&gt;$$
\begin{bmatrix}
4 &amp;amp; 10 \newline
10 &amp;amp; 30
\end{bmatrix} \hat{\boldsymbol{\beta}} = \begin{bmatrix}
17 \newline
50
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;To solve this, we first compute the inverse of $\mathbf{X}^T \mathbf{X}$:&lt;/p&gt;
&lt;p&gt;$$
(\mathbf{X}^T \mathbf{X})^{-1} = \frac{1}{(4)(30) - (10)(10)} \begin{bmatrix}
30 &amp;amp; -10 \newline
-10 &amp;amp; 4
\end{bmatrix} = \frac{1}{20} \begin{bmatrix}
30 &amp;amp; -10 \newline
-10 &amp;amp; 4
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Next, we compute $\hat{\boldsymbol{\beta}}$:&lt;/p&gt;
&lt;p&gt;$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$&lt;/p&gt;
&lt;p&gt;$$
\hat{\boldsymbol{\beta}} = \frac{1}{20} \begin{bmatrix}
30 &amp;amp; -10 \newline
-10 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
17 \newline
50
\end{bmatrix}
= \frac{1}{20} \begin{bmatrix}
(30)(17) + (-10)(50) \newline
(-10)(17) + (4)(50)
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;$$
\hat{\boldsymbol{\beta}} = \frac{1}{20} \begin{bmatrix}
510 - 500 \newline
-170 + 200
\end{bmatrix}
= \frac{1}{20} \begin{bmatrix}
10 \newline
30
\end{bmatrix}
= \begin{bmatrix}
0.5 \newline
1.5
\end{bmatrix}
$$&lt;/p&gt;
&lt;h2 id=&#34;least-squares-estimate&#34;&gt;Least Squares Estimate
&lt;/h2&gt;&lt;p&gt;By solving the normal equation, we find $\hat{\beta}_0 = 0.5$ and $\hat{\beta}_1 = 1.5$. Thus, the regression model is:&lt;/p&gt;
&lt;p&gt;$$
\hat{y} = 0.5 + 1.5x
$$&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Using the projection approach, we see that the least squares estimate is the projection of the observation vector $\mathbf{y}$ onto the space spanned by the columns of the design matrix $\mathbf{X}$. By solving the normal equation, we found the parameters $\hat{\beta}_0 = 0.5$ and $\hat{\beta}_1 = 1.5$, which minimize the sum of squared errors.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Coding Theory 编码理论 (以后再更)</title>
        <link>http://localhost:1313/p/coding-theory-%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA-%E4%BB%A5%E5%90%8E%E5%86%8D%E6%9B%B4/</link>
        <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/coding-theory-%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA-%E4%BB%A5%E5%90%8E%E5%86%8D%E6%9B%B4/</guid>
        <description>&lt;img src="http://localhost:1313/p/coding-theory-%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA-%E4%BB%A5%E5%90%8E%E5%86%8D%E6%9B%B4/cover.png" alt="Featured image of post Coding Theory 编码理论 (以后再更)" /&gt;&lt;h2 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h2&gt;&lt;h3 id=&#34;alphabet-字母表&#34;&gt;Alphabet 字母表
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; &lt;u&gt;Alphabet&lt;/u&gt; is just another name for &amp;ldquo;finite set&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; An Alphabet $Q$ determines a new set $Q^*$, which contains all finite sequences of elements in $Q$. The elements in $Q^&lt;em&gt;$ are called &lt;u&gt;codewords&lt;/u&gt; (on Q). i.e.,
$$
Q^&lt;/em&gt; := \bigcup_{i \in \mathbb{N}_+} Q^i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Alphabet 中的元素可以是任何东西！比如：&lt;/p&gt;
&lt;p&gt;$S$ := {&amp;ldquo;dogs&amp;rdquo;, &amp;ldquo;cats&amp;rdquo;, &amp;ldquo;birds&amp;rdquo;}，(含有待编码的东西，&amp;ldquo;source alphabet&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;$T$ := {0, 1}, (含有用来编码的符号, &amp;ldquo;target alphabet&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;$T^*$ := {00, 01, 10, 11}，(含有编码后的东西, &amp;ldquo;codewords&amp;rdquo;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;source alphabet 中的元素一般称为 clear text，target alphabet $Q$ 中的元素一般称为 letters, Q* 中的元素是 codewords。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;codewords 有等长和不等长之分。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; Let $S$ and $T$ be alphabets, 编码(code)是一个从 $S$ 到 $T^*$ 的映射，i.e. code: $S \to T^*$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;但是我们一般认为 $T^*$ 中的元素才是 code 而不是映射本身，这有点像随机变量一般认为是 $\tilde{\Omega}$ 中的元素而不是可测映射本身。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ex.&lt;/strong&gt; $Q := {0, 1}$, $\mathbf{c} = 00100 (\equiv (0, 0, 1, 0, 0)) \in Q^*$，$\mathbf{c}$ 称为 2-ary code of length 5.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hamming-space-汉明空间&#34;&gt;Hamming Space 汉明空间
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; 设 $Q$ 是 alphabet, $N \in \mathbb{N}&lt;em&gt;+$, 度量空间 $(Q^N, h_d)$ 称为 $Q$ 上的 Hamming space. $h_d: Q^N \times Q^N \to \mathbb{N}&lt;/em&gt;+$ 称为 Hamming distance, 定义为：&lt;/p&gt;
&lt;p&gt;$$
\forall \mathbf{a}, \mathbf{b} \in Q^N, h_d(\mathbf{a}, \mathbf{b}) := # {i \in {1, 2, \ldots N} : a_i \neq b_i}
$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Boolean Algebra and Category of Lattices 布尔代数与格范畴</title>
        <link>http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/</link>
        <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/</guid>
        <description>&lt;img src="http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/cover.png" alt="Featured image of post Boolean Algebra and Category of Lattices 布尔代数与格范畴" /&gt;&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;首先我们提出研究布尔代数的动机，为啥要研究布尔代数？1和0的与、或、非运算是人都能算明白，简直显然到家，为啥还要花篇幅研究这个？首先请考虑以下问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集合和布尔代数都有De Morgan&amp;rsquo;s laws，这是巧合吗？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\overline{A \cup B} = \overline{A} \cap \overline{B} \newline
\overline{A \cap B} = \overline{A} \cup \overline{B}
$$
$$
\overline{A \vee B} = \overline{A} \wedge \overline{B} \newline
\overline{A \wedge B} = \overline{A} \vee \overline{B}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XOR和OR哪个运算更fundamental？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\vee$和$\wedge$为什么满足双侧分配律？为什么一般的模或向量空间只满足乘法对加法的分配律？分配律的本质是什么？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;逻辑学与布尔代数有什么关系？逻辑学是研究命题以及命题之间的关系的，布尔代数就是0和1一通操作的代数，这两者有什么关系？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为什么电路会跟逻辑相关？电路能实现一切运算吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;很想用抽代的语言来描述布尔代数，具体怎么描述？&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;希望通过这篇文章，能够解答上述问题。&lt;/p&gt;
&lt;h2 id=&#34;boole-algebra-mathbbbn-和-boolean-algebra-k-是不一样的&#34;&gt;Boole Algebra $(\mathbb{B^n})$ 和 Boolean Algebra $(K)$ 是不一样的
&lt;/h2&gt;&lt;p&gt;想到0和1及它们的运算，最先能够联想到的（maybe）是 环 $\mathbb{Z}/2\mathbb{Z}$ 上的运算。我们就从 $\mathbb{Z}/2\mathbb{Z}$ 出发，不断加上结构，首先得到 Boole 代数，然后再得到 Boolean 代数，这也是历史上的发展顺序。&lt;/p&gt;
&lt;h2 id=&#34;boole-algebra-mathbbbn&#34;&gt;Boole Algebra $(\mathbb{B^n})$
&lt;/h2&gt;&lt;p&gt;简单起见，我们把 $\mathbb{Z}/2\mathbb{Z}$ 看作特征为2的有限域（而不是环，因为域上向量空间的性质好一点），并将其记作 $\mathbb{B}$:&lt;/p&gt;
&lt;p&gt;$$
\mathbb{B} \equiv { \bar{0}, \bar{1} }
$$&lt;/p&gt;
&lt;p&gt;（下文我们将省略 bar）&lt;/p&gt;
&lt;p&gt;很容易验证 $\mathbb{B}$ 上的加法对应 XOR 运算，乘法对应 AND 运算。（从这个角度看，XOR 是比 OR 更fundamental的运算，因为XOR并不依赖任何的电子元件的特征，仅仅是从数学中自然而然引出的概念，即就是 $\mathbb{Z}/2\mathbb{Z}$ 上的加法，仅此而已）。&lt;/p&gt;
&lt;p&gt;自然地，我们可以定义 coordinate space $\mathbb{B}^n$，其中的元素是 $n$-tuples of elements of $\mathbb{B}$，并且加法的数乘的定义是 component-wise 的。$\mathbb{B}^n$ 构成 $\mathbb{B}$ 上的向量空间，所以我们称 $\mathbb{B}^n$ 中的元素是一个 bit vector，比如：&lt;/p&gt;
&lt;p&gt;$$
\mathbb{B}^5 \ni \mathbf{b} := (1, 0, 1, 1, 0)
$$&lt;/p&gt;
&lt;p&gt;现在来研究 $\mathbb{B}^n$ 上线性组合、span、dimension 等概念。&lt;/p&gt;
&lt;h2 id=&#34;boolean-algebra-k&#34;&gt;Boolean Algebra $(K)$
&lt;/h2&gt;&lt;p&gt;下面给出 Boolean 代数的定义：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; 一个 Boolean 代数 $(K,\vee, \wedge, \bar{} \space)$ 是一个集合 $K$ equipped with 两个二元运算 $\vee$ 和 $\wedge$, 还有一个一元运算 $\space$ $\bar{}$ $\space$, 使得：&lt;/p&gt;
&lt;p&gt;A1. $(K, \vee)$ 和 $(K, \wedge)$ 是交换幺半群&lt;/p&gt;
&lt;p&gt;A2. $\vee$ 和 $\wedge$ 满足双侧分配律，且二者地位对等&lt;/p&gt;
&lt;p&gt;A3. 存在元素 $0, 1 \in K$ 使得:
&lt;img src=&#34;http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/boolean-def.jpg&#34;
	width=&#34;1280&#34;
	height=&#34;820&#34;
	srcset=&#34;http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/boolean-def_hu15896879855589954061.jpg 480w, http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/boolean-def_hu14243650574475763826.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;A4. $\space$ $\bar{}$ $\space$ 运算由以下公理定义：&lt;/p&gt;
&lt;p&gt;$$
x \wedge \bar{x} = 0 \newline
x \vee \bar{x} = 1
$$&lt;/p&gt;
&lt;p&gt;A1-A3 称为单调公理，A4 称为补公理。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>

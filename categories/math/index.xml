<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Math on Marcobisky</title>
        <link>http://localhost:1313/categories/math/</link>
        <description>Recent content in Math on Marcobisky</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Marcobisky</copyright>
        <lastBuildDate>Mon, 25 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/categories/math/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Machine Learning Notes (Updating)</title>
        <link>http://localhost:1313/p/machine-learning-notes-updating/</link>
        <pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/machine-learning-notes-updating/</guid>
        <description>&lt;img src="http://localhost:1313/p/machine-learning-notes-updating/cover.png" alt="Featured image of post Machine Learning Notes (Updating)" /&gt;&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;人类最伟大的成就之一就是发明了集合(set)和映射(mapping)这两个概念。他们的抽象性使得自然界的&lt;strong&gt;所有&lt;/strong&gt;存在都可以被这两个概念描述。比如分类就是图片集合到类别集合的映射，图片生成就是文字集合到图片集合的映射，信源编码就是字符集合到codewords集合的映射，等等。要注意这里的集合都是非常大的！比如图片集合是所有图片，不同大小，黑白的、彩色的等等等等，我们姑且称这种集合为“大集合”吧。比如 $\mathbb{R}$ 到 $\mathbb{R}$ 上所有函数构成的集合(denoted $\mathbb{R}^{\mathbb{R}}$)，这也是“大集合”，这种集合在实践中很难把握和分析。&lt;/p&gt;
&lt;p&gt;说回映射，他们往往都是在人脑中发生的，比如我们看到一张图片，立即认为这是这是一条狗而不是其他的东西，即把图片映成拉狗这个对象，但这个映射写不出表达式！&lt;/p&gt;
&lt;p&gt;或者说，可以？！如果我们粗略地写出了某种表达式，且图片通过这个表达式输出的结果和人脑中输出的结果很像，那是不是可以说我们找到了这种映射的显式表达？&lt;/p&gt;
&lt;p&gt;这正是机器学习干的事情，我们尝试找到一种映射，定义域是所有图片的集合(for example)，值域是物品字符串的集合。如果我们把所有图片到物品字符串的&lt;strong&gt;映射&lt;/strong&gt;也都收集在一起构成一个“大集合”（或称为大函数空间），其实我们要找的就是这个大函数空间中的一个元素！注意是元素，这个元素就是我们脑中的那个映射。&lt;/p&gt;
&lt;p&gt;但是怎么找呢？首先这个映射都没有一个固定的形式，比如它可以是 $y=ax^2$，也可以是 $y = \sin(ax)+bx+cx^2+d\log(x)$ 等等所有这样的表达式 (这里简单起见就用一维变量的来说明)。如果我们仅仅把问题就局限在某种特定的函数形式上，这个过程就叫参数化(parametrization)，这里的 $a, b, c, d$ 就叫做参数(parameters)。某一种特定的参数化，远远不能涵盖大函数空间中的所有元素！&lt;/p&gt;
&lt;p&gt;但是有一种参数化形式，它几乎能涵盖整个函数空间！！也就是说，我们只需要考虑这&lt;strong&gt;一种&lt;/strong&gt;形式的函数，就几乎相当于考虑了函数空间的&lt;strong&gt;所有&lt;/strong&gt;函数！这怎么可能呢？当然上面举例的两种参数化显然是不行的，你能想出什么样的映射形式能够“逼近”大函数空间中的任何映射？&lt;/p&gt;
&lt;h2 id=&#34;universal-approximation-theorem&#34;&gt;Universal approximation theorem
&lt;/h2&gt;&lt;p&gt;这种参数化长这样：&lt;/p&gt;
&lt;p&gt;$$
y = \sum_{i=1}^m c_i , \sigma(w_i \cdot x + b_i) &lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;其中 $\sigma(.)$ 是一种给定的具体的非线性函数，$w_i$, $b_i$, $c_i$都是参数。$x$ 是哪个集合里面的？我知道你很急，但你先别急，总之你看到了一种特殊的参数化形式，现在我跟你说它能拟合任意的函数！&lt;/p&gt;
&lt;p&gt;如果只有三个参数的话，提供的自由度只有三个，肯定不行。所以它们其实是很多很多（但有限个）参数，只是分成了三类。如果能将这种参数话形式可视化出来就好了，有什么方法呢？&lt;/p&gt;
&lt;p&gt;这种形式的函数都可以表示成这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/machine-learning-notes-updating/neural-network.png&#34;
	width=&#34;1364&#34;
	height=&#34;656&#34;
	srcset=&#34;http://localhost:1313/p/machine-learning-notes-updating/neural-network_hu14705155456146977583.png 480w, http://localhost:1313/p/machine-learning-notes-updating/neural-network_hu15926872759567313556.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;499px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;称为（全连接的）“神经网络”。&lt;/p&gt;
&lt;p&gt;神经网络可以拟合任意函数。&lt;/p&gt;
&lt;p&gt;这个结论使得机器学习得以安稳地存在，因为我们知道理论上，只要网络足够复杂，就可以拟合出任意的映射（而不会忙活半天最终证明压根拟合不了！）&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Least Squares as Projection 最小二乘法的投影解释</title>
        <link>http://localhost:1313/p/least-squares-as-projection-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E6%8A%95%E5%BD%B1%E8%A7%A3%E9%87%8A/</link>
        <pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/least-squares-as-projection-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E6%8A%95%E5%BD%B1%E8%A7%A3%E9%87%8A/</guid>
        <description>&lt;img src="http://localhost:1313/p/least-squares-as-projection-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E6%8A%95%E5%BD%B1%E8%A7%A3%E9%87%8A/cover.png" alt="Featured image of post Least Squares as Projection 最小二乘法的投影解释" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;The goal is to find the linear model $y = \beta_0 + \beta_1 x$ such that the sum of squared errors between the predicted values and the actual data is minimized.&lt;/p&gt;
&lt;h2 id=&#34;linear-model&#34;&gt;Linear Model
&lt;/h2&gt;&lt;p&gt;The form of the linear model is:&lt;/p&gt;
&lt;p&gt;$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$&lt;/p&gt;
&lt;p&gt;where $y_i$ is the observed value, $x_i$ is the independent variable, $\beta_0$ is the intercept, $\beta_1$ is the slope, and $\epsilon_i$ is the error term.&lt;/p&gt;
&lt;p&gt;We wish to find $\beta_0$ and $\beta_1$ such that the predicted values $\hat{y}_i = \beta_0 + \beta_1 x_i$ minimize the sum of squared errors between $\hat{y}_i$ and the observed values $y_i$.&lt;/p&gt;
&lt;h2 id=&#34;design-matrix-and-observation-vector&#34;&gt;Design Matrix and Observation Vector
&lt;/h2&gt;&lt;p&gt;To make the problem more convenient, we represent it using vectors and matrices.&lt;/p&gt;
&lt;h3 id=&#34;design-matrix&#34;&gt;Design Matrix
&lt;/h3&gt;&lt;p&gt;Define the design matrix $\mathbf{X}$ as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{X} = \begin{bmatrix}
1 &amp;amp; 1 \newline
1 &amp;amp; 2 \newline
1 &amp;amp; 3 \newline
1 &amp;amp; 4
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;The first column contains only 1s, representing the constant term $\beta_0$, and the second column contains the values of the independent variable $x_i$.&lt;/p&gt;
&lt;h3 id=&#34;observation-vector&#34;&gt;Observation Vector
&lt;/h3&gt;&lt;p&gt;Define the observation vector $\mathbf{y}$ as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{y} = \begin{bmatrix}
2 \newline
3 \newline
5 \newline
7
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;This vector contains all the observed values $y_i$.&lt;/p&gt;
&lt;h3 id=&#34;parameter-vector&#34;&gt;Parameter Vector
&lt;/h3&gt;&lt;p&gt;Define the parameter vector $\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \newline \beta_1 \end{bmatrix}$.&lt;/p&gt;
&lt;h2 id=&#34;sum-of-squared-errors-objective-function&#34;&gt;Sum of Squared Errors Objective Function
&lt;/h2&gt;&lt;p&gt;In regression, our goal is to find the parameters $\boldsymbol{\beta}$ such that the predicted values $\hat{\mathbf{y}} = \mathbf{X} \boldsymbol{\beta}$ are as close as possible to the observed values $\mathbf{y}$, by minimizing the sum of squared errors (SSE):&lt;/p&gt;
&lt;p&gt;$$
S(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$&lt;/p&gt;
&lt;h2 id=&#34;deriving-the-normal-equation&#34;&gt;Deriving the Normal Equation
&lt;/h2&gt;&lt;p&gt;The key idea of least squares is to find $\boldsymbol{\beta}$ such that the residual $\mathbf{y} - \mathbf{X} \boldsymbol{\beta}$ is minimized. Geometrically, this means that the residual should be orthogonal to the column space of the design matrix $\mathbf{X}$, which leads to the normal equation:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{X}^T (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}) = 0
$$&lt;/p&gt;
&lt;p&gt;Expanding this:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{X}^T \mathbf{y} = \mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}}
$$&lt;/p&gt;
&lt;p&gt;This is the normal equation, which can be solved to find the least squares estimate $\hat{\boldsymbol{\beta}}$.&lt;/p&gt;
&lt;h2 id=&#34;solving-the-normal-equation&#34;&gt;Solving the Normal Equation
&lt;/h2&gt;&lt;p&gt;Now, let&amp;rsquo;s compute the parts of the normal equation.&lt;/p&gt;
&lt;h3 id=&#34;compute-mathbfxt-mathbfx&#34;&gt;Compute $\mathbf{X}^T \mathbf{X}$
&lt;/h3&gt;&lt;p&gt;$$
\mathbf{X}^T \mathbf{X} = \begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 1 \newline
1 &amp;amp; 2 \newline
1 &amp;amp; 3 \newline
1 &amp;amp; 4
\end{bmatrix}
= \begin{bmatrix}
4 &amp;amp; 10 \newline
10 &amp;amp; 30
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;compute-mathbfxt-mathbfy&#34;&gt;Compute $\mathbf{X}^T \mathbf{y}$
&lt;/h3&gt;&lt;p&gt;$$
\mathbf{X}^T \mathbf{y} = \begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
2 \newline
3 \newline
5 \newline
7
\end{bmatrix}
= \begin{bmatrix}
17 \newline
50
\end{bmatrix}
$$&lt;/p&gt;
&lt;h3 id=&#34;solve-the-normal-equation&#34;&gt;Solve the Normal Equation
&lt;/h3&gt;&lt;p&gt;Now we solve the normal equation:&lt;/p&gt;
&lt;p&gt;$$
\begin{bmatrix}
4 &amp;amp; 10 \newline
10 &amp;amp; 30
\end{bmatrix} \hat{\boldsymbol{\beta}} = \begin{bmatrix}
17 \newline
50
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;To solve this, we first compute the inverse of $\mathbf{X}^T \mathbf{X}$:&lt;/p&gt;
&lt;p&gt;$$
(\mathbf{X}^T \mathbf{X})^{-1} = \frac{1}{(4)(30) - (10)(10)} \begin{bmatrix}
30 &amp;amp; -10 \newline
-10 &amp;amp; 4
\end{bmatrix} = \frac{1}{20} \begin{bmatrix}
30 &amp;amp; -10 \newline
-10 &amp;amp; 4
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Next, we compute $\hat{\boldsymbol{\beta}}$:&lt;/p&gt;
&lt;p&gt;$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$&lt;/p&gt;
&lt;p&gt;$$
\hat{\boldsymbol{\beta}} = \frac{1}{20} \begin{bmatrix}
30 &amp;amp; -10 \newline
-10 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
17 \newline
50
\end{bmatrix}
= \frac{1}{20} \begin{bmatrix}
(30)(17) + (-10)(50) \newline
(-10)(17) + (4)(50)
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;$$
\hat{\boldsymbol{\beta}} = \frac{1}{20} \begin{bmatrix}
510 - 500 \newline
-170 + 200
\end{bmatrix}
= \frac{1}{20} \begin{bmatrix}
10 \newline
30
\end{bmatrix}
= \begin{bmatrix}
0.5 \newline
1.5
\end{bmatrix}
$$&lt;/p&gt;
&lt;h2 id=&#34;least-squares-estimate&#34;&gt;Least Squares Estimate
&lt;/h2&gt;&lt;p&gt;By solving the normal equation, we find $\hat{\beta}_0 = 0.5$ and $\hat{\beta}_1 = 1.5$. Thus, the regression model is:&lt;/p&gt;
&lt;p&gt;$$
\hat{y} = 0.5 + 1.5x
$$&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Using the projection approach, we see that the least squares estimate is the projection of the observation vector $\mathbf{y}$ onto the space spanned by the columns of the design matrix $\mathbf{X}$. By solving the normal equation, we found the parameters $\hat{\beta}_0 = 0.5$ and $\hat{\beta}_1 = 1.5$, which minimize the sum of squared errors.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Coding Theory 编码理论 (以后再更)</title>
        <link>http://localhost:1313/p/coding-theory-%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA-%E4%BB%A5%E5%90%8E%E5%86%8D%E6%9B%B4/</link>
        <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/coding-theory-%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA-%E4%BB%A5%E5%90%8E%E5%86%8D%E6%9B%B4/</guid>
        <description>&lt;img src="http://localhost:1313/p/coding-theory-%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA-%E4%BB%A5%E5%90%8E%E5%86%8D%E6%9B%B4/cover.png" alt="Featured image of post Coding Theory 编码理论 (以后再更)" /&gt;&lt;h2 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h2&gt;&lt;h3 id=&#34;alphabet-字母表&#34;&gt;Alphabet 字母表
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; &lt;u&gt;Alphabet&lt;/u&gt; is just another name for &amp;ldquo;finite set&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; An Alphabet $Q$ determines a new set $Q^*$, which contains all finite sequences of elements in $Q$. The elements in $Q^&lt;em&gt;$ are called &lt;u&gt;codewords&lt;/u&gt; (on Q). i.e.,
$$
Q^&lt;/em&gt; := \bigcup_{i \in \mathbb{N}_+} Q^i
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Alphabet 中的元素可以是任何东西！比如：&lt;/p&gt;
&lt;p&gt;$S$ := {&amp;ldquo;dogs&amp;rdquo;, &amp;ldquo;cats&amp;rdquo;, &amp;ldquo;birds&amp;rdquo;}，(含有待编码的东西，&amp;ldquo;source alphabet&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;$T$ := {0, 1}, (含有用来编码的符号, &amp;ldquo;target alphabet&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;$T^*$ := {00, 01, 10, 11}，(含有编码后的东西, &amp;ldquo;codewords&amp;rdquo;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;source alphabet 中的元素一般称为 clear text，target alphabet $Q$ 中的元素一般称为 letters, Q* 中的元素是 codewords。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;codewords 有等长和不等长之分。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; Let $S$ and $T$ be alphabets, 编码(code)是一个从 $S$ 到 $T^*$ 的映射，i.e. code: $S \to T^*$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;但是我们一般认为 $T^*$ 中的元素才是 code 而不是映射本身，这有点像随机变量一般认为是 $\tilde{\Omega}$ 中的元素而不是可测映射本身。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ex.&lt;/strong&gt; $Q := {0, 1}$, $\mathbf{c} = 00100 (\equiv (0, 0, 1, 0, 0)) \in Q^*$，$\mathbf{c}$ 称为 2-ary code of length 5.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hamming-space-汉明空间&#34;&gt;Hamming Space 汉明空间
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Defn.&lt;/strong&gt; 设 $Q$ 是 alphabet, $N \in \mathbb{N}&lt;em&gt;+$, 度量空间 $(Q^N, h_d)$ 称为 $Q$ 上的 Hamming space. $h_d: Q^N \times Q^N \to \mathbb{N}&lt;/em&gt;+$ 称为 Hamming distance, 定义为：&lt;/p&gt;
&lt;p&gt;$$
\forall \mathbf{a}, \mathbf{b} \in Q^N, h_d(\mathbf{a}, \mathbf{b}) := # {i \in {1, 2, \ldots N} : a_i \neq b_i}
$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Boolean Algebra and Category of Lattices 布尔代数与格范畴</title>
        <link>http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/</link>
        <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/</guid>
        <description>&lt;img src="http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/cover.png" alt="Featured image of post Boolean Algebra and Category of Lattices 布尔代数与格范畴" /&gt;&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;首先我们提出研究布尔代数的动机，为啥要研究布尔代数？1和0的与、或、非运算是人都能算明白，简直显然到家，为啥还要花篇幅研究这个？首先请考虑以下问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集合和布尔代数都有De Morgan&amp;rsquo;s laws，这是巧合吗？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\overline{A \cup B} = \overline{A} \cap \overline{B} \newline
\overline{A \cap B} = \overline{A} \cup \overline{B}
$$
$$
\overline{A \vee B} = \overline{A} \wedge \overline{B} \newline
\overline{A \wedge B} = \overline{A} \vee \overline{B}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XOR和OR哪个运算更fundamental？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\vee$和$\wedge$为什么满足双侧分配律？为什么一般的模或向量空间只满足乘法对加法的分配律？分配律的本质是什么？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;逻辑学与布尔代数有什么关系？逻辑学是研究命题以及命题之间的关系的，布尔代数就是0和1一通操作的代数，这两者有什么关系？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为什么电路会跟逻辑相关？电路能实现一切运算吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;很想用抽代的语言来描述布尔代数，具体怎么描述？&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;希望通过这篇文章，能够解答上述问题。&lt;/p&gt;
&lt;h2 id=&#34;boole-algebra-mathbbbn-和-boolean-algebra-k-是不一样的&#34;&gt;Boole Algebra $(\mathbb{B^n})$ 和 Boolean Algebra $(K)$ 是不一样的
&lt;/h2&gt;&lt;p&gt;想到0和1及它们的运算，最先能够联想到的（maybe）是 环 $\mathbb{Z}/2\mathbb{Z}$ 上的运算。我们就从 $\mathbb{Z}/2\mathbb{Z}$ 出发，不断加上结构，首先得到 Boole 代数，然后再得到 Boolean 代数，这也是历史上的发展顺序。&lt;/p&gt;
&lt;h2 id=&#34;boole-algebra-mathbbbn&#34;&gt;Boole Algebra $(\mathbb{B^n})$
&lt;/h2&gt;&lt;p&gt;简单起见，我们把 $\mathbb{Z}/2\mathbb{Z}$ 看作特征为2的有限域（而不是环，因为域上向量空间的性质好一点），并将其记作 $\mathbb{B}$:&lt;/p&gt;
&lt;p&gt;$$
\mathbb{B} \equiv { \bar{0}, \bar{1} }
$$&lt;/p&gt;
&lt;p&gt;（下文我们将省略 bar）&lt;/p&gt;
&lt;p&gt;很容易验证 $\mathbb{B}$ 上的加法对应 XOR 运算，乘法对应 AND 运算。（从这个角度看，XOR 是比 OR 更fundamental的运算，因为XOR并不依赖任何的电子元件的特征，仅仅是从数学中自然而然引出的概念，即就是 $\mathbb{Z}/2\mathbb{Z}$ 上的加法，仅此而已）。&lt;/p&gt;
&lt;p&gt;自然地，我们可以定义 coordinate space $\mathbb{B}^n$，其中的元素是 $n$-tuples of elements of $\mathbb{B}$，并且加法的数乘的定义是 component-wise 的。$\mathbb{B}^n$ 构成 $\mathbb{B}$ 上的向量空间，所以我们称 $\mathbb{B}^n$ 中的元素是一个 bit vector，比如：&lt;/p&gt;
&lt;p&gt;$$
\mathbb{B}^5 \ni \mathbf{b} := (1, 0, 1, 1, 0)
$$&lt;/p&gt;
&lt;p&gt;现在来研究 $\mathbb{B}^n$ 上线性组合、span、dimension 等概念。&lt;/p&gt;
&lt;h2 id=&#34;boolean-algebra-k&#34;&gt;Boolean Algebra $(K)$
&lt;/h2&gt;&lt;p&gt;下面给出 Boolean 代数的定义：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; 一个 Boolean 代数 $(K,\vee, \wedge, \bar{} \space)$ 是一个集合 $K$ equipped with 两个二元运算 $\vee$ 和 $\wedge$, 还有一个一元运算 $\space$ $\bar{}$ $\space$, 使得：&lt;/p&gt;
&lt;p&gt;A1. $(K, \vee)$ 和 $(K, \wedge)$ 是交换幺半群&lt;/p&gt;
&lt;p&gt;A2. $\vee$ 和 $\wedge$ 满足双侧分配律，且二者地位对等&lt;/p&gt;
&lt;p&gt;A3. 存在元素 $0, 1 \in K$ 使得:
&lt;img src=&#34;http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/boolean-def.jpg&#34;
	width=&#34;1280&#34;
	height=&#34;820&#34;
	srcset=&#34;http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/boolean-def_hu15896879855589954061.jpg 480w, http://localhost:1313/p/boolean-algebra-and-category-of-lattices-%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A0%BC%E8%8C%83%E7%95%B4/boolean-def_hu14243650574475763826.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;A4. $\space$ $\bar{}$ $\space$ 运算由以下公理定义：&lt;/p&gt;
&lt;p&gt;$$
x \wedge \bar{x} = 0 \newline
x \vee \bar{x} = 1
$$&lt;/p&gt;
&lt;p&gt;A1-A3 称为单调公理，A4 称为补公理。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>

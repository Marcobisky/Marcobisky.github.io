[
  {
    "objectID": "posts/vector-operation/index.html",
    "href": "posts/vector-operation/index.html",
    "title": "Three ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解",
    "section": "",
    "text": "This property of vector operation bother me for a loooooong time:\nMixed product1: \\[\n\\boxed{\na \\cdot (b \\times c) = b \\cdot (c \\times a) = c \\cdot (a \\times b).\n}\n\\tag{1}\\]\n1 Also known as “scalar triple product”.This says nothing but the mixed product is unchanged under a circular shift.\nYou will understand it in this article!"
  },
  {
    "objectID": "posts/vector-operation/index.html#hello",
    "href": "posts/vector-operation/index.html#hello",
    "title": "Three ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解",
    "section": "",
    "text": "This property of vector operation bother me for a loooooong time:\nMixed product1: \\[\n\\boxed{\na \\cdot (b \\times c) = b \\cdot (c \\times a) = c \\cdot (a \\times b).\n}\n\\tag{1}\\]\n1 Also known as “scalar triple product”.This says nothing but the mixed product is unchanged under a circular shift.\nYou will understand it in this article!"
  },
  {
    "objectID": "posts/vector-operation/index.html#three-approaches-to-the-mixed-product",
    "href": "posts/vector-operation/index.html#three-approaches-to-the-mixed-product",
    "title": "Three ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解",
    "section": "2 Three Approaches to the Mixed Product",
    "text": "2 Three Approaches to the Mixed Product\n\n\n\n\n\n\n\nLemma\n\n\n\n\nLemma 1 Let \\(a, b, c \\in \\mathbb{R}^3\\), then \\[\na \\cdot (b \\times c) =\n\\begin{vmatrix}\n| & | & | \\\\\na & b & c \\\\\n| & | & |\n\\end{vmatrix}\n\\]\n\n\n\n\nOnce this is established, we can use the fact that: \\[\n\\begin{vmatrix}\n| & | & | \\\\\na & b & c \\\\\n| & | & |\n\\end{vmatrix} =\n\\begin{vmatrix}\n| & | & | \\\\\nb & c & a \\\\\n| & | & |\n\\end{vmatrix} =\n\\begin{vmatrix}\n| & | & | \\\\\nc & a & b \\\\\n| & | & |\n\\end{vmatrix}\n\\] to prove Equation 1.\nNow let’s understand Lemma 1 in three different ways!\n\n2.1 Geometric Approach\nSee Figure 1, dot product make the slanted black box straight but maintains its volume.\n\n\n\n\n\n\nFigure 1: Geometric proof of Lemma 1\n\n\n\n\n\n2.2 3b1b Approach\nI called this “3b1b Approach” because this method is inspired by a great mathematician Grant Sanderson. “3b1b” is the name of his Youtube channel.\nIn a video created by him, there is a very interesting function \\(f: \\mathbb{R}^3 \\to \\mathbb{R}\\): \\[\nf(x) =\n\\begin{vmatrix}\n| & | & | \\\\\nx & v & w \\\\\n| & | & |\n\\end{vmatrix}\n\\tag{2}\\] which takes in a vector in \\(\\mathbb{R}^3\\) and output a number in \\(\\mathbb{R}\\) (\\(v\\) and \\(w\\) are predefined and fixed).2 Now I claim that:\n\n2 Functions from a vector space to a scalar is often referred to as a functional.\n\n\n\n\n\nClaim\n\n\n\n\nTheorem 1 The functional \\(f\\) in Equation 2 is linear, i.e., \\[\nf(x + y) = f(x) + f(y), \\forall x, y \\in \\mathbb{R}^3\n\\] and \\[\nf(\\alpha x) = \\alpha f(x), \\forall \\alpha \\in \\mathbb{R}.\n\\]\n\n\n\n\nThis is trivial.\nThere is another theorem3:\n\n3 There is a video created by myself that explains this in detail.\n\n\n\n\n\nRiesz Representation Theorem (reduced version)\n\n\n\n\nTheorem 2 Every linear functional in \\(\\mathbb{R}^n\\) induces a vector \\(p \\in \\mathbb{R}^n\\) such that \\[\nf(x) = x \\cdot p, \\forall x \\in \\mathbb{R}^n.\n\\]\n\n\n\n\nTherefore, Equation 2 becomes \\[\n\\boxed{\nx \\cdot p =\n\\begin{vmatrix}\n| & | & | \\\\\nx & v & w \\\\\n| & | & |\n\\end{vmatrix}\n}\n\\tag{3}\\]\n\n\n\n\n\n\n\nMnemonic device inspired by Equation 3\n\n\n\n\n\nThe Equation 3 gives us a way to write the cross product in a more “dot-product” way: \\[\nv \\times w =\n\\begin{vmatrix}\n\\hat{\\imath} & v_1 & w_1 \\\\\n\\hat{\\jmath} & v_2 & w_2 \\\\\n\\hat{k} & v_3 & w_3\n\\end{vmatrix}\n\\] because the result \\(v \\times w = (v_2w_3 - v_3w_2)\\hat{\\imath} + (v_3w_1 - v_1w_3)\\hat{\\jmath} + (v_1w_2 - v_2w_1)\\hat{k}\\) is very much like a dot product! Remember \\(v_2w_3 - v_3w_2, v_3w_1 - v_1w_3, v_1w_2 - v_2w_1\\) are just three numbers unless you you associate each number with a direction.\nIf you feel uncomfortable with this notation, there is more: \\[\n\\operatorname{curl} F\n\\equiv \\nabla \\times F =\n\\begin{vmatrix}\n\\hat{\\imath} & \\frac{\\partial}{\\partial x} & F_x \\\\\n\\hat{\\jmath} & \\frac{\\partial}{\\partial y} & F_y \\\\\n\\hat{k} & \\frac{\\partial}{\\partial z} & F_z\n\\end{vmatrix}\n\\] where we treat \\(\\nabla\\) like a vector! The first column is just an indicator that the result should be interpreted as a vector not a dot product. I took long time to think \\(\\nabla\\) as a kind of special vector, but I failed. Feel free to ignore the notation \\(\\nabla \\times F\\) if you don’t like it!\n\n\n\n\nWhat is \\(p\\) then? Well, \\(p\\) is a special vector in \\(\\mathbb{R}^3\\) such that the dot product with any vector \\(x\\) gives a number that is equal to the volume of a box spaned by \\(x\\), \\(v\\), and \\(w\\). This is a little bit mouthful, but we can see immediately from Figure 1 that \\(p\\) is just the blue vector, which is \\(v \\times w\\) in this case! So Equation 3 becomes: \\[\nx \\cdot (v \\times w) = \\begin{vmatrix}\n| & | & | \\\\\nx & v & w \\\\\n| & | & |\n\\end{vmatrix}\n\\]\nWe are done!\n\n\n2.3 Tensor Approach\nNow this explanation requires some knowledge about tensors4. But once you understand it, you will completely change your view on determinants! (If you did not respect them at all in the past anyway.)\n\n4 Just a quick joke, tensors are exactly linear functionals but we allow multiple vectors to be the input (instead of one).\n\n\n\n\n\nProposition\n\n\n\n\nProposition 1 The mapping \\(g(a, b, c) := a \\cdot (b \\times c)\\) and \\(\\det\\) are both alternating 3-tensor, i.e., \\[\ng, \\det \\in \\bigwedge\\nolimits^{\\!3} \\left(\\mathbb{R}^3\\right).\n\\tag{4}\\]\n\n\n\n\nWe also have this theorem:\n\n\n\n\n\n\n\nUniqueness of volume form\n\n\n\n\nTheorem 3 The dimension of the vector space of \\(k\\)-tensors on \\(\\mathbb{R}^n\\) is \\({n \\choose k}\\): \\[\n\\operatorname{dim} \\bigwedge\\nolimits^{\\!k} \\left(\\mathbb{R}^n\\right) = {n \\choose k}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDemonstration of Theorem 3\n\n\n\n\n\nPick \\(n = 3\\) and \\(k = 2\\). This is because in order to get the result of a \\(2\\)-tensor5 \\(B\\) acting on any two vectors of dimension \\(3\\), we would only need to specify the values of \\(B\\) acting on any \\(2\\) of the three basis vectors (\\(e_x, e_y, e_z\\)). How many numbers do we need? Well, only \\({3 \\choose 2} = 3\\) numbers: \\[\nB(e_x, e_y) = a_{12}, B(e_y, e_z) = a_{23}, B(e_z, e_x) = a_{31}.\n\\]\nThese are the “basis” of the space \\(\\bigwedge\\nolimits^{\\!2} \\left(\\mathbb{R}^3\\right)\\). So its dimension is \\(3\\). Then we just use the bilinear and alternating property of \\(B\\) to calculate the result of any two input vectors say \\(x = 2e_x + e_y - e_z\\) and \\(y = -e_x + 3e_z\\): \\[\n\\begin{aligned}\nB(x, y)\n&= B(2e_x + e_y - e_z, -e_x + 3e_z) \\\\\n&= B(2e_x + e_y - e_z, -e_x) + B(2e_x + e_y - e_z, 3e_z) \\\\\n&= -2B(e_x, e_x) - B(e_y, e_x) + B(e_z, e_x) \\\\\n&+ 6B(e_x, e_z) + 3B(e_y, e_z) - 3B(e_z, e_z) \\\\\n&= 0 - (-a_{12}) + a_{31} \\\\\n&+ 6(-a_{31}) + 3a_{23} - 0 \\\\\n&= a_{12} + 3a_{23} - 5a_{31}.\n\\end{aligned}\n\\]\n\n\n\n\n5 This is also called an alternating bilinear form.6 This type of tensor is also called “volume form”.As a special case6 when \\(n = k = 3\\), we have: \\[\n\\boxed{\n\\operatorname{dim} \\bigwedge\\nolimits^{\\!3} \\left(\\mathbb{R}^3\\right) = {3 \\choose 3} = 1.\n}\n\\]\nLet’s think about what this means. The determinant is a very special object that every volume form in \\(n\\) dimension is just a scalar multiple of it! In other words, every alternating \\(n\\)-tensor in \\(\\mathbb{R}^n\\) must be the determinant (up to a scalar)! So with Equation 4, both \\(g\\) and \\(\\det\\) are volume forms! So \\(g\\) must be a scalar multiple of \\(\\det\\): \\[\ng = k \\det.\n\\]\nWe can evaluate \\(k\\) by choosing a very special set of \\(a, b, c \\in \\mathbb{R}^3\\), say \\(a = e_x, b = e_y, c = e_z\\): \\[\ng(e_x, e_y, e_z) = e_x \\cdot (e_y \\times e_z) = e_x \\cdot e_x = 1,\n\\] \\[\n\\det(e_x, e_y, e_z) =\n\\begin{vmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{vmatrix} = 1.\n\\]\nTherefore, \\(k = 1\\) and \\(g = \\det\\). We are done!"
  },
  {
    "objectID": "posts/symmetry/index.html",
    "href": "posts/symmetry/index.html",
    "title": "What is symmetry? 什么是对称性?",
    "section": "",
    "text": "You may have noticed these concepts:\n\n\n\n\n\n\n\n\nEven/odd complex-valued function\n\n\n\n\nDefinition 1 A function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) is called\n\nconjugate symmetric \\(:\\iff f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff -f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\nSpecial cases of Definition 1\n\n\n\n\n\n\n\n\n\nEven/odd real function\n\n\n\n\nDefinition 2 A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-x) = f(x), \\forall x \\in \\mathbb{R}\\),\nodd \\(:\\iff -f(-x) = f(x), \\forall x \\in \\mathbb{R}\\)\n\n\n\n\n\n\n\n\n\n\nEven/odd multivariate real function\n\n\n\n\nDefinition 3 A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nodd \\(:\\iff -f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposition Property\n\n\n\n\nTheorem 1 Any function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) can be decomposed1 into a symmetric part \\(Sf\\) and a anti-symmetric part \\(Af\\): \\[\nf = \\frac{Sf + Af}{2},\n\\] \\[\nSf := f(\\mathbf{v})+\\overline{f(-\\mathbf{v})},\n\\] \\[\nAf := f(\\mathbf{v})-\\overline{f(-\\mathbf{v})}.\n\\]\nIn fancier language, \\[\n\\mathbb{C}^{\\mathbb{R}^n} = S\\mathbb{C}^{\\mathbb{R}^n} \\oplus A\\mathbb{C}^{\\mathbb{R}^n}.\n\\]\n\n\n\n1 The reason why I do NOT define \\(Sf = (f(\\mathbf{v})+\\overline{f(-\\mathbf{v})})/2\\) will be clear later.\n\n\n\n\n\nNote\n\n\n\nAs a special case of Theorem 1, any function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is a sum of an even and an odd function: \\[\nf = \\frac{(f(x) + f(-x))+(f(x) - f(-x))}{2}.\n\\]\n\n\n\n\n\nThere are also multiplicative version of Definition 1 and Theorem 1:\n\n\n\n\n\n\nMultiplicative version of Definition 1\n\n\n\n\nDefinition 4 A function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) is called2\n\nMultiplicative conjugate symmetric \\(:\\iff f(\\frac{1}{\\mathbf{v}}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff \\frac{1}{f(\\frac{1}{\\mathbf{v}})} = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n,\\)\n\nwhere \\(\\frac{1}{\\mathbf{v}}\\) is another vector in \\((\\mathbb{R}^{\\times})^n\\) whose components are the reciprocal of those of \\(\\mathbf{v}\\).\n\n\n\n2 \\(\\mathbb{R}^{\\times} := \\mathbb{R} \\backslash \\{0\\}\\).\n\n\n\n\n\nMultiplicative version of Decomposition Property\n\n\n\n\nTheorem 2 Any function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) can be decomposed into a symmetric part \\(S^{\\bullet}f\\) and a anti-symmetric part \\(A^{\\bullet}f\\): \\[\nf = \\sqrt{S^{\\bullet}f \\cdot A^{\\bullet}f},\n\\] \\[\nS^{\\bullet}f := f(\\mathbf{v}) \\cdot \\overline{f(\\mathbf{v}^{-1})},\n\\] \\[\nA^{\\bullet}f := \\frac{f(\\mathbf{v})}{\\overline{f(\\mathbf{v}^{-1})}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSymmetric/Alternating Tensor\n\n\n\n\nDefinition 5 A symmetric rank-\\(k\\) tensor \\(f: V^k \\to \\mathbb{R}\\) is symmetric iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\nIt is alternating iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = (\\operatorname{sgn} \\sigma) f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\n\n\n\nThough generally we cannot decompose an arbitrary tensor into a symmetric and alternating part, we could build them by introducing two operators:\n\n\n\n\n\n\nSymmetric/Alternating Operator for Tensors\n\n\n\n\nDefinition 6 Given \\(\\forall f: V^k \\to \\mathbb{R}\\), the operator \\(S\\) and \\(A\\) defined below always give a symmetric and alternating tensor3: \\[\nSf := \\sum_{\\sigma \\in S_k} \\sigma f,\n\\] \\[\nAf := \\sum_{\\sigma \\in S_k} \\operatorname{sgn}(\\sigma) \\sigma f.\n\\]\n\n\n\n3 \\(\\sigma f\\) is defined by \\((\\sigma f)(v_1, v_2, \\ldots, v_k) := f(v_{\\sigma(1)}, v_{\\sigma(2)}, \\ldots, v_{\\sigma(k)}).\\)\n\n\n\n\n\n\n\n\nSelf-adjoint and Skew-adjoint Matrices\n\n\n\n\nDefinition 7 A linear operator \\(\\phi \\in \\operatorname{Hom}(V)\\) is called self-adjoint iff \\[\n\\phi^H = \\phi,\n\\] and skew-adjoint iff \\[\n\\phi^H = -\\phi.\n\\]"
  },
  {
    "objectID": "posts/symmetry/index.html#sec-sec1",
    "href": "posts/symmetry/index.html#sec-sec1",
    "title": "What is symmetry? 什么是对称性?",
    "section": "",
    "text": "You may have noticed these concepts:\n\n\n\n\n\n\n\n\nEven/odd complex-valued function\n\n\n\n\nDefinition 1 A function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) is called\n\nconjugate symmetric \\(:\\iff f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff -f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\nSpecial cases of Definition 1\n\n\n\n\n\n\n\n\n\nEven/odd real function\n\n\n\n\nDefinition 2 A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-x) = f(x), \\forall x \\in \\mathbb{R}\\),\nodd \\(:\\iff -f(-x) = f(x), \\forall x \\in \\mathbb{R}\\)\n\n\n\n\n\n\n\n\n\n\nEven/odd multivariate real function\n\n\n\n\nDefinition 3 A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nodd \\(:\\iff -f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposition Property\n\n\n\n\nTheorem 1 Any function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) can be decomposed1 into a symmetric part \\(Sf\\) and a anti-symmetric part \\(Af\\): \\[\nf = \\frac{Sf + Af}{2},\n\\] \\[\nSf := f(\\mathbf{v})+\\overline{f(-\\mathbf{v})},\n\\] \\[\nAf := f(\\mathbf{v})-\\overline{f(-\\mathbf{v})}.\n\\]\nIn fancier language, \\[\n\\mathbb{C}^{\\mathbb{R}^n} = S\\mathbb{C}^{\\mathbb{R}^n} \\oplus A\\mathbb{C}^{\\mathbb{R}^n}.\n\\]\n\n\n\n1 The reason why I do NOT define \\(Sf = (f(\\mathbf{v})+\\overline{f(-\\mathbf{v})})/2\\) will be clear later.\n\n\n\n\n\nNote\n\n\n\nAs a special case of Theorem 1, any function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is a sum of an even and an odd function: \\[\nf = \\frac{(f(x) + f(-x))+(f(x) - f(-x))}{2}.\n\\]\n\n\n\n\n\nThere are also multiplicative version of Definition 1 and Theorem 1:\n\n\n\n\n\n\nMultiplicative version of Definition 1\n\n\n\n\nDefinition 4 A function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) is called2\n\nMultiplicative conjugate symmetric \\(:\\iff f(\\frac{1}{\\mathbf{v}}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff \\frac{1}{f(\\frac{1}{\\mathbf{v}})} = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n,\\)\n\nwhere \\(\\frac{1}{\\mathbf{v}}\\) is another vector in \\((\\mathbb{R}^{\\times})^n\\) whose components are the reciprocal of those of \\(\\mathbf{v}\\).\n\n\n\n2 \\(\\mathbb{R}^{\\times} := \\mathbb{R} \\backslash \\{0\\}\\).\n\n\n\n\n\nMultiplicative version of Decomposition Property\n\n\n\n\nTheorem 2 Any function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) can be decomposed into a symmetric part \\(S^{\\bullet}f\\) and a anti-symmetric part \\(A^{\\bullet}f\\): \\[\nf = \\sqrt{S^{\\bullet}f \\cdot A^{\\bullet}f},\n\\] \\[\nS^{\\bullet}f := f(\\mathbf{v}) \\cdot \\overline{f(\\mathbf{v}^{-1})},\n\\] \\[\nA^{\\bullet}f := \\frac{f(\\mathbf{v})}{\\overline{f(\\mathbf{v}^{-1})}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSymmetric/Alternating Tensor\n\n\n\n\nDefinition 5 A symmetric rank-\\(k\\) tensor \\(f: V^k \\to \\mathbb{R}\\) is symmetric iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\nIt is alternating iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = (\\operatorname{sgn} \\sigma) f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\n\n\n\nThough generally we cannot decompose an arbitrary tensor into a symmetric and alternating part, we could build them by introducing two operators:\n\n\n\n\n\n\nSymmetric/Alternating Operator for Tensors\n\n\n\n\nDefinition 6 Given \\(\\forall f: V^k \\to \\mathbb{R}\\), the operator \\(S\\) and \\(A\\) defined below always give a symmetric and alternating tensor3: \\[\nSf := \\sum_{\\sigma \\in S_k} \\sigma f,\n\\] \\[\nAf := \\sum_{\\sigma \\in S_k} \\operatorname{sgn}(\\sigma) \\sigma f.\n\\]\n\n\n\n3 \\(\\sigma f\\) is defined by \\((\\sigma f)(v_1, v_2, \\ldots, v_k) := f(v_{\\sigma(1)}, v_{\\sigma(2)}, \\ldots, v_{\\sigma(k)}).\\)\n\n\n\n\n\n\n\n\nSelf-adjoint and Skew-adjoint Matrices\n\n\n\n\nDefinition 7 A linear operator \\(\\phi \\in \\operatorname{Hom}(V)\\) is called self-adjoint iff \\[\n\\phi^H = \\phi,\n\\] and skew-adjoint iff \\[\n\\phi^H = -\\phi.\n\\]"
  },
  {
    "objectID": "posts/symmetry/index.html#symmetry-as-group-action",
    "href": "posts/symmetry/index.html#symmetry-as-group-action",
    "title": "What is symmetry? 什么是对称性?",
    "section": "2 Symmetry as Group Action",
    "text": "2 Symmetry as Group Action\n\n2.1 Problem\nIs there any way to unify these seemingly “symmetric” concepts? What kind of mathematical object can be symmetrize and and alternate? When does the object itself expressible by only its symmetrized and and alternated ones?\n\n\n2.2 Important Observation\nThe common thing of the above examples in Section 1 is that the domain of the objects (functions, tensors, matrices4) could be manipulated by some kind of actions:\n4 This is left as an exercise.\n\\(f: \\mathbb{R}^n \\to \\mathbb{C}\\): additive inversion,\n\\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\): multiplicative inversion,\n\\(f: V^k \\to \\mathbb{R}\\): permutation.\n\nThe first two can be viewed as the 2-element group \\(S_2\\) acts on the domain of \\(f\\), where \\(S_2\\) is the group generated by the operation of “taking inverse”: \\[\nS_2 := \\langle\\cdot^{-1}\\rangle = \\{e, \\cdot^{-1}\\},\n\\] or equivalently, the permutation group on two letters: \\[\nS_2 = \\{e, (12)\\}.\n\\]\nTherefore, in the first two cases, we could define a \\(S_2\\)-action: \\[\n(\\sigma f)(\\mathbf{v}) := \\overline{f(\\mathbf{v}^{-1})},\n\\] where \\(\\mathbf{v}^{-1}\\) is either \\(-\\mathbf{v}\\) (additive inverse) or \\(1/\\mathbf{v}\\) (multiplicative inverse).\nTherefore, the definition of the operator \\(S\\) and \\(A\\) in Definition 6 also applies for the first two cases: \\[\nSf\n:= \\sum_{\\sigma \\in S_2} \\sigma f\n= f(\\mathbf{v}) + \\overline{f(-\\mathbf{v})} \\quad (\\text{or } f(\\mathbf{v})\\cdot \\overline{f(-\\mathbf{v})}),\n\\] \\[\nAf\n:= \\sum_{\\sigma \\in S_k} \\operatorname{sgn}(\\sigma) \\sigma f\n= f(\\mathbf{v}) - \\overline{f(-\\mathbf{v})} \\quad (\\text{or } \\frac{f(\\mathbf{v})}{\\overline{f(\\mathbf{v}^{-1})}}).\n\\]\n\n\n2.3 When Decomposable?\nIn the first two cases, \\(f\\) can be expressed purely by \\(Sf\\) and \\(Af\\): \\[\nf = \\frac{Sf + Af}{2} \\quad (\\text{or } \\sqrt{Sf \\cdot Af}),\n\\] which is just the average of them! (Arithmetic average and geometric average respectively)\nBut we don’t have this relationship for tensors, i.e., not every rank \\(k\\) tensor can be purely expressed using \\(Sf\\) and \\(Af\\) – apart from the case when \\(k = 2\\): \\[\nf(v_1, v_2) = \\frac{(f(v_1, v_2)+f(v_2, v_1))+(f(v_1, v_2)-f(v_2, v_1))}{2} = \\frac{Sf + Af}{2}.\n\\]\nWhat happened when \\(k \\ge 3\\)?\nLet \\(f: V^3 \\to \\mathbb{R}\\), we have \\[\nSf = f(v_1, v_2, v_3) + f(v_2, v_3, v_1) + f(v_3, v_1, v_2) + f(v_2, v_1, v_3) + f(v_1, v_3, v_2) + f(v_3, v_2, v_1),\n\\] \\[\nAf = f(v_1, v_2, v_3) + f(v_2, v_3, v_1) + f(v_3, v_1, v_2) - f(v_2, v_1, v_3) - f(v_1, v_3, v_2) - f(v_3, v_2, v_1).\n\\]\n\n\n\nVisualize group action\n\n\nThe result \\[\n\\frac{Sf + Af}{2} = f(v_1, v_2, v_3) + f(v_2, v_3, v_1) + f(v_3, v_1, v_2) = \\sum_{\\sigma \\in A_3} \\sigma f \\neq f,\n\\] where \\(A_3\\) is the alternating group (the group of even permutations) on three letters.\n\n\n2.4 Try Yourself!\n\nExercise 1 (\\(S\\) and \\(A\\) operator for matrices \\(\\phi\\)) Let \\(\\phi \\in \\operatorname{End} (\\mathbb{C}^n)\\), derive the definition of \\(S\\phi\\) and \\(A \\phi\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\nS \\phi := \\frac{\\phi + \\phi^H}{2},\n\\] \\[\nA \\phi := \\frac{\\phi - \\phi^H}{2}.\n\\]\nWe also have \\[\n\\phi = \\frac{S \\phi + A \\phi}{2}.\n\\]"
  },
  {
    "objectID": "posts/projective-space/index.html",
    "href": "posts/projective-space/index.html",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "",
    "text": "The projective space of a vector space is the set of all lines1 through the origin with some extra structures.\n1 Or “1-dimensional subspaces”.\n\n\n\nThis can be formally defined by identifying some elements in a vector space \\(V\\), or classifying the vectors in \\(V\\) according to the spaces they span. This is exactly the idea of “quotients” in math.\n\n\n\n\n\n\n\nDefinition of Projective Space\n\n\n\n\nDefinition 1 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\), define a equivalence relation \\(\\sim\\) on \\(V\\) as2: \\[\n\\forall x, y \\in V, x \\sim y :\\iff \\exists \\lambda \\in \\mathbb{F}^*, x = \\lambda y.\n\\] Then we can define projective space \\(\\mathbb{P}(V)\\) as: \\[\n\\mathbb{P}(V) := \\frac{V - \\{0_V\\}}{\\sim}.\n\\]\n\n\n\n\n\n2 \\(\\mathbb{F}^* := \\mathbb{F} - \\{0_\\mathbb{F}\\}\\), i.e., the non-zero elements of the field \\(\\mathbb{F}\\).\n\n\n\n\n\nProjective space as a special case of the Grassmannian manifold\n\n\n\n\n\nAll \\(1\\)-dimensional subspaces form the projective space. What about \\(n\\)-dimensional subspaces? They form the so-called Grassmannian manifold!\n\n\n\n\n\n\nFigure 1: Projective spaces are special case of Grassmannian manifolds\n\n\n\n\n\n\n\n\n\n\nThere is a (surjective) canonical projection \\[\n\\begin{aligned}\n\\pi: V - \\{0_V\\} &\\twoheadrightarrow \\mathbb{P}(V) \\\\\nx &\\mapsto [x]_{\\sim}\n\\end{aligned}\n\\] that maps each non-zero vector to its equivalence class (as shown in Figure 2).\n\n\n\n\n\n\nFigure 2: The mental picture of \\(\\pi\\) for \\(V = \\mathbb{R}^3\\)\n\n\n\nIn the following post, we will only consider the case where \\(\\mathbb{F} \\in \\{\\mathbb{R}, \\mathbb{C}\\}\\) and \\(V = \\mathbb{F}^n\\) since they are the most common cases that we will encounter. It’s necessary to have a clear mental picture of each of the example in the following sections."
  },
  {
    "objectID": "posts/projective-space/index.html#projective-space",
    "href": "posts/projective-space/index.html#projective-space",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "",
    "text": "The projective space of a vector space is the set of all lines1 through the origin with some extra structures.\n1 Or “1-dimensional subspaces”.\n\n\n\nThis can be formally defined by identifying some elements in a vector space \\(V\\), or classifying the vectors in \\(V\\) according to the spaces they span. This is exactly the idea of “quotients” in math.\n\n\n\n\n\n\n\nDefinition of Projective Space\n\n\n\n\nDefinition 1 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\), define a equivalence relation \\(\\sim\\) on \\(V\\) as2: \\[\n\\forall x, y \\in V, x \\sim y :\\iff \\exists \\lambda \\in \\mathbb{F}^*, x = \\lambda y.\n\\] Then we can define projective space \\(\\mathbb{P}(V)\\) as: \\[\n\\mathbb{P}(V) := \\frac{V - \\{0_V\\}}{\\sim}.\n\\]\n\n\n\n\n\n2 \\(\\mathbb{F}^* := \\mathbb{F} - \\{0_\\mathbb{F}\\}\\), i.e., the non-zero elements of the field \\(\\mathbb{F}\\).\n\n\n\n\n\nProjective space as a special case of the Grassmannian manifold\n\n\n\n\n\nAll \\(1\\)-dimensional subspaces form the projective space. What about \\(n\\)-dimensional subspaces? They form the so-called Grassmannian manifold!\n\n\n\n\n\n\nFigure 1: Projective spaces are special case of Grassmannian manifolds\n\n\n\n\n\n\n\n\n\n\nThere is a (surjective) canonical projection \\[\n\\begin{aligned}\n\\pi: V - \\{0_V\\} &\\twoheadrightarrow \\mathbb{P}(V) \\\\\nx &\\mapsto [x]_{\\sim}\n\\end{aligned}\n\\] that maps each non-zero vector to its equivalence class (as shown in Figure 2).\n\n\n\n\n\n\nFigure 2: The mental picture of \\(\\pi\\) for \\(V = \\mathbb{R}^3\\)\n\n\n\nIn the following post, we will only consider the case where \\(\\mathbb{F} \\in \\{\\mathbb{R}, \\mathbb{C}\\}\\) and \\(V = \\mathbb{F}^n\\) since they are the most common cases that we will encounter. It’s necessary to have a clear mental picture of each of the example in the following sections."
  },
  {
    "objectID": "posts/projective-space/index.html#real-projective-space-mathbbrpn",
    "href": "posts/projective-space/index.html#real-projective-space-mathbbrpn",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "2 Real Projective Space \\(\\mathbb{RP}^n\\)",
    "text": "2 Real Projective Space \\(\\mathbb{RP}^n\\)\nWhen \\(V = \\mathbb{R}^n\\), we will denote \\(\\mathbb{P}(\\mathbb{R}^n)\\) by \\(\\mathbb{RP}^{n-1}\\), where the upper index indicates the dimension of the projective space (actually a manifold of dimension \\(n-1\\)).\nThe “shape” of \\(\\mathbb{RP}^n\\) can be understood by Equation 1.\n\\[\n\\boxed{\n\\mathbb{RP}^n \\simeq \\frac{\\mathbb{S}^n}{\\mathbb{Z}_2} \\simeq \\frac{H^n}{\\sim} \\simeq \\frac{D^n}{\\sim}.\n}\n\\tag{1}\\]\n\n\n\n\n\n\n\nNote for Equation 1\n\n\n\n\n\n\nThis is a diffeomorphism in the category of smooth manifolds.\nThe notations in Equation 1 are defined below:\n\n\\(\\mathbb{S}^n\\) is the \\(n\\)-dimensional sphere: \\[\n\\mathbb{S}^n := \\{x \\in \\mathbb{R}^{n+1} : \\|x\\| = 1\\}.\n\\]\n\\(H^n\\) is the upper hemisphere: \\[\nH^n := \\{x \\in \\mathbb{R}^{n+1} : \\|x\\| = 1, x_j \\geq 0\\},\n\\] where \\(j\\) can be any coordinate index of \\(x\\).\n\\(D^n\\) is the closed \\(n\\)-disk: \\[\nD^n := \\{x \\in \\mathbb{R}^{n} : \\|x\\| \\leq 1\\}.\n\\]\n\n\\(\\mathbb{S}^n/\\mathbb{Z}_2\\) means:\n\\[\n\\mathbb{S}^n/\\text{\\{antipodal points\\}}\n\\] Let the group \\(\\mathbb{Z}/2\\mathbb{Z} = \\{\\bar{0}, \\bar{1}\\}\\) acts on \\(\\mathbb{S}^n\\) by taking the antipodal point: \\[\n\\bar{1} \\cdot x = -x, \\quad \\forall x \\in \\mathbb{S}^n.\n\\] This action defines the equivalence relation (See wiki here): \\[\n\\forall x \\in \\mathbb{S}^n, x \\sim y :\\iff \\exists \\lambda \\in \\mathbb{Z}_2, y = \\lambda x.\n\\]\nThe equivalence relation \\(\\sim\\) on \\(H^n\\) is defined as: \\[\n\\forall x, y \\in H^n, x \\sim y :\\iff x = y \\text{ or }\n\\begin{cases} x, y \\in \\partial H^n = \\mathbb{S}^{n-1}, \\\\ x=-y.  \\end{cases}\n\\]\nThe equivalence relation \\(\\sim\\) on \\(D^n\\) is defined as: \\[\n\\forall x, y \\in D^n, x \\sim y :\\iff x = y \\text{ or }\n\\begin{cases} x, y \\in \\partial D^n = \\mathbb{S}^{n-1}, \\\\ x=-y.  \\end{cases}\n\\]\n\n\n\n\n\nThis is explained in detail below.\n\n2.1 Real Projective line3 \\(\\mathbb{RP}^1\\)\nLet \\(V = \\mathbb{R}^2\\), then4: \\[\n\\mathbb{RP}^1 \\simeq \\frac{\\mathbb{S}^1}{\\mathbb{Z}_2} \\simeq \\frac{H^1}{\\sim} \\simeq \\frac{D^1}{\\sim} \\simeq \\mathbb{S}^1 \\simeq \\mathbb{\\hat{R}}.\n\\tag{2}\\]\n4 \\(\\mathbb{\\hat{R}} := \\mathbb{R} \\cup \\{\\infty\\}\\) is called the projectively extended real line. Distinguish it from the extended real line \\(\\overline{\\mathbb{R}} := \\mathbb{R} \\cup \\{-\\infty, +\\infty\\}\\).5 The blue curly arrow in Figure 3 indicates that the two points are equivalent. One can be immediately transported to the other side by this arrow.Visually speaking5,\n\n\n\n\n\n\nFigure 3: Visual demonstration of Equation 2\n\n\n\nNote that in Equation 2, \\(D^1/{\\sim}\\) happens to be \\(\\mathbb{S}^1\\) again, which is very special and not general for \\(n &gt; 1\\).\nInstead of let a line intersects the unit circle at the origin (as shown in the first isomorphism in Equation 2), there are different graphical representations of \\(\\mathbb{RP}^1\\):\n\n\n\n\n\n\nFigure 4: Visual demonstration of \\(\\mathbb{RP}^1 \\simeq \\mathbb{\\hat{R}}\\)\n\n\n\nThe horizontal line in Figure 4 does not intersect the blue line. We thus define it belongs to the equivalent class \\(\\{\\infty\\}\\).\n\n3 The projective space for \\(V = \\mathbb{R}^1\\) is too trivial, since everything in \\(\\mathbb{R}\\) is in one equivalence class. Therefore, \\(\\mathbb{RP}^0 = \\{e\\}\\).\n2.2 Real Projective plane \\(\\mathbb{RP}^2\\)\nLet \\(V = \\mathbb{R}^3\\), then: \\[\n\\mathbb{RP}^2 \\simeq \\frac{\\mathbb{S}^2}{\\mathbb{Z}_2} \\simeq \\frac{H^2}{\\sim} \\simeq \\frac{D^2}{\\sim}.\n\\tag{3}\\]\nVisually speaking,\n\n\n\n\n\n\nFigure 5: Visual demonstration of Equation 3\n\n\n\nNote that we cannot glue the opposite points on \\(H^2\\) nor \\(D^2\\) in \\(\\mathbb{R}^3\\) without a self-intersection. So we ended up at \\(D^2/{\\sim}\\). This structure is often expressed using fundamental polygon. Figure 6 [1] is what you get if you force to glue the opposite points on the equator of \\(H^2\\) together.\n\n\n\n\n\n\nFigure 6: \\(H^2/{\\sim}\\) immersed as a cross-cap in \\(\\mathbb{R}^3\\)\n\n\n\nInstead of let a line intersects the unit sphere at the origin (as shown in the first isomorphism in Equation 3), there are a different graphical representation of \\(\\mathbb{RP}^2\\):\n\n\n\n\n\n\nFigure 7: Infinity in \\(\\mathbb{RP}^2\\) is not unique\n\n\n\nThe lines in the \\(xy\\) plane in Figure 7 do not intersect the blue plane. We thus define them to “intersect with the circle at infinity”. The right figure of Figure 7 is NOT true since there is only one infinity, so it cannot tell the difference of the lines in the \\(xy\\) plane. So notably, \\[\n\\begin{aligned}\n\\mathbb{RP}^2 &\\simeq \\mathbb{R}^2 \\cup \\{\\text{circle at infinity}\\} \\\\\n&\\not\\simeq \\mathbb{R}^2 \\cup \\{\\infty\\}.\n\\end{aligned}\n\\]\n\n\n2.3 Real Projective space \\(\\mathbb{RP}^3\\)\nLet \\(V = \\mathbb{R}^4\\), one could prove that Equation 1 is still symbolically true: \\[\n\\mathbb{RP}^3 \\simeq \\frac{\\mathbb{S}^3}{\\mathbb{Z}_2} \\simeq \\frac{H^3}{\\sim} \\simeq \\frac{D^3}{\\sim}.\n\\tag{4}\\]\nHowever this time, the only object in Equation 4 that can be visualized is \\(D^3/{\\sim}\\):\n\n\n\n\n\n\nFigure 8: Every point on this solid ball (with antipodal points glued) represents an element in \\(\\mathbb{RP}^3\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbb{RP}^3, \\mathbb{S}^3, SO(3), SU(2)\\) and the unit quaternions \\(U(\\mathbb{H})\\)\n\n\n\n\n\nTheir relations are: \\[\n\\boxed{\n\\text{Spin}(3) \\simeq SU(2) \\simeq \\mathbb{S}^3 \\simeq U(\\mathbb{H}) \\overset{2:1} \\twoheadrightarrow \\mathbb{RP}^3 \\simeq SO(3).\n}\n\\]\n\n\\(SU(2) \\simeq \\mathbb{S}^3\\): Rotation in \\(\\mathbb{C}^2\\) sits bijectively on the surface of the 3-sphere \\(S^3\\).\n\n\n\n\n\n\n\n\nProof 1\n\n\n\n\n\nAn element in \\(SU(2)\\) has the form: \\[\nU = \\begin{pmatrix} \\alpha & \\gamma \\\\ \\beta & \\delta \\\\\\end{pmatrix} \\in SU(2)\n\\] with constraints: \\[\nU^H = U^{-1}, \\det U = 1.\n\\] Therefore, \\[\n\\begin{pmatrix} \\bar{\\alpha} & \\bar{\\beta} \\\\ \\bar{\\gamma} & \\bar{\\delta} \\\\\\end{pmatrix}\n=\n\\frac{1}{\\det U} \\begin{pmatrix} \\delta & -\\gamma \\\\ -\\beta & \\alpha \\\\\\end{pmatrix}\n=\n\\begin{pmatrix} \\delta & -\\gamma \\\\ -\\beta & \\alpha \\\\\\end{pmatrix}.\n\\] Thus, \\(U\\) must look like: \\[\n\\begin{align*}\n& U = \\begin{pmatrix} \\alpha & -\\beta \\\\ \\beta & \\bar{\\alpha} \\\\\\end{pmatrix}, \\text{ with } \\det U = 1 \\\\\n\\implies\\quad & \\alpha \\bar{\\alpha} + \\beta \\bar{\\beta} = 1 \\\\\n\\implies\\quad & |\\alpha|^2 + |\\beta|^2 = 1 \\\\\n\\implies\\quad & |a + bi|^2 + |c + di|^2 = 1 \\\\\n\\implies\\quad & a^2 + b^2 + c^2 + d^2 = 1.\n\\end{align*}\n\\] So, \\[\n\\forall U \\in SU(2), U \\in \\mathbb{S}^3.\n\\]\n\n\n\n\n\n\\(\\mathbb{S}^3 \\simeq U(\\mathbb{H})\\): Unit quaternions live on the 3-sphere.\n\n\n\n\n\n\n\n\nProof 2\n\n\n\n\n\nThis is easy to see from the definition of unit quaternions: \\[\n\\begin{aligned}\nU(\\mathbb{H}) &:= \\{ q \\in \\mathbb{H} : |q| = 1 \\} \\\\\n&\\simeq \\{ (a, b, c, d) \\in \\mathbb{R}^4 : a^2 + b^2 + c^2 + d^2 = 1 \\} \\\\\n&=: \\mathbb{S}^3\n\\end{aligned}\n\\] where \\(q = a + b\\mathbf{i} + c \\mathbf{j} + d \\mathbf{k}\\).\n\n\n\n\n\n\\(\\mathbb{RP}^3 \\simeq SO(3)\\): If you look around in \\(\\mathbb{R}^4\\), what you see is rotations in \\(\\mathbb{R}^3\\)!\n\n\n\n\n\n\n\n\nProof 3 (Not rigorous)\n\n\n\n\n\nAn element \\(R \\in SO(3)\\) is specified by the rotation axis and the rotation angle. We could encode these two information by a single vector in a solid ball of radius \\(\\pi\\), where the length of \\(V\\) is the rotation angle and the direction of \\(V\\) is the rotation axis. However, we soon realize that antipodal points on the surface of \\(D^3\\) represent the same rotation since rotating by \\(\\pi\\) does not care about the rotation direction. So we end up with the quotient space \\(D^3/{\\sim}\\) (shown in Figure 9).\n\n\n\n\n\n\nFigure 9: How elements in \\(SO(3)\\) live in \\(D^3/{\\sim}\\)\n\n\n\nBut Figure 9 is also the space of \\(\\mathbb{RP}^3\\), so we get: \\[\n\\mathbb{RP}^3 \\simeq SO(3).\n\\]\n\n\n\n\n\n\\(\\mathbb{S}^3 \\overset{2:1}{\\twoheadrightarrow} SO(3)\\): A rotation in \\(\\mathbb{R}^3\\) is represented by two points on the 3-sphere.\n\n\n\n\n\n\n\n\nProof 4\n\n\n\n\n\nThis is easy since we know: \\[\n\\mathbb{S}^3 \\overset{2:1}{\\twoheadrightarrow} \\frac{\\mathbb{S}^3}{\\mathbb{Z}_2},\n\\] and Equation 4 gives: \\[\n\\mathbb{RP}^3 \\simeq \\frac{\\mathbb{S}^3}{\\mathbb{Z}_2}.\n\\] We’ve just shown: \\[\n\\mathbb{RP}^3 \\simeq SO(3).\n\\]\n\n\n\n\n\n\\(\\text{Spin}(3) \\simeq \\mathbb{S}^3\\): Spin group double covers the rotation group.\n\n\n\n\n\n\n\n\nProof 5\n\n\n\n\n\n\\(\\text{Spin}(3)\\) could be defined to be the universal cover of \\(SO(3)\\), which is \\(\\mathbb{S}^3\\)."
  },
  {
    "objectID": "posts/projective-space/index.html#complex-projective-space-mathbbcpn",
    "href": "posts/projective-space/index.html#complex-projective-space-mathbbcpn",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "3 Complex Projective Space \\(\\mathbb{CP}^n\\)",
    "text": "3 Complex Projective Space \\(\\mathbb{CP}^n\\)\nWe don’t have a formula like Equation 1 for \\(\\mathbb{CP}^n\\) by the way. We will only focus on one case where \\(V = \\mathbb{C}^2\\).\n\n3.1 Complex projective line6 \\(\\mathbb{CP}^1\\)\nLet \\(V = \\mathbb{C}^2\\). Although it’s impossible to have a mental picture of \\(\\mathbb{C}^2\\), we could have a legit imagination for \\(\\mathbb{CP}^1\\) using homogeneous coordinates: \\[\n\\mathbb{C}^2 - \\{0\\} \\ni \\begin{pmatrix} z_1 \\\\ z_2 \\\\\\end{pmatrix}\n\\xmapsto{\\pi} \\left[\\frac{z_1}{z_2} : 1\\right]\n\\equiv \\left[a+bi : 1\\right]\n\\simeq \\mathbb{C}.\n\\tag{5}\\]\nAccording to Equation 5, one would need one single complex number \\(a+bi\\) to specify an element in \\(\\mathbb{CP}^1\\). Does it implies that \\(\\mathbb{CP}^1\\) is just the complex line7? No! We missed the case where \\(z_2 = 0\\)! Similar to Section 2.1, we define the line \\(\\begin{pmatrix} z_1 \\\\ 0 \\\\\\end{pmatrix} \\in \\mathbb{C}^2 - \\{0\\}\\) belongs to the equivalence class denoted by the symbol \\(\\infty\\): \\[\n\\begin{pmatrix} z_1 \\\\ 0 \\\\\\end{pmatrix}\n:\\xmapsto{\\pi} \\left[1 : 0\\right]\n\\equiv \\infty\n\\]\n7 \\(\\mathbb{C}\\) has different dimension as a real or complex vector space: \\[\\operatorname{dim}_{\\mathbb{R}} \\mathbb{C} = 2, \\operatorname{dim}_{\\mathbb{C}} \\mathbb{C} = 1.\\] We consider \\(\\mathbb{C}\\) as a complex vector space here, so we call it the complex line instead of the complex plane.Therefore, we would need one complex number plus only one extra symbol \\(\\infty\\) (together called the projectively extended complex number \\(\\mathbb{\\hat{C}}\\)) to specify an element in \\(\\mathbb{CP}^1\\): \\[\n\\mathbb{CP}^1 \\simeq \\mathbb{C} \\cup \\{\\infty\\} \\equiv \\mathbb{\\hat{C}}.\n\\]\nWe could also put a 2-sphere across or above \\(\\mathbb{\\hat{C}}\\) (as shown in Figure 10) and construct a bijective correspondence (called the Stereographic projection) between the points on the sphere and the points on \\(\\mathbb{\\hat{C}}\\). This sphere is a compact, connected, 1-dimensional complex manifold, called the Riemann sphere. We have the isomorphism: \\[\n\\mathbb{CP}^1 \\simeq \\mathbb{S}^2.\n\\]\n\n\n\n\n\n\nFigure 10: Two ways of stereographic projection both show that \\(\\mathbb{CP}^1 \\simeq \\mathbb{S}^2\\)\n\n\n\n\n6 The projective space for \\(V = \\mathbb{C}^1\\) is too trivial, since everything in \\(\\mathbb{C}\\) is in one equivalence class. Therefore, \\(\\mathbb{CP}^0 = \\{e\\}\\)."
  },
  {
    "objectID": "posts/projective-space/index.html#homography",
    "href": "posts/projective-space/index.html#homography",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "4 Homography",
    "text": "4 Homography\nWhenever mathematicians define a new object, they will soon talk about the morphisms between them, i.e., the mappings that preserve the structures of the objects. We are interested specifically in bijective morphisms between projective spaces, which are called homography. Just like the general linear group \\(\\text{GL}(V)\\), all homographies form a group called the projective linear group \\(\\text{PGL}(V)\\).\n\nHomography is an isomorphism between projective spaces induced by bijective linear transformations of vector spaces. \\[\\text{PGL}(V) \\equiv \\operatorname{Aut}(\\mathbb{P}(V)).\\]\n\nThe canonical projection \\(\\pi\\) in Section 1.3 not only project vector spaces but also the linear transformations between vector spaces. So it could be viewed as a functor from the category of vector spaces (v.s.) to the category of projective spaces (p.s.):\n\n\n\n\n\n\nFigure 11: Canonical projection \\(\\pi\\) as a functor\n\n\n\nWe first give two examples of homography in the case of \\(V \\in \\{\\mathbb{R}^2, \\mathbb{C}^2\\}\\) before dealing with the case of abstract \\(V\\).\n\n4.1 Homography on \\(\\mathbb{RP}^1\\)\nLet \\(V = \\mathbb{R}^2, A \\in \\text{GL}(V)\\), what is \\(A\\) under the functor \\(\\pi\\)?\n\n\n\n\n\n\nFigure 12: Homography on \\(\\mathbb{RP}^1\\)\n\n\n\n\\(A\\) could be represented by a \\(2 \\times 2\\) non-singular matrix multiplied by a non-zero vector \\(v\\): \\[\n\\begin{aligned}\nA: \\mathbb{R}^2 &\\xrightarrow[]{\\sim} \\mathbb{R}^2 \\\\\n\\begin{pmatrix} x_0 \\\\ y_0 \\\\\\end{pmatrix}\n&\\mapsto\n\\begin{pmatrix} a & c \\\\ b & d \\\\\\end{pmatrix}\n\\begin{pmatrix} x_0 \\\\ y_0 \\\\\\end{pmatrix}\\\\\n\\begin{pmatrix} x \\\\ 1 \\\\\\end{pmatrix}\n&\\mapsto\n\\begin{pmatrix} a & c \\\\ b & d \\\\\\end{pmatrix}\n\\begin{pmatrix} x \\\\ 1 \\\\\\end{pmatrix}\n=\n\\begin{pmatrix} ax + c \\\\ bx + d \\\\\\end{pmatrix}\n\\end{aligned}\n\\] where \\(x \\in \\mathbb{\\hat{R}}\\), \\(x_0, y_0, a, b, c, d \\in \\mathbb{R}\\) and \\(ad-bc \\neq 0\\). By definition, \\(\\pi(A)\\) should map the equivalence class \\([x : 1]\\) to \\([ax + c : bx + d]\\): \\[\n\\begin{aligned}\n\\pi(A): \\mathbb{RP}^1 &\\xrightarrow[]{\\sim} \\mathbb{RP}^1 \\\\\n[x : 1] &\\mapsto \\left[\\frac{ax+c}{bx+d} : 1\\right] \\\\\n\\end{aligned}\n\\tag{6}\\]\nNote that \\(\\pi(A)\\) maps \\(\\infty\\) to \\(\\frac{a}{b}\\).\n\n\n4.2 Möbius transformations – Homography on \\(\\mathbb{CP}^1\\)\n\nHomographies on \\(\\mathbb{CP}^1\\) are called Möbius transformations. The object \\(\\text{PGL}(\\mathbb{C}^2)\\) is called the Möbius group.\n\nLet \\(V = \\mathbb{C}^2, A \\in \\text{GL}(V)\\). We could simply write Equation 6 as its complex version: \\[\n\\begin{aligned}\n\\pi(A): \\mathbb{CP}^1 &\\xrightarrow[]{\\sim} \\mathbb{CP}^1 \\\\\n[z : 1] &\\mapsto \\left[\\frac{az+c}{bz+d} : 1\\right] \\\\\n\\end{aligned}\n\\] where \\(z \\in \\mathbb{\\hat{C}}\\), \\(a, b, c, d \\in \\mathbb{C}\\) and \\(ad-bc \\neq 0\\). Also, \\(\\pi(A)\\) maps \\(\\infty\\) to \\(\\frac{a}{b}\\).\nThere is an astounding fact [2] that the elements in the Möbius group correspond bijectively to the rigid motions of the Riemann sphere.\n\n\n\n\n\n\nFigure 13: Möbius transformations are rigid motions of the Riemann sphere [3]\n\n\n\nHowever, this does NOT mean that \\(\\text{PGL}(\\mathbb{C}^2) \\simeq SO(3)\\) as groups. In fact, one can show that the Möbius group is isomorphic to the so-called Lorenz group: \\[\n\\text{PGL}(\\mathbb{C}^2) \\simeq SO^+(1, 3).\n\\]"
  },
  {
    "objectID": "posts/projective-space/index.html#projective-linear-group",
    "href": "posts/projective-space/index.html#projective-linear-group",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "5 Projective Linear group",
    "text": "5 Projective Linear group\nThe projective space is a classification of vectors in a vector space. Similarly, we will define the projective linear group \\(\\text{PGL}(V)\\) as a classification of isomorphisms in the general linear group \\(\\text{GL}(V)\\) in this section.\n\n5.1 Definition\nIt’s obvious that some linear transformations are equivalent when we view them projectively. For example in Figure 14, a matrix \\(A \\in \\text{GL}(\\mathbb{R}^2)\\) looks the same as its scalar multiples \\(kA\\) for \\(k \\in \\mathbb{R}^*\\) under the functor \\(\\pi\\):\n\n\n\n\n\n\nFigure 14: A matrix is equivalent to its scalar multiples projectively\n\n\n\nIt turns out its converse is also true: All equivalent matrices are scalar multiples of each other! We have:\n\n\n\n\n\n\n\nEquivalent matrices \\(\\iff\\) Scalar multiples\n\n\n\n\nTheorem 1 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\) and \\(A, B \\in \\text{GL}(V)\\), then: \\[\n\\pi(A) = \\pi(B) \\iff \\exists \\lambda \\in \\mathbb{F}^*, A = \\lambda B.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof of Theorem 1\n\n\n\n\n\n\\((\\impliedby)\\) is trivial. We only show the direction \\((\\implies)\\) here:\n\\(\\pi(A) = \\pi(B)\\) implies: \\[\n\\forall v \\in V - \\{0\\}, \\pi(Av) = \\pi(Bv)\n\\]\nBy the definition of projective space, \\(Av\\) and \\(Bv\\) must be scalar multiple \\(\\lambda\\) of each other and that scalar may depend on \\(v\\) (We will show that \\(\\lambda\\) is actually independent of \\(v\\) later): \\[\n\\exists \\lambda(v) \\in \\mathbb{F}^*, Av = \\lambda(v)Bv.\n\\tag{7}\\]\nPick a basis \\(\\{e_1, e_2, \\cdots, e_n\\}\\) of \\(V\\) and take \\(v\\) to be each of the basis: \\[\nAe_i = \\lambda(e_i)Be_i, \\forall i = 1, 2, \\cdots, n.\n\\tag{8}\\]\n\\(\\forall v \\in V\\) is a linear combination of the basis: \\(v = \\sum \\alpha_i e_i\\). Multiply \\(A\\) both sides and from Equation 8, we have: \\[\n\\begin{aligned}\nAv &= \\sum \\alpha_i Ae_i = \\sum \\alpha_i \\underbrace{\\lambda(e_i) Be_i}_{Ae_i} \\\\\n&= \\sum \\lambda(e_i) \\alpha_i Be_i\n\\end{aligned}\n\\tag{9}\\]\nBut Equation 7 tells us: \\[\nAv = \\lambda(v)Bv = \\lambda(v) \\sum \\alpha_i Be_i.\n\\tag{10}\\]\nCompare Equation 9 and Equation 10, all \\(\\lambda\\) s must be the same: \\[\n\\lambda(v) = \\lambda(e_1) = \\lambda(e_2) = \\cdots = \\lambda(e_n) \\equiv \\lambda = \\text{const}.\n\\]\nTherefore, Equation 7 implies: \\[\n\\forall v \\in V - \\{0\\}, Av = \\lambda Bv \\implies A = \\lambda B.\n\\]\n\n\n\n\nWe can define the projective linear group \\(\\text{PGL}(V)\\) according to the classification of matrices “up to scalar”:\n\n\n\n\n\n\n\nDefinition of \\(\\text{PGL}(V)\\)\n\n\n\n\nDefinition 2 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\), then the projective linear group \\(\\text{PGL}(V)\\) is defined as: \\[\n\\text{PGL}(V) := \\frac{\\text{GL}(V)}{\\mathbb{F}^* I},\n\\] where \\(I\\) is the identity in \\(\\text{GL}(V)\\).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{PGL}(V)\\) as the inner automorphism group of \\(\\text{GL}(V)\\)\n\n\n\n\n\nWe know from here that the center of \\(\\text{GL}(V)\\) is exactly \\(\\mathbb{F}^* I\\): \\[\n\\text{ZGL}(V) \\equiv Z(\\text{GL}(V)) = \\mathbb{F}^* I.\n\\]\nWe know from this post that for any group \\(G\\), \\[\n1 \\xrightarrow[]{} Z(G) \\hookrightarrow G \\twoheadrightarrow \\operatorname{Inn}(G) \\xrightarrow[]{} 1,\n\\] where \\(\\text{Inn}(G)\\) = \\(G/{Z(G)}\\). Let \\(G = \\text{GL}(V)\\), then we have: \\[\n1 \\xrightarrow[]{} \\text{ZGL}(V) \\hookrightarrow \\text{GL}(V) \\twoheadrightarrow \\operatorname{PGL}(V) \\xrightarrow[]{} 1,\n\\] where \\[\n\\text{Inn}(\\text{GL}(V)) = \\frac{\\text{GL}(V)}{\\text{ZGL}(V)} = \\text{PGL}(V)\n\\]\n\n\n\n\n\n\n5.2 Grid of short exact sequence about projective linear groups\nThe general linear group \\(\\text{GL}(V)\\), special linear group \\(\\text{SL}(V)\\), their centers \\(\\text{ZGL}(V)\\) and \\(\\text{ZSL}(V)\\), and their projective linear groups \\(\\text{PGL}(V)\\) and \\(\\text{PSL}(V)\\) form the following grid of short exact sequence:\n\n\n\n\n\n\n\n\nFigure 15\n\n\n\nEach row and column in Figure 15 is a short exact sequence8. Everything would be clear if you stare at the cartoon in Figure 16 for a moment.\n8 \\((\\mathbb{C}^{\\times})^n\\) here means \\(\\{z^n: z \\in \\mathbb{C}^{\\times }\\}\\), not \\(\\mathbb{C}^{\\times} \\times \\cdots \\times \\mathbb{C}^{\\times}\\). \\(\\mathbb{C}^{\\times}\\) is the same as \\(\\mathbb{C}^*\\), which represents the multiplicative group of non-zero complex numbers.\n\n\n\n\n\nFigure 16: Illustration of the commutative diagram in Figure 15"
  },
  {
    "objectID": "posts/pde-derivation/index.html",
    "href": "posts/pde-derivation/index.html",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "",
    "text": "Conservation law seems underratedly important.\nOscillation and overshooting comes from second order time derivative. Smooth exponential decay comes from first order time derivative.\n\\(a_{n+1} + a_{n-1} - 2a_n\\) is the second order spatial derivative, the mental picture of it is the difference between current value and the average of its two neighbors.\nForcing terms are usually comes with second order spacial derivative. This is because first order spacial derivative only represent force from one side, and second order spacial derivative is the net force of the neighboring particles acting on the particle we analyze."
  },
  {
    "objectID": "posts/pde-derivation/index.html#takeaways",
    "href": "posts/pde-derivation/index.html#takeaways",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "",
    "text": "Conservation law seems underratedly important.\nOscillation and overshooting comes from second order time derivative. Smooth exponential decay comes from first order time derivative.\n\\(a_{n+1} + a_{n-1} - 2a_n\\) is the second order spatial derivative, the mental picture of it is the difference between current value and the average of its two neighbors.\nForcing terms are usually comes with second order spacial derivative. This is because first order spacial derivative only represent force from one side, and second order spacial derivative is the net force of the neighboring particles acting on the particle we analyze."
  },
  {
    "objectID": "posts/pde-derivation/index.html#welcome",
    "href": "posts/pde-derivation/index.html#welcome",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "1 Welcome!",
    "text": "1 Welcome!\nThis is the first chapter of my PDE series. You will learn how to derive two of the most important PDEs in physics: Wave equation and heat equation, with nearly no prerequisites. The goal of this article is to make these two equations as obvious as possible1 to you:\n1 We avoid using the notation \\(\\Delta\\) for \\(\\nabla^2\\). \\(\\Delta\\) in this article just mean “difference in” blablabla. We also use \\(\\xi_{t}\\) and \\(\\xi_{tt}\\) to represent \\(\\frac{\\partial \\xi}{\\partial t}\\) and \\(\\frac{\\partial^2 \\xi}{\\partial t^2}\\).\nWave equation: \\(\\xi_{tt} = c^2 \\nabla^2 \\xi\\).\nHeat equation: \\(T_t = \\alpha \\nabla^2 T\\).\n\nLet’s do it!"
  },
  {
    "objectID": "posts/pde-derivation/index.html#wave-equation",
    "href": "posts/pde-derivation/index.html#wave-equation",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "2 Wave Equation",
    "text": "2 Wave Equation\n\n2.1 1D Wave Equation\nThere are two kinds of one-dimensional wave: longitudinal and transverse.\n\n\n\nLongitudinal wave and transverse wave cartoon\n\n\nThey look totally different, but they share exactly the same properties and could be described by the same wave PDE! Let’s have a look at longitudinal wave first.\n\n2.1.1 Longitudinal Wave\nThis kind of wave can be considered as little balls connected by springs:\n\n\n\n\n\n\nFigure 1: Balls at equilibrium position\n\n\n\nAs shown in Figure 1, each small ball is in a tranquil state called “equilibrium position” where all resultant force acting on it is zero. Now we want to describe the system when there are perturbations using PDEs. So there should be a variable that depends on time and space. What is that variable? Well, we should define it. Let’s make it the displacement value \\(\\xi(x,t)\\) of each ball from its equilibrium position.\n\n\n\n\n\n\nFigure 2: Zoom in at one of the balls\n\n\n\nConsider the scenario shown in Figure 2, we zoom in at the \\(n\\)-th ball. In order to find the equation of motion for this ball, we need to define some parameters:\n\n\\(k\\): Microscopic string constant.\n\\(m\\): Mass of each ball.\n\nThe net force acting on the \\(n\\)-th ball is the sum of the forces from the two adjacent balls: \\[\n\\begin{aligned}\nF_n &= F_L + F_R \\\\\n&= (-F_0 - k(\\xi_n - \\xi_{n-1})) + (F_0 + k(\\xi_{n+1} - \\xi_n)) \\\\\n&= k((\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})),\n\\end{aligned}\n\\] where \\(F_0\\) is the force from both left and right balls acting on the \\(n\\)-th ball at equilibrium position (they balanced out at equilibrium position). According to Newton’s second law2 \\(F_n = m \\ddot \\xi_n\\), we have: \\[\nk((\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})) = m \\ddot \\xi_n.\n\\tag{1}\\]\n2 \\(\\ddot \\xi_n\\) means \\(\\frac{\\partial^2 \\xi}{\\partial t^2}\\) in the article.Now Equation 1 should be it! But in reality, we do not know \\(m\\) and \\(k\\) since they are microscopic parameters. We now define some new parameters to connect the quantities that we can measure with the microscopic parameters:\n\n\\(N\\): Total number of balls.\n\\(\\Delta l\\): Microscopic distance between adjacent balls.\n\\(L\\): Total length, \\(L = N \\Delta l\\).\n\\(M\\): Total mass of the system, \\(M = Nm\\).\n\\(\\rho\\): Mass density, \\(\\rho = \\frac{M}{L}\\).\n\\(K\\): Total spring constant3, \\(K = \\frac{k}{N}\\).\n\n3 The spring constant of serial spring is smaller than that of individual spring by a factor of \\(N\\).Now Equation 1 becomes: \\[\n\\begin{align*}\n&k \\frac{(\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})}{(\\Delta l)^2} = \\frac{\\rho \\ddot \\xi_n}{\\Delta l} \\\\\n\\implies\\quad &k \\xi_{xx} = \\frac{\\rho \\ddot \\xi_n}{\\Delta l} \\\\\n\\implies\\quad &\\ddot \\xi_n = \\frac{k \\Delta l}{\\rho} \\xi_{xx} = \\frac{NK \\frac{L}{N}}{\\frac{M}{L}} \\xi_{xx} \\\\\n\\implies\\quad &\\ddot \\xi_n = \\frac{KL^2}{M} \\xi_{xx}\n\\end{align*}\n\\]\n\\(n\\) is arbitrary, so we got: \\[\n\\boxed{\n\\ddot \\xi = \\frac{KL^2}{M} \\xi_{xx}.\n}\n\\]\n\n\n2.1.2 Transverse wave\nTo derive the wave equation for transverse wave, we need to make some basic assumptions:\n\nEvery particle could only move in a fixed line perpendicular to the direction of wave propagation, which means the horizontal resultant force is zero for each particle.\nThe vibration amplitude is extremely small.\nFollowing above, the tension (density) is constant throughout the string.\n\n\n\n\n\n\nOnly verticle movement of each particle\n\n\n\n\n\nVibration amplitude of each particle is small\n\n\n\n\n\nTension is asssumed to be invariant\n\n\nNow we define some parameters of the string:\n\n\\(T\\): Tension of the string.\n\\(\\lambda\\): Line density of the string (mass per unit length).\n\nWe focus on an element of the string shown in Figure 3.\n\n\n\n\n\n\nFigure 3: Line segment of the string\n\n\n\nSuppose the mass of that element is \\(\\Delta m\\), the displacement is \\(\\xi\\). The resultant force \\[\n\\begin{align*}\n& \\Sigma F = \\Delta m \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad & T \\sin \\theta_2 - T \\sin \\theta_1 = \\lambda \\frac{\\Delta x}{\\cos \\theta_1} \\cdot \\ddot{\\xi}\n\\end{align*}\n\\]\nSince \\(\\theta_1, \\theta_2 \\to 0, \\cos \\theta_1 \\approx 1, \\sin \\theta \\approx \\tan \\theta\\), we have: \\[\n\\begin{align*}\n&T(\\tan \\theta_2 - \\tan \\theta_1) = \\lambda \\Delta x \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &T \\left( \\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x+\\Delta x} - \\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x}  \\right) = \\lambda \\Delta x \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &T \\frac{\\partial^2 \\xi}{\\partial x^2} \\Delta x = \\lambda \\Delta x \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &\\frac{T}{\\lambda} \\xi_{xx} = \\ddot{\\xi}.\n\\end{align*}\n\\]\nDone! This is the wave equation for transverse wave. We can also see the famous result that the wave speed is the square root of tension divide by density: \\[\nv = \\sqrt{\\frac{T}{\\lambda}}.\n\\]\n\n\n\n\n\n\n\nSpatial Second order derivative\n\n\n\nIn the derivation of the two kinds of waves, the crucial part! is the realization that these two expressions: \\[\n(\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})\n\\tag{2}\\] and \\[\n\\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x+\\Delta x} - \\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x}\n\\] are essentially the second order spatial derivative \\(\\xi_{xx}\\). This is obvious if you recall the definition of second derivative: just the derivative of derivative. Or in discrete version, the difference of difference. In particular, Equation 2 can be also interpreted as the comparison between current value and the average of its two neighbors4: \\[\n(\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1}) \\propto \\frac{T_{n-1} + T_{n+1}}{2} - T_n.\n\\tag{3}\\]\nWe will encounter this in the derivation of heat equation as well, so keep this in mind!\n\n\n\n4 This incredible idea is inspired by this video by 3Blue1Brown.\n\n\n2.2 2D Wave Equation\nYou may think the 2D analog is something like Figure 4. However, this kind of wave is vector-valued just like electromagnetic wave, where the “amplitude” \\(\\mathbf{\\xi}: \\mathbb{R}^n \\times \\mathbb{R} \\to \\mathbb{R}^n\\) is a time-dependent vector field. We will focus on scalar-valued waves in this article, leaving vector-valued waves in the future discussion5. In fact, the transverse wave (shown in Figure 5) is quite accurate for 2D scalar-valued waves.\n5 For those of the curious reader, the wave equation \\(\\mathbf{\\xi}_{tt} = c^2 \\nabla^2 \\mathbf{\\xi}\\) is also true component-wise for vector-valued waves, i.e., \\((\\mathbf{\\xi}_i)_{tt} = c^2 \\nabla^2 \\mathbf{\\xi}_i\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Vector-valued 2D longitudinal wave\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Scalar-valued 2D transverse wave\n\n\n\n\n\n\nYou can imagine Figure 5 as a curved rubber band with similar assumptions in Section 2.1.2. Suppose:\n\n\\(T\\): Tension per unit length6.\n\\(\\sigma\\): Surface mass density of the rubber band (mass per unit area).\n\n6 Note that it doesn’t make sense to say “tension at a point” but “tension at both sides of a line of a given length” (unit: N/m).Along \\(z\\) axies, \\[\n\\begin{align*}\n&\\Sigma F = \\Delta m \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &(T \\Delta y) \\frac{\\partial^2 \\xi}{\\partial x^2} \\Delta x + (T \\Delta x) \\frac{\\partial^2 \\xi}{\\partial y^2} \\Delta y = \\sigma (\\Delta x \\Delta y) \\frac{\\partial^2 \\xi}{\\partial t^2} \\\\\n\\implies\\quad &\\frac{T}{\\sigma} (\\xi_{xx} + \\xi_{yy}) = \\ddot{\\xi}.\n\\end{align*}\n\\]\nThis can also be generalized to wave equation in \\(n\\) dimension: \\[\n\\boxed{\n\\ddot{\\xi} = \\frac{T}{\\sigma} \\nabla^2 \\xi. }\n\\]"
  },
  {
    "objectID": "posts/pde-derivation/index.html#heat-equation",
    "href": "posts/pde-derivation/index.html#heat-equation",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "3 Heat Equation",
    "text": "3 Heat Equation\n\n3.1 1D Heat Equation\n\n3.1.1 Modelling\nThe following physical model of heat conduction on a rod is based on the following assumptions and facts:\n\nNo thermal source on the thin rod.\n(Prop Relation 1) \\(T\\) (in Kelvin) is defined to be proportional7 to thermal energy \\(Q\\) (in \\(\\text{J}\\)), i.e., \\[\nT : \\propto Q.\n\\]\nThis is reflected in the following well-known fact: \\[\n\\Delta Q = cm \\Delta T,\n\\] where \\(c\\) is the specific heat capacity of the rod, \\(m\\) is the mass we analyze on.\n(Prop Relation 2) Conduction of heat energy is proportional to temperature difference8: \\[\n\\boxed{\nq = -k \\nabla T,}\n\\] where \\(q\\) is called heat flux density9, the amount of thermal energy passing through a point (1D case) or a unit area (3D case)\nThermal energy in conserved!\n\n\n7 Loosely speaking, this is the definition of temperature. If there is no temperature (\\(T = 0\\text{ K}\\)), then there is no particle motion, \\(Q = 0\\).8 This famous equation is called Fourier’s law for heat conduction.9 Since in 1D case, \\(q\\) is just the amount of thermal energy pssing through a point. Thus in 1D, we often drop the word “density” by just saying “heat flux”.\n\n\n\n\n\nImportance of proportion relations\n\n\n\nYou may have noticed there are two proportional relations: \\[\n    T : \\propto Q.\n\\] This allows us to use temperature to reflect energy! Temperature is more tangible than energy. \\[\n    q = -k \\nabla T.\n\\] This connects time derivative and space derivative. So Heat equation is actully easier to derive since this Fourier’s law directly gives us nearly the final result. I say nearly because the only thing left is the conservation of energy!\n\n\n\n\n\n3.1.2 Derivation\nThe notations used are defined below:\n\n\\(\\lambda\\): Line density of the rod (mass per unit length).\n\\(x, x + \\Delta x\\): The coordinates of the start and end points of the rod segment we analyze (Right is the positive direction).\n\\(T: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\): Temperature field (time dependent) on the rod.\n\nThe quantity we care is obviously how the temperature changes over time and space. We chooce a tiny segment of the rod of length \\(\\Delta x\\) and build equation on it. The temperature on that extremely small segment can be viewed as a constant.\n\n\n\n\n\n\nFigure 6: Energy conservation on a small element of the rod\n\n\n\nAs shown in Figure 6, suppose the left side of the rod is hotter than its right side10. We know that the thermal energy increased in \\(\\mathrm{d}t\\) should equal to thermal energy entering the segment on the left minus thermal energy leaving the segment on the right: \\[\n\\frac{\\mathrm{d} Q}{\\mathrm{d} t} = q_{\\text{in, left}} - q_{\\text{out, right}}.\n\\tag{4}\\]\n10 This is an assumption without loss of generality in terms of writing out the heat equation. The signs can be a lot tricker to determine if you don’t have a certain physical picture.Now we expect \\(q_{\\text{in, left}}\\) and \\(q_{\\text{out, right}}\\) to be both positive. The gradient of temperature \\(\\frac{\\partial T}{\\partial x}\\) on position \\(x\\) and \\(x + \\Delta x\\) are both negative (since temperature decreases along the positive direction), therefore, \\[\n\\begin{aligned}\nq_{\\text{in, left}} &= \\left. -k \\frac{\\partial T}{\\partial x} \\right|_{x} &gt; 0 \\\\\nq_{\\text{out, right}} &= \\left. -k \\frac{\\partial T}{\\partial x} \\right|_{x + \\Delta x} &gt; 0.\n\\end{aligned}\n\\]\nTherefore, Equation 4 becomes: \\[\n\\begin{align*}\n& \\frac{\\mathrm{d} Q}{\\mathrm{d} t} = k (T_x|_{x+\\Delta x} - T_x|_{x}) \\\\\n\\implies\\quad & c (\\lambda \\Delta x) \\frac{\\partial T}{\\partial t} = k T_{xx} \\Delta x \\\\\n\\implies\\quad & T_t = \\frac{k}{c \\lambda} T_{xx}.\n\\end{align*}\n\\]\nJust this simple, we derived the heat equation! \\[\n\\boxed{\nT_t = \\frac{k}{c \\lambda} T_{xx}.}\n\\]\n\n\n\n\n\n\n\nWhy do wave equation and heat equation so similar and different?\n\n\n\n\nWhy do they both have \\(\\xi_{xx}\\) term?\nThe “forcing” on a small element is proportional to the change of changes. This is very common in nature.\n\nIn wave case, the force on each particle not depend on the change of displacement of adjacent particles, but the change of the change (because first order determines force on either side, second order determines the net force on the particle we analyze)!\nIn heat case, the heat increase in a small segment does not depend on first order derivative of temperature field, that’s just one side. There’s yet another side that may compensate for this change in temperature. So it’s the difference of difference that matters.\n\nWhy temperature only cares first order time derivative?\nThere’s a fundamental difference between wave equation and heat equation: Heat will never overshoot but wave will. In other words, a spring will overshoot its equilibrium with maximal speed, resulting a movement of back and forth. But heat will evolve as smooth and efficient as possible, never oscillating.\n\n\n\n\n\n\n\n\n\n\nHeat diffusion\n\n\n\n\n\n\n\nWave propagation11\n\n\n\n\n\n\nA second order derivative in time means overshooting.\n\nSecond order time derivative appears in wave equation is because Newton’s second law involves acceleration. Whereas in heat equation the first order time derivative actually comes from conservation laws. There is a wide class of PDEs (called continuity equation) that can be derived from conservation law. We will discuss this later.\nJust to give a intuitive understanding of the difference behavior of wave and heat, recall that in Equation 3 we introduced a new way of looking at second order spatial derivative (Difference to the average of its neighbors). As shown in Figure 7, the second order spatial derivative is proportional to the blue \\(d\\).\n\n\n\n\n\n\nFigure 7: Difference between \\(d \\propto a\\) and \\(d\\ propto v\\)\n\n\n\nThe only difference in wave and heat equation is that if \\(d\\) is proportional to the acceleration \\(a\\) (of the middle particle), or its velocity \\(v\\), i.e., \\[\n\\dot{\\xi} = \\xi\n\\tag{5}\\] or \\[\n\\ddot{\\xi} = \\xi.\n\\tag{6}\\]\nThe solution to Equation 5 is the famous exponential decay, whereas the solution to Equation 6 is obviously the ocsillation!\n\n\n\n11 Made with this website where you can explore different PDEs visually!\n\n\n3.2 3D Heat Equation\nNow substitude the rod with a 3D material like metal. Suppose\n\n\\(\\rho\\): Volume density of the rod (mass per unit volume).\n\\(\\Delta V\\): Volume of the small segment we analyze, \\(\\Delta V = \\Delta x \\Delta y \\Delta z\\).\n\\(T: \\mathbb{R}^3 \\times \\mathbb{R} \\to \\mathbb{R}\\): Temperature field (time dependent) on the rod.\n\n\n\n\nIsolate a small sube inside the metal\n\n\nFrom conservation of thermal energy, the increase of \\(Q\\) is unit time equal to net energy entering the volume from \\(x, y, z\\) directions. We already know that in one dimension, say \\(x\\), the net increase in thermal energy is proportional to the second spatial derivative times the nudge length: \\[\nq = k T_{xx} \\overbrace{\\Delta x}^{\\text{nudge}}.\n\\]\nHowever, \\(q\\) is three-dimension is the heat flux density, so we also need to multiply the cross section area: \\[\n\\frac{\\mathrm{d} Q}{\\mathrm{d} t} = q \\cdot \\Delta \\text{Area} = \\overbrace{k T_{xx} \\Delta x}^{\\text{density}} \\cdot \\overbrace{(\\Delta y \\Delta z)}^{\\text{area size}}.\n\\]\nSum up all heat contributions from three directions: \\[\n\\begin{align*}\n& \\Sigma \\frac{\\mathrm{d} Q}{\\mathrm{d} t} = q_x (\\Delta y \\Delta z) + q_y (\\Delta z \\Delta x) + q_z (\\Delta x \\Delta y) \\\\\n\\implies\\quad & c (\\rho \\Delta V) \\frac{\\partial T}{\\partial t} = (k T_{xx} \\Delta x)(\\Delta y \\Delta z) + (k T_{yy} \\Delta y)(\\Delta z \\Delta x) + (k T_{zz} \\Delta z)(\\Delta x \\Delta y) \\\\\n\\implies\\quad & T_t = \\frac{k}{c \\rho} (T_{xx} + T_{yy} + T_{zz}),\n\\end{align*}\n\\] i.e., \\[\n\\boxed{\nT_t = \\frac{k}{c \\rho} \\nabla^2 T.}\n\\]\nThis could be natually generalized to \\(n\\) dimensions."
  },
  {
    "objectID": "posts/pde-derivation/index.html#references",
    "href": "posts/pde-derivation/index.html#references",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "References",
    "text": "References\n\n\nLamoureux, Michael. 2006. “The Mathematics of PDEs and the Wave Equation.” https://mathtube.org/sites/default/files/lecture-notes/Lamoureux_Michael.pdf."
  },
  {
    "objectID": "posts/jk-ff/index.html",
    "href": "posts/jk-ff/index.html",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "",
    "text": "Most online VHDL descriptions of JK flip-flops (FF) are based on “processes” or circuit functionality (behavioral). Is it possible to simulate them only by constructing the circuit structure of the JK flip-flop?"
  },
  {
    "objectID": "posts/jk-ff/index.html#intro",
    "href": "posts/jk-ff/index.html#intro",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "",
    "text": "Most online VHDL descriptions of JK flip-flops (FF) are based on “processes” or circuit functionality (behavioral). Is it possible to simulate them only by constructing the circuit structure of the JK flip-flop?"
  },
  {
    "objectID": "posts/jk-ff/index.html#jk-ff-review",
    "href": "posts/jk-ff/index.html#jk-ff-review",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "2 JK FF Review",
    "text": "2 JK FF Review\nThe circuit structure is JK FF is very familiar to everybody, which is: \nThe corresponding truth table is shown below:\n\n\n\nC\nJ\nK\nQ\nQ̅\n\n\n\n\n↑\n0\n0\nlatch\nlatch\n\n\n↑\n0\n1\n0\n1\n\n\n↑\n1\n0\n1\n0\n\n\n↑\n1\n1\ntoggle\ntoggle\n\n\nx\n0\n0\nlatch\nlatch\n\n\nx\n0\n1\nlatch\nlatch\n\n\nx\n1\n0\nlatch\nlatch\n\n\nx\n1\n1\nlatch\nlatch\n\n\n\nwhere “latch” represent the Q output remembers whatever the last stored value was. “Toggle” means to flip Q, i.e. 0 -&gt; 1, 1 -&gt; 0. “↑” means the clock signal in a leading edge."
  },
  {
    "objectID": "posts/jk-ff/index.html#problem",
    "href": "posts/jk-ff/index.html#problem",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "3 Problem",
    "text": "3 Problem\n\n3.1 First try\nWrite the following content in JKFF.vhdl:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity JKFF_nodelay is\n    Port (\n        J : in STD_LOGIC;     -- J input\n        K : in STD_LOGIC;     -- K input\n        clk : in STD_LOGIC;   -- Clock input\n        Q : out STD_LOGIC;    -- Output Q\n        QN : out STD_LOGIC    -- Output QN (complement of Q)\n    );\nend JKFF_nodelay;\n\narchitecture Structural of JKFF_nodelay is\n    -- Internal signals for latch and clock gating\n    signal S, R : STD_LOGIC;    -- Set and Reset inputs for the latch\n    signal Q_int, QN_int : STD_LOGIC := '0'; -- Internal Q and QN for feedback\n\nbegin\n    S &lt;= (J and clk) and QN_int;\n    R &lt;= (K and clk) and Q_int;\n\n    -- NOR gate-based latch\n    Q_int &lt;= not (R or QN_int);\n    QN_int &lt;= not (S or Q_int);\n\n    -- Outputs\n    Q &lt;= Q_int;    -- Main output\n    QN &lt;= QN_int;  -- Complementary output\nend Structural;\nThen write a testbench file to test the behavior of this circuit, you will find it will not work well.\n\n\n3.2 Reason\nThe reason (probably, only personal view) is that JK FF have two layers of feedback (instead of SR FF, which just have one), since we loop our output Q not only to the NOR gates, but also AND to our initial inputs J and K. This confuse the compiler because the resultant signal changes so fast and maybe not have a stable consequence, so the compiler do not know how to respond to this kind of feedback.\nSo we introduce some delay in the gates to simulate the reality closer."
  },
  {
    "objectID": "posts/jk-ff/index.html#solution",
    "href": "posts/jk-ff/index.html#solution",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "4 Solution",
    "text": "4 Solution\n\n4.1 Adding delay in the gates\nWe write the following content in another file JKFF.vhdl:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity JKFF is\n    Port (\n        J : in STD_LOGIC;     -- J input\n        K : in STD_LOGIC;     -- K input\n        clk : in STD_LOGIC;   -- Clock input\n        Q : out STD_LOGIC;    -- Output Q\n        QN : out STD_LOGIC    -- Output QN (complement of Q)\n    );\nend JKFF;\n\narchitecture Structural of JKFF is\n    -- Internal signals for latch and clock gating\n    signal S, R : STD_LOGIC;    -- Set and Reset inputs for the latch\n    signal Q_int, QN_int : STD_LOGIC := '0'; -- Internal Q and QN for feedback\n\nbegin\n    S &lt;= (J and clk) and QN_int after 0.1 ns;\n    R &lt;= (K and clk) and Q_int after 0.1 ns;\n\n    -- NOR gate-based latch\n    Q_int &lt;= not (R or QN_int) after 0.2 ns;\n    QN_int &lt;= not (S or Q_int) after 0.2 ns;\n\n    -- Outputs\n    Q &lt;= Q_int;    -- Main output\n    QN &lt;= QN_int;  -- Complementary output\nend Structural;\nAnd run the following JKFF_tb.vhdl:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity JKFF_tb is\n-- No ports for testbench\nend JKFF_tb;\n\narchitecture Behavioral of JKFF_tb is\n    -- Component declaration\n    component JKFF\n        Port (\n            J : in STD_LOGIC;\n            K : in STD_LOGIC;\n            clk : in STD_LOGIC;\n            Q : out STD_LOGIC;\n            QN : out STD_LOGIC\n        );\n    end component;\n\n    -- Signals to connect to the JKFF\n    signal J, K, clk : STD_LOGIC := '0';\n    signal Q, QN : STD_LOGIC;\n\nbegin\n    -- Instantiate the JK Flip-Flop\n    uut: JKFF\n        Port Map (\n            J =&gt; J,\n            K =&gt; K,\n            clk =&gt; clk,\n            Q =&gt; Q,\n            QN =&gt; QN\n        );\n\n    -- Clock generation process\n    clk_gen: process\n    begin\n        for i in 0 to 9 loop -- Generate 10 clock cycles\n            clk &lt;= '0';\n            wait for 9 ns; -- Low for 5 ns\n            clk &lt;= '1';\n            wait for 1 ns; -- High for 5 ns\n        end loop;\n        wait; -- End simulation after clock finishes\n    end process;\n\n    -- Stimulus process to apply test cases\n    stimulus: process\n    begin\n        -- Test Case 1: Hold state (J = 0, K = 0)\n        J &lt;= '0'; K &lt;= '0';\n        wait for 20 ns;\n\n        -- Test Case 2: Set state (J = 1, K = 0)\n        J &lt;= '1'; K &lt;= '0';\n        wait for 20 ns;\n\n        -- Test Case 3: Reset state (J = 0, K = 1)\n        J &lt;= '0'; K &lt;= '1';\n        wait for 20 ns;\n\n        -- Test Case 4: Toggle state (J = 1, K = 1)\n        J &lt;= '1'; K &lt;= '1';\n        wait for 40 ns;\n\n        -- Return to Hold state\n        J &lt;= '0'; K &lt;= '0';\n        wait for 20 ns;\n\n        wait; -- End simulation\n    end process;\nend Behavioral;\n\n\n4.2 Results\nWe will get the following waveform:\n\nThere are several strange things happen here:\n\n4.2.1 What happen before around 30 ns?\nThe Q and QN oscillates at the same pace. Why? It’s because both J and K are zero. For a JK FF, this means to remember the last value. But the last value of Q and QN are both zero (we initialize them in the JKFF.vhdl file):\nsignal Q_int, QN_int : STD_LOGIC := '0'; -- Internal Q and QN for feedback\nThis is invalid and unstable! So they oscillates with a period of 0.2 ns, which is exactly the delay time of the NOR gates.\n\n\n\nLocally zoomed in\n\n\nOK, if instead we initialize the Q and QN a valid value, say Q_int=0 and QN_int=1 like this (in JKFF.vhdl file):\nsignal Q_int : STD_LOGIC := '0'; -- Internal Q for feedback\nsignal QN_int : STD_LOGIC := '1'; -- Internal QN for feedback\nSince they are valid, hence stable, Q and QN will not oscillates as expected:\n\n\n\n4.2.2 Why not toggle successfully?\nAt around \\(t = 70\\) ns, both J and K are 1. This means at the leading edge of the clock signal clk, Q and QN should both flipped! But according to the waveform, they tried but failed, with a tiny pulse around that time.\nI tried several clk pulse width and analysed the JK circuit in “slow-motion” carefully (Try this! Very surprising!). Finally I figured it out:\nIt’s because JK FF don’t want the clk signal be high for too long. This is because if the clk line hold high for a sufficient long period, the signal at Q and QN will “backpropagate” (Haha just borrow the term) to the inputs, continue to control whether or not the J and K signal should come in. If we increase the so-called “duty-ratio” of the clk signal, we will see these:\n\n\n\nDuty ratio = 0.20\n\n\n\n\n\nDuty ratio = 0.30\n\n\n\n\n\nDuty ratio = 0.40\n\n\n\n\n\nDuty ratio = 0.65\n\n\nSome value of duty ratio (e.g. 0.65) happen to toggle the Q successfully, while others do not.\nYou can think of what value could the duty ratio be? (Given the clock cycle and the propagation delay of all gates) This is a very intereting yet tedious problem to consider. But as long as you understand why Q oscillates, you understand this.\nTherefore people say that there are no “JK latches”."
  },
  {
    "objectID": "posts/jk-ff/index.html#conclusion",
    "href": "posts/jk-ff/index.html#conclusion",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThe VHDL realization of a JK FF can be achieved by introducing propagation delay to the gates."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html",
    "href": "posts/isomorphism-theorem/index.html",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "",
    "text": "Prerequisites: Quotient group, kernel of a homomorphism, normal subgroup, basic category theory.\nApologize for the handwriting, since it’s hard for quarto to support tikz-cd package."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#takeaways",
    "href": "posts/isomorphism-theorem/index.html#takeaways",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "1 Takeaways",
    "text": "1 Takeaways\n\nYou gotta think of a homomorphism whenever there is a quotient.\nHomomorphisms are WAAAAY more important than groups themselves!\nFirst isomorphism theorem: Universal property of quotients, most important.\nSecond isomorphism theorem: How subgroups behave under projections.\nThird isomorphism theorem: Just a diagram commutes."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#normal-subgroup-iff-kernel",
    "href": "posts/isomorphism-theorem/index.html#normal-subgroup-iff-kernel",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "2 Normal Subgroup \\(\\iff\\) Kernel",
    "text": "2 Normal Subgroup \\(\\iff\\) Kernel\nIn this section, I will introduce to you a very important mathematical habit: always think of a homomorphism whenever there is a quotient. This helps me a lot in not only understanding the three isomorphism theorems, but also in various topics such as projective geometry, cohomology, etc. First, let’s look at some concepts, if you are familiar with them, you can skip this section.\n\n2.1 Center and Centralizer\nThere are two very similar concepts: center of a group and conjugation operation. Center can be generalized to the concept of a centralizer. Conjugation is related to the concept of a normal subgroup and normalizer.\nWe know that not every group is abelian, but there are some elements in a group that commute with every other element in the group. We collect them to form a set. In fact, not only a set, but also a subgroup, called the center of the group.\n\n\n\n\n\n\n\nCenter\n\n\n\n\nDefinition 1 The center \\(Z(G)\\) of a group \\(G\\) is: \\[\n\\begin{aligned}\nZ(G) :=& \\{ z \\in G \\mid \\forall g \\in G, zg = gz \\} \\\\\n=& \\{ z \\in G \\mid \\forall g \\in G, z = gzg^{-1} \\}.\n\\end{aligned}\n\\]\n\n\n\n\nHere are some properties of the center, you can think about them yourself:\n\n\\(Z(G) \\trianglelefteq G\\). (See Definition 4 for the definition of normal subgroup.)\n\\(Z(G) = G\\) iff \\(G\\) is abelian.\n\\(Z(G)\\) is itself abelian.\n\nIf we do not want to make it commutes with every element in the group, but only with some elements in a set \\(S\\) (not necessarily a subgroup1 of \\(G\\)), this generalize to the definition of the centralizer of \\(S\\) in \\(G\\):\n\n1 \\(S \\le G\\) means \\(S\\) is a subgroup of \\(G\\). \\(S \\subseteq G\\) means \\(S\\) is a subset of \\(G\\).\n\n\n\n\n\nCentralizer\n\n\n\n\nDefinition 2 The centralizer \\(C_G(S)\\) of a set \\(S\\) \\((S \\subseteq G)\\) of group \\(G\\) is: \\[\n\\begin{aligned}\nC_G(S) :=& \\{ c \\in G \\mid \\forall s \\in S, cs = sc \\} \\\\\n=& \\{ c \\in G \\mid \\forall s \\in S, c = scs^{-1} \\}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof: \\(C_G(S) \\le G\\)\n\n\n\n\n\n\nClosure:\nLet \\(a, b \\in C_G(S)\\), i.e., \\(\\forall s \\in S, as = sa, bs = sb\\). We want to show that \\(ab \\in C_G(S)\\): \\[\n(ab)s = a(bs) = a(sb) = (as)b = (sa)b = s(ab) \\implies ab \\in C_G(S), \\forall s \\in S.\n\\]\nAssociativity: Inherited from \\(G\\).\nIdentity: \\(e_G \\in C_G(S)\\) because \\(\\forall s \\in S, es = se = s\\).\nInverse: Let \\(a \\in C_G(S)\\), i.e., \\(\\forall s \\in S, as = sa\\). We want to show that \\(a^{-1} \\in C_G(S)\\): \\[\n\\begin{aligned}\nas = sa & \\implies a^{-1}as = a^{-1}sa \\\\\n& \\implies s = a^{-1}sa \\\\\n& \\implies sa^{-1} = a^{-1}saa^{-1} \\\\\n& \\implies sa^{-1} = a^{-1}s \\\\\n& \\implies a^{-1} \\in C_G(S).\n\\end{aligned}\n\\]\n\n\n\n\n\nClearly, the center is a special case of the centralizer: \\[\nZ(G) = C_G(G).\n\\]\n\n\n2.2 Conjugation\nThe sandwich operation \\(gzg^{-1}\\) in Definition 1 pops out frequenctly in math2. We give it a name: conjugation of \\(z\\) by \\(g\\).\n\n2 Similar matrices, quarternion representation of 3D rotation, etc.\n\n\n\n\n\nConjugation\n\n\n\n\nDefinition 3 \\(a, b \\in G\\) conjugate \\(:\\iff\\) \\(\\exists g \\in G\\) such that \\(a = gbg^{-1}\\).\n\n\n\n\nIt’s easy to show that conjugation defines an equivalence relation on \\(G\\):\n\n\n\n\n\n\n\nProof: Conjugation is an Equivalence Relation\n\n\n\n\n\nTo show conjugation defined in Definition 3 is an equivalence relation \\(\\sim\\), we need to show it is reflexive, symmetric, and transitive.\n\nReflexive: \\(\\exists e \\in G, a = eae^{-1}\\), hence \\(a \\sim a\\).\nSymmetric: \\[\n\\begin{aligned}\na \\sim b & \\iff \\exists g \\in G, a = gbg^{-1} \\\\\n& \\iff \\exists g^{-1} \\in G, b = g^{-1}ag \\\\\n& \\iff b \\sim a.\n\\end{aligned}\n\\]\nTransitive: \\[\n\\begin{aligned}\na \\sim b, b \\sim c & \\iff \\exists g, h \\in G, a = gbg^{-1}, b = hch^{-1} \\\\\n& \\iff \\exists g, h \\in G, a = ghch^{-1}g^{-1} \\\\\n& \\iff \\exists gh \\in G, a = (gh)c(gh)^{-1} \\\\\n& \\iff a \\sim c.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n2.3 A Natural Homomorphism\n\n\n\n\n\n\n\nConjugation Homomorphism\n\n\n\n\nProposition 1 Let \\(G\\) be a group and \\(f \\in \\operatorname{Hom}(G, \\operatorname{Aut}(G))\\), where \\(f(g)\\) is defined by conjugation by \\(g\\). Then \\[\n\\ker f = Z(G).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof of Proposition 1\n\n\n\n\n\n\nConjugation by \\(g\\) (denoted \\(\\operatorname{conj}_g\\)) is in \\(\\operatorname{Aut}(G)\\).\nWe need to check that \\(\\operatorname{conj}_g\\) gives an isomorphism from \\(G\\) to itself.\n\nClaim: \\(\\forall g \\in G, f(g) \\in \\operatorname{End}(G)\\).\n\\[\n  f(g)(h h') = g(h h')g^{-1} = ghg^{-1}gh'g^{-1} = f(g)(h) f(g)(h').\n  \\]\nClaim: \\(f(g)\\) is bijective.\n\\[\n  f^{-1}(g) = f(g^{-1}).\n  \\]\n\n\\(f\\) is indeed a homomorphism.\nTake \\(\\forall g, g' \\in G\\), then: \\[\n\\begin{aligned}\nf(g g')(h) &= gg'h(gg')^{-1} \\\\\n&= gg'hg'^{-1}g^{-1} \\\\\n&= f(g) \\circ f(g')(h).\n\\end{aligned}\n\\]\n\\(\\ker f = Z(G)\\).\n\\[\n\\begin{aligned}\n\\ker f &= \\{g \\in G : \\forall h \\in G, ghg^{-1} = h\\} \\\\\n&= \\{g \\in G : gh = hg \\text{ for all } h \\in G\\} \\\\\n&= Z(G).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nInner and Outer Automorphisms\n\n\n\n\nProposition 2 Under the condition in Proposition 1, \\[\n\\operatorname{im} f \\triangleleft \\operatorname{Aut}(G).\n\\]\nDefine the inner automorphism group to be: \\[\n\\operatorname{Inn}(G) := \\operatorname{im} f,\n\\] and naturally the Outer automorphism group to be: \\[\n\\operatorname{Out}(G) := \\frac{\\operatorname{Aut}(G)}{\\operatorname{Inn}(G)}.\n\\] Then we have the following exact sequence: \\[\n1 \\xrightarrow[]{} Z(G) \\hookrightarrow G \\xrightarrow[]{f} \\operatorname{Aut}(G) \\twoheadrightarrow \\operatorname{Out}(G) \\xrightarrow[]{} 1.\n\\]\n\n\n\n\nWe omitted the proof, but visually,\n\n\n\n\n\n\nFigure 1: Relation between \\(Z(G)\\), \\(\\operatorname{Inn}(G)\\), \\(\\operatorname{Out}(G)\\)\n\n\n\n\n\n2.4 Normal Subgroup and Normalizer\nIn fact, we could weaken the condition of \\(z = gzg^{-1}\\) in Definition 1 and \\(c = scs^{-1}\\) in Definition 2 to \\(N = gNg^{-1}\\) and \\(S = gSg^{-1}\\) to get the definition of a normal subgroup \\(N\\) and a normalizer \\(N_G(S)\\) respectively. This is different, because we no longer need pointwise commutativity but only commutativity as a set. In other words, the element after conjugation does not need to be exactly itself necessarily as long as it remains in some set.\n\n\n\n\n\n\n\nNormal Subgroup\n\n\n\n\nDefinition 4 A subgroup \\(N\\) of a group \\(G\\) is normal (denoted \\(N \\trianglelefteq G\\)), iff \\(\\forall g \\in G, N = gNg^{-1}\\), i.e., \\[\n\\forall g \\in G, \\exists n \\in N, \\forall m \\in N, gmg^{-1} = n,\n\\] i.e., normal subgroups are invariant under any conjugation.\n\n\n\n\n\n\n\n\n\n\n\nNormalizer\n\n\n\n\nDefinition 5 The normalizer \\(N_G(S)\\) of a set \\(S\\) \\((S \\subseteq G)\\) of group \\(G\\) is: \\[\n\\begin{aligned}\nN_G(S) :=& \\{ g \\in G \\mid gS = Sg \\} \\\\\n=& \\{ g \\in G \\mid S = gSg^{-1} \\}.\n\\end{aligned}\n\\]\n\n\n\n\nTheir relationship is: If \\(S \\trianglelefteq G\\), then \\(N_G(S) = G\\).\nThe following two results are important:\n\n\n2.5 Kernels are normal\n\n\n\n\n\n\n\nKernel of a homomorphism is normal\n\n\n\n\nLemma 1 Let \\(G, H\\) be groups and \\(\\varphi \\in \\operatorname{Hom}(G, H)\\). Then \\(\\ker(\\varphi) \\trianglelefteq G\\), \\(\\operatorname{im}(\\varphi) \\le H\\).\n\n\n\n\n\n\n\n\n\n\n\nProof: Kernel of a homomorphism is normal\n\n\n\n\n\nWe just need to show that \\(\\forall k \\in \\ker(\\varphi)\\), the conjugate of \\(k (gkg^{-1})\\) is also in \\(\\ker(\\varphi)\\), i.e., \\[\n\\varphi(gkg^{-1}) = e.\n\\] This is trivial.\n\n\n\n\n\n\n2.6 Normal subgroups are kernels\n\n\n\n\n\n\n\nEvery normal subgroup is the kernel of a homomorphism\n\n\n\n\nLemma 2 Let \\(N \\trianglelefteq G\\). Then \\(\\exists \\varphi \\in \\operatorname{Hom}(G, H)\\) s.t. \\(N = \\ker(\\varphi)\\).\n\n\n\n\nHINT: Take \\(\\varphi: G \\to G/N\\), the natural projection from \\(G\\) to the group of cosets (called the quotient group \\(G/N\\)).\n\n\n\n\n\n\nFigure 2: All homomorphisms factors uniquely through the quotient\n\n\n\nAs shown in Figure 2, any group homomorphism \\(\\varphi: G \\to H\\) factors uniquely through the quotient group \\(G/N\\), where \\(N = \\ker(\\varphi)\\). The unique embedding \\(\\bar{\\varphi}\\) implies the universal property of the quotient group (the pair \\((G/N, \\pi)\\) could be viewed as a initial object of a coslice category3).\n3 A coslice category is the dual of a slice category, which is a special case of a comma category.So we have establish the fact that quotients and group homomorphisms are equivalent!"
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#isomorphism-theorem-i",
    "href": "posts/isomorphism-theorem/index.html#isomorphism-theorem-i",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "3 Isomorphism Theorem I",
    "text": "3 Isomorphism Theorem I\nFigure 2 could be viewed as part of the canonical decomposition of a group homomorphism:\n\n\n\n\n\n\nFigure 3: The red box in the canonical decomposition is exactly the first isomorphism theorem\n\n\n\nThe red box in Figure 3 is exactly the first isomorphism theorem:\n\n\n\n\n\n\n\nFirst Isomorphism Theorem\n\n\n\n\nTheorem 1 Let \\(G\\) and \\(H\\) be groups and let \\(\\varphi: G \\to H\\) be a homomorphism. Then: \\[\nG/\\ker(\\varphi) \\simeq \\operatorname{im}(\\varphi)\n\\]\n\n\n\n\nThe rest of the isomorphism theorems largely relies on this first theorem."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#isomorphism-theorem-ii",
    "href": "posts/isomorphism-theorem/index.html#isomorphism-theorem-ii",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "4 Isomorphism Theorem II",
    "text": "4 Isomorphism Theorem II\n\n\n\n\n\n\n\nSome lemmas\n\n\n\n\nLemma 3 Let \\(G\\) be a group and \\(N \\trianglelefteq G\\), \\(S \\le G\\). Then: \\[\nN \\trianglelefteq SN \\le G \\ge S \\trianglerighteq S \\cap N.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\(SN \\le G\\):\n\nNonempty: Trivial.\nClosure: Let \\(s_1n_1, s_2n_2 \\in SN\\), we want to show that \\((s_1n_1)(s_2n_2) \\in SN\\): \\[\n  (s_1n_1)(s_2n_2) = s_1(n_1s_2)n_2.\n  \\] How to deal with the middle term? Since \\(N \\trianglelefteq G\\), we have \\(n_1s_2 = sn_1\\) for some \\(s \\in N\\). Then: \\[\n  s_1(n_1s_2)n_2 = s_1(sn_1)n_2 = (s_1s)(n_1n_2) \\in SN.\n  \\]\nInverses: Let \\(sn \\in SN\\), we want to show that \\((sn)^{-1} = n^{-1}s^{-1} \\in SN\\). Again use the fact that \\(N \\trianglelefteq G\\): \\[\n  \\exists s' \\in N, n^{-1}s^{-1} = s'n^{-1} \\in SN.\n  \\]\n\nThe rest can be proved by this diagram:\n\n\n\n\n\n\nFigure 4: Proof of the isomorphism theorem 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Isomorphism Theorem\n\n\n\n\nTheorem 2 Let \\(G\\) be a group and \\(N \\trianglelefteq G\\), \\(S \\le G\\). Then: \\[\n\\frac{SN}{N} \\simeq \\frac{S}{S \\cap N}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs shown in Figure 4, we have:\n\n\\(N \\trianglelefteq SN\\), since \\(N = \\ker(\\varphi|_{SN})\\). By the first isomorphism theorem, we have \\(\\frac{SN}{N} \\simeq \\varphi (SN)\\).\n\\(S \\cap N \\trianglelefteq S\\), since \\(S \\cap N = \\ker(\\varphi|_{S})\\). By the first isomorphism theorem, we have \\(\\frac{S}{S \\cap N} \\simeq \\varphi (S)\\).\n\nPlus, \\(\\varphi (SN) = \\varphi(S)\\), therefore: \\[\n\\frac{SN}{N} \\simeq \\varphi (SN) = \\varphi(S) \\simeq \\frac{S}{S \\cap N},\n\\] i.e., \\[\n\\frac{SN}{N} \\simeq \\frac{S}{S \\cap N}.\n\\]"
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#isomorphism-theorem-iii",
    "href": "posts/isomorphism-theorem/index.html#isomorphism-theorem-iii",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "5 Isomorphism Theorem III",
    "text": "5 Isomorphism Theorem III\n\n\n\n\n\n\n\nThird Isomorphism Theorem\n\n\n\n\nTheorem 3 Let \\(G\\) be a group and \\(N \\trianglelefteq G\\), \\(K \\trianglelefteq G\\), \\(N \\subset K\\). Then \\(N \\trianglelefteq K\\) and \\[\n\\frac{G/N}{K/N} \\simeq \\frac{G}{K}\n\\]\n\n\n\n\nTo prove this theorem, we need to prove a lemma:\n\n\n\n\n\n\n\nThe image of normal subgroup is normal\n\n\n\n\nLemma 4 Let \\(G\\) be a group, \\(K \\trianglelefteq G\\), \\(\\varphi \\in \\operatorname{Hom}(G, H)\\). The image of \\(K\\) under \\(\\varphi\\) is normal in \\(H\\), i.e., \\[\n\\varphi(K) \\trianglelefteq \\operatorname{im} \\varphi\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe need to show \\(\\forall h \\in \\operatorname{im} \\varphi\\) and \\(\\forall k \\in K\\), \\(h\\varphi(k)h^{-1} \\in \\varphi(K)\\).\nLet:\n\n\\(h \\in \\operatorname{im}(\\varphi)\\), so there exists \\(g \\in G\\) such that \\(h = \\varphi(g)\\),\n\\(k \\in K\\), so \\(\\varphi(k) \\in \\varphi(K)\\)\n\nNow compute: \\[\nh \\varphi(k) h^{-1} = \\varphi(g) \\varphi(k) \\varphi(g)^{-1}\n= \\varphi(g k g^{-1}) \\quad \\text{(since \\( \\varphi \\) is a homomorphism)}\n\\]\nBut since $ K G $, we know \\(g k g^{-1} \\in K\\), so: \\[\n\\varphi(g k g^{-1}) \\in \\varphi(K).\n\\] Therefore: \\[\nh \\varphi(k) h^{-1} \\in \\varphi(K).\n\\]\nSo \\(\\varphi(K)\\) is closed under conjugation by elements of \\(\\operatorname{im}(\\varphi)\\), i.e.,\n\\[\n\\varphi(K) \\trianglelefteq \\operatorname{im}(\\varphi).\n\\]\n\n\n\n\nNow the proof of the third isomorphism theorem can be proved by this diagram:\n\n\n\n\n\n\nFigure 5: Commutative diagram of groups\n\n\n\n\n\n\n\n\n\n\nProof of the third isomorphism theorem\n\n\n\n\n\nFigure 5 can be visually represented by this cartoon:\n\n\n\n\n\n\nFigure 6: Visual demo of the proof of the third isomorphism theorem\n\n\n\nby the first isomorphism theorem, we have: \\[\nG/N \\simeq \\sigma_1 (G) \\triangleright \\sigma_1 (K) \\simeq K/N.\n\\] Therefore, \\[\n\\frac{G/N}{K/N} = \\frac{\\sigma_1 (G)}{\\sigma_1 (K)} \\simeq \\sigma_2 \\circ \\sigma_1 (G) =: \\sigma (G) \\simeq \\frac{G}{K}.\n\\]"
  },
  {
    "objectID": "posts/electrostatics/index.html",
    "href": "posts/electrostatics/index.html",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "",
    "text": "Changing the mindset from forces to fields.\nMaxwell’s First Equation = Inverse Square + Superposition\nCurl-free (conservative) property of static electric field has nothing to do with the inverse square property! (It’s because the radial direction of the field.)"
  },
  {
    "objectID": "posts/electrostatics/index.html#takeaway",
    "href": "posts/electrostatics/index.html#takeaway",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "",
    "text": "Changing the mindset from forces to fields.\nMaxwell’s First Equation = Inverse Square + Superposition\nCurl-free (conservative) property of static electric field has nothing to do with the inverse square property! (It’s because the radial direction of the field.)"
  },
  {
    "objectID": "posts/electrostatics/index.html#sec-welcome",
    "href": "posts/electrostatics/index.html#sec-welcome",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "1 Welcome!",
    "text": "1 Welcome!\nThis is the first chapter of my electromagnetics series. I will introduce to you the theory of electromagnetism through this series, not in the order of history, but in the order of a self-contained logic that satisfies people. The reason I did this is that history is too complicated, confusing and sometimes even wrong. One would not fully understand the trajactory of any subject (geometry, Newton’s law, General relativity, etc) without experiencing it. So I would NOT recommend you go through the tedious history of any subject but develop your own logic-complete explanation of it after reading tons of wikipedia and stackexchange, in other words, that explanation should make sense in the history in another parallel universe!\nYou will understand the first one of the famous Maxwell’s Equations in this blog:\n\\[\n\\begin{aligned}\n    \\mathbf{\\nabla} \\cdot \\mathbf{E} &= \\frac{\\rho}{\\varepsilon_0} \\quad &\\text{(Gauss's law)} \\\\\n    \\mathbf{\\nabla} \\cdot \\mathbf{B} &= 0 \\quad &\\text{(Gauss's law for magnetism)} \\\\\n    \\mathbf{\\nabla} \\times \\mathbf{E} &= -\\frac{\\partial \\mathbf{B}}{\\partial t} \\quad &\\text{(Faraday's law)} \\\\\n    \\mathbf{\\nabla} \\times \\mathbf{B} &= \\mu_0 \\mathbf{J} + \\mu_0 \\varepsilon_0 \\frac{\\partial \\mathbf{E}}{\\partial t} \\quad &\\text{(Ampère's law)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/electrostatics/index.html#physics-world-to-math-world",
    "href": "posts/electrostatics/index.html#physics-world-to-math-world",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "2 Physics World to Math World",
    "text": "2 Physics World to Math World\nPhysics originates from observations in the real world. If you keep an eye on it, you could discover the following phenomena yourself:\n\nThe force between two charges is proportional to their charge, and inversely proportional to their distance, squared.\nNo isolated “magnetic charges” found in nature.\nChanging magnetic flux leads to current in a circuit!\nTwo parallel wires attract or repel each other when current is applied!\n\nThese four observations leads to the four equations by Maxwell."
  },
  {
    "objectID": "posts/electrostatics/index.html#integral-form-of-maxwells-first-equation",
    "href": "posts/electrostatics/index.html#integral-form-of-maxwells-first-equation",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "3 Integral Form of Maxwell’s First Equation",
    "text": "3 Integral Form of Maxwell’s First Equation\n\n3.1 Inverse Square Law\nAccording to observation 1, we could write1: \\[\nF \\propto \\frac{Qq}{r^2},\n\\] or in an equation: \\[\n\\boxed{\nF = k \\frac{Qq}{r^2},\n}\n\\]\n1 Non-bold-face letters (\\(F\\)) are scalars, bold-face letters (\\(\\mathbf{F}\\)) are vectors.2 The unit of charge (Coulomb) is defined in the following way (The reason Ampere are defined first is because Ampere happens to be one of the base unit of SI): - Current: Ampere (I: A): First we define Ampere (surprise!) to be the intensity of current on two ideal wires that are 1 metre apart and produces a force of \\(2 \\times 10^{-7}\\) N between them (1N is defined to be the force that makes an object of mass 1kg moves at the acceleration of \\(1\\text{m}/\\text{s}^2\\)) - Charge: Coulomb (Q: C): Then we define Coulomb to be the amount of charge the that passes through a point with a current of 1A over 1 second.where \\(k\\) is a constant, \\(Q\\) and \\(q\\) are the amount of charge2 on two small objects, \\(r\\) is their distance.\n\n\n3.2 Conservative Property\nLet’s consider the work done when we fix \\(Q\\) and gradually move \\(q\\) in a certain path. Since the force from \\(Q\\) to \\(q\\) is always radially from \\(Q\\), it turns out that the work done only depends on the initial and final position of \\(q\\) regardless of the moving process in between! We say a force like this Conservative3. We would like to think that there is a number \\(U\\) attached at every spatial point around \\(Q\\) such that the the work done from point \\(A\\) and \\(B\\) is just that \\(U(A)-U(B)\\)4, i.e., \\[\nW_{AB} =: U(A) - U(B).\n\\]\n3 It’s worth noting that conservativity has nothing to do with the inverse square property. The potential energy of hypothetical “inverse force” and “inverse cubic force” are: \\[\nU_{\\frac{1}{r}} = -kQq \\ln r,\n\\] and \\[\nU_{\\frac{1}{r^3}} = \\frac{kQq}{2} \\frac{1}{r^2}.\n\\]4 Why not \\(U(B)-U(A)\\)? Because we want the quantity \\(U\\) also indicates the tendancy that \\(q\\) would move. More likely to move, \\(U\\) should be larger.\n\n\n\n\\(U(A)\\) is expected to be larger than \\(U(B)\\)\n\n\n5 Actually, should be \\(U(r) = -k \\frac{Q}{r} + \\operatorname{const}\\). But when \\(r \\to \\infty\\), we expect \\(U\\) to be \\(0\\) because \\(q\\) do not have the ability to do work.\n\\(U(r)\\) is called the potential energy at \\(r\\). We claim5 that \\[\n\\boxed{\nU(r) = -k \\frac{Qq}{r},\n}\n\\] because \\[\nW_{AB}\n= \\int_{A \\to B} \\mathbf{F} \\cdot \\mathbf{\\mathrm{d}r}\n= \\int_{r_0}^{r_1} k \\frac{Qq}{r^2} \\mathrm{d}r\n= -k \\frac{Qq}{r_1} - k \\frac{Qq}{r_0}\n=: U(A) - U(B).\n\\]\nTherefore, the work done is just the difference of \\(U\\) with a negative sign: \\[\nW_{AB} = - \\Delta U.\n\\]\nWe also know that\n\\[\n\\mathrm{d} W_{AB} = \\mathbf{F} \\cdot \\mathrm{d} \\mathbf{l} = F \\cdot \\mathrm{d} l_\\parallel,\n\\] where \\(\\mathrm{d} \\mathbf{l}\\) is a small displacement and \\(\\mathrm{d} l_\\parallel\\) is the length of the projection of that small displacement onto the direction of \\(\\mathbf{F}\\), i.e., the direction of \\(\\mathbf{\\nabla} U\\), reversed. Hence, \\[\nF = \\frac{\\mathrm{d} W_{AB}}{\\mathrm{d} l_\\parallel} = - \\frac{\\mathrm{d} U}{\\mathrm{d} l_\\parallel}.\n\\]\nIn vector notation, \\[\n\\boxed{\n\\mathbf{F} = - \\mathbf{\\nabla} U.\n}\n\\tag{1}\\]\n\n\n3.3 Get Rid of Test-charge\nWhen \\(q\\) is far smaller than \\(Q\\), it is called a test charge, which is used to “test” the effect of \\(Q\\) to its surroundings and minimize other interference. It is naturally to get rid of \\(q\\) and define a quantity \\(E\\) that only depends on \\(Q\\), we expect that \\(E\\) satisfies: \\[\n\\mathbf{F} =: \\mathbf{E} q,\n\\] where \\(E\\) obviously equal to: \\[\n\\boxed{\nE = k \\frac{Q}{r^2}.\n}\n\\]\nWe call \\(E\\) the electric field generated by \\(Q\\)6.\n6 Since \\(\\mathbf{E}\\) is equivalent to force (just up to a constant), all properties of \\(\\mathbf{E}\\) is inherited from \\(\\mathbf{F}\\), such as vector property, superposition, conservativity, etc.Therefore, Equation 1 could be written as: \\[\n\\mathbf{E}q = - \\mathbf{\\nabla} U.\n\\tag{2}\\]\nWe could also get rid of \\(q\\) in Equation 2 by defining a quantity \\(V\\) called the (Electric) potential generated by \\(Q\\): \\[\n\\boxed{\nU := Vq,\n}\n\\] so we have \\[\n\\mathbf{E}q = -\\mathbf{\\nabla} V q\n\\] \\[\n\\implies\n\\boxed{\n\\mathbf{E} = - \\mathbf{\\nabla} V.\n}\n\\tag{3}\\]\nYou can compare Equation 1 and Equation 3, the latter is test-charge-free version of the former!\n\n\n\n\n\n\nTip 1: The spirit of fields\n\n\n\nThis mindset from force to field is extremely important! “Field” originates from “force” but later evolves independently from it, as you will see. Thinking in terms of “fields” rather than “forces” is a key factor that distinguishes beginners from experts. Now you have evolved to the second level – make “fields” be your second nature!\n\n\n\n\n3.4 An Interesting Question\nHow to know how many charges inside some closed region?\n\n\n\n\n\nThe question\n\n\n\nProposition 1 The charge inside some closed surface \\(S\\) can be calculated by only looking at the field sitting on its surface7: \\[\n\\text{Flux} \\propto Q_{\\text{in}},\n\\] where the flux \\(\\Phi\\) is defined: \\[\n\\Phi := \\oiint_S \\mathbf{E} \\cdot \\mathrm{d} \\boldsymbol{A}.\n\\]\n7 This claim directly comes from the inverse square law and superposition principle of fields.\n\nSolution 1. \n\nWe will consider the case then there is only one point charge \\(q\\) inside \\(S\\).\nConsider a sphere \\(R\\) of radius \\(r\\) around \\(q\\) in Figure 1, it’s obviously that the flux through \\(R\\) does not related to \\(r\\), because the surface area increases at the rate of \\(r^2\\) and the field decays at the rate of \\(1/r^2\\). Just to be intimidating, \\[\n\\Phi_R = \\oiint_R k \\frac{q}{r^2} dA = k \\frac{q}{r^2} \\cdot 4 \\pi r^2 = 4 \\pi k q \\propto q.\n\\]\nWe commonly let \\[\n\\boxed{\n     k = \\frac{1}{4 \\pi \\epsilon_0}\n}\n  \\tag{4}\\] to simpify8 Equation 4 to be \\[\n\\Phi_R = \\frac{q}{\\epsilon_0}.\n\\]\n\n8 We introduce the symbol \\(\\epsilon_0\\) by the motivation to eliminate the “\\(4 \\pi\\)” Equation 4. But the meaning of \\(\\epsilon_0\\) would be clear later until we introduce the electric fields in matter. Don’t worry.\nNow we claim that \\[\n\\Phi_R = \\Phi_S,\n\\] where \\(S\\) is an arbitrary closed surface outside \\(R\\).\nAgain we use the inverse square property, the flux through \\(\\mathrm{d}R\\) should be same as the flux through the blue circle in Figure 1. Plus, the flux through the blue circle is exactly the same as the flux through \\(\\mathrm{d}S\\) as shown in Figure 2 (since their “perpendicular” surface area are the same)\nWe then use superposition property of fields to obtain the equation of multiple charges enclosed.\nSuppose there are \\(N=3\\) point charges inside \\(S\\) as shown in Figure 3, the total flux is\n\\[\n\\Phi\n= \\oiint_S \\mathbf{E} \\cdot  \\mathrm{d} \\mathbf{A}\n= \\sum_{i = 1}^3 \\left(\\oiint_S \\mathbf{E}_i \\cdot  \\mathrm{d} \\mathbf{A}\\right)\n= \\sum_{i = 1}^3 \\frac{q_i}{\\epsilon_0}\n= \\frac{Q_{\\text{in}}}{\\epsilon_0}.\n  \\tag{5}\\]\nOf course Equation 5 can be generalized when \\(N\\) is arbitrary. And ura! We have just proof the integral version of Maxwell’s first equation! \\[\n\\boxed{\n\\oiint_S \\mathbf{E} \\cdot  \\mathrm{d} \\mathbf{A} = \\frac{Q_{\\text{in}}}{\\epsilon_0}.\n}\n  \\tag{6}\\]\nThis is also known as Gauss’s Law. Equation 6 holds for any closed surface \\(S\\).\n\n\n\n\n\n\n\n\n\n\nFigure 1: Single point charge case\n\n\n\n\n\n\n\n\n\nFigure 2: The flux through the blue circle is exactly the same as the flux through \\(\\mathrm{d}S\\)\n\n\n\n\n\n\n\n\n\nFigure 3: Superposition of the fields of every point charge inside \\(S\\)"
  },
  {
    "objectID": "posts/electrostatics/index.html#differential-form-of-maxwells-first-equation",
    "href": "posts/electrostatics/index.html#differential-form-of-maxwells-first-equation",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "4 Differential Form of Maxwell’s First Equation",
    "text": "4 Differential Form of Maxwell’s First Equation\nThe charges in the real world are not commonly appears like an infinitesimal point. They distributed evenly through a body instead of concentrate on a point of no size. Therefore, \\(Q_{\\text{in}}\\) on the RHS9 of Equation 6 could be write as an integral: \\[\n\\frac{Q_{\\text{in}}}{\\epsilon_0} = \\iiint_V \\frac{\\rho}{\\epsilon_0} \\mathrm{d}V,\n\\] where \\(V\\) is the region enclosed by \\(S\\), \\(\\rho\\) is the density10 of charges at some place inside \\(S\\).\n9 Right hand side.10 In the case of point charges, this density if infinite. Mathematicians use so-called \\(\\delta\\)-function to describe the behaviour of this kind of “degenerated” density. It is not a function, just a symbol following some rules that deals with the Mathematical OCD that forces the form of a certain integral unchanged when some mass is concentrated on a small point.\n\n\n\nReal world charges are not point charge\n\n\n\nWhat about the LHS of Equation 6? We expect it to be also written in a kind of volume integral to cancel it out with the RHS: \\[\n\\oiint_S \\mathbf{E} \\cdot  \\mathrm{d} \\mathbf{A} = \\iiint_V \\boxed{???} \\mathrm{d}V.\n\\]\nAnd then \\[\n\\iiint_V \\boxed{???} \\mathrm{d}V = \\iiint_V \\frac{\\rho}{\\epsilon_0} \\mathrm{d}V\n\\tag{7}\\] holds for any11 volume \\(V\\), so we can claim that \\[\n\\boxed{???} = \\frac{\\rho}{\\epsilon_0}.\n\\tag{8}\\]\n11 Any is very important! Without any, we cannot derive Equation 7 from Equation 8.12 Also denoted \\(\\mathbf{\\nabla} \\cdot \\mathbf{E}\\).Luckily! Stoke’s Theorem tells us \\[\n\\boxed{???} = \\operatorname{div} \\mathbf{E}.\n\\] For those of you not familiar with multivariable calculus, \\(\\operatorname{div} \\mathbf{E}\\) is called the divergence12 of \\(\\mathbf{E}\\), which is a scalar-valued function purely derived from \\(\\mathbf{E}\\).\nFinally! We got the differential form: \\[\n\\boxed{\n\\operatorname{div} \\mathbf{E} = \\frac{\\rho}{\\epsilon_0}.\n}\n\\tag{9}\\]"
  },
  {
    "objectID": "posts/electrostatics/index.html#thinking-problem",
    "href": "posts/electrostatics/index.html#thinking-problem",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "5 Thinking Problem",
    "text": "5 Thinking Problem\n\nExercise 1 Think about what properties of \\(\\mathbf{E}\\) ensures that Equation 9 holds true?\n\n\nSolution 2. There are only two properties of electric fields that are used to obtain Equation 9:\n\nInverse square\nSuperposition\n\nStoke’s Theorem don’t count because it holds for any vector fields.\nSuperposition is trivial. The non-trivial part is inverse square. This property ensures that we can extend the sphere to an arbitrary surface in Figure 1. What is the nature of inverse square?\nWell, we live in 3-dimensional space. Everything that spreads should somehow decay at the rate of \\(1/r^2\\), like light, gravity, sound, etc. Otherwise it will against the conservation of energy. In general, if we live in a \\(N\\)-dim world, fields should naturally decay at the rate of \\(1/r^{N-1}\\). In other words, everything in \\(N\\)-dim world should decay at the rate of “area” decay. This paper by myself delved a little deeper inside this inspiration."
  },
  {
    "objectID": "posts/create-first-web/index.html",
    "href": "posts/create-first-web/index.html",
    "title": "Create your own website using Hugo on MacOS",
    "section": "",
    "text": "Building a generic website from scratch is a tough work. However, personal websites for blogs, a special type of website, is actually programmatic. Hugo provides a convenient building templates for that. To make a new blog in Hugo, one could only just create a new folder, write a markdown file (in a specific format) and that’s it. You don’t need ANY knowledge about HTML or CSS. Well do you need to buy a domain name for everyone to see your posts? Well, GitHub Pages is a free service where you just push some contents in a repository of your own with some extremely easy command line, you created your sites of names like yourname.github.io."
  },
  {
    "objectID": "posts/create-first-web/index.html#introduction-and-overview",
    "href": "posts/create-first-web/index.html#introduction-and-overview",
    "title": "Create your own website using Hugo on MacOS",
    "section": "",
    "text": "Building a generic website from scratch is a tough work. However, personal websites for blogs, a special type of website, is actually programmatic. Hugo provides a convenient building templates for that. To make a new blog in Hugo, one could only just create a new folder, write a markdown file (in a specific format) and that’s it. You don’t need ANY knowledge about HTML or CSS. Well do you need to buy a domain name for everyone to see your posts? Well, GitHub Pages is a free service where you just push some contents in a repository of your own with some extremely easy command line, you created your sites of names like yourname.github.io."
  },
  {
    "objectID": "posts/create-first-web/index.html#tools-needed",
    "href": "posts/create-first-web/index.html#tools-needed",
    "title": "Create your own website using Hugo on MacOS",
    "section": "2 Tools needed",
    "text": "2 Tools needed\n\n2.1 Homebrew (Packet Manager)\nWe will use a packet manager called Homebrew to install Hugo. Follow the commands here to download Homebrew first, or you can execute:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nOnce downloaded, if you don’t get any error by entering this in the terminal in any folder, you installed it properly (Install testing):\nbrew --version\n\n\n2.2 Git (Version Control System)\nYou can use Homebrew to install Git:\nbrew install git\ngit --version # install testing"
  },
  {
    "objectID": "posts/create-first-web/index.html#install-hugo",
    "href": "posts/create-first-web/index.html#install-hugo",
    "title": "Create your own website using Hugo on MacOS",
    "section": "3 Install Hugo",
    "text": "3 Install Hugo\nRun this in any folder:\nbrew install hugo\nInstall testing:\nhugo version"
  },
  {
    "objectID": "posts/create-first-web/index.html#run-example-theme",
    "href": "posts/create-first-web/index.html#run-example-theme",
    "title": "Create your own website using Hugo on MacOS",
    "section": "4 Run Example Theme",
    "text": "4 Run Example Theme\n\n4.1 Create Framework\nEnter the terminal in any folder, you will be creating another main folder called mysite in it. Folder mysite will contain all the contents that are relevant to your website:\nhugo new site mysite\ncd mysite\nFolder mysite should look like this:\n\n\n\nInside mysite folder\n\n\nwith most folders in it empty. This is the framework.\n\n\n4.2 Choose Theme\nThe themes folder is empty, now we will add some code representing a theme inside it. Now choose a template here and downlaod its source code folder inside the themes folder. I use Stack\ncd themes/\ngit clone https://github.com/CaiJimmy/hugo-theme-stack.git # Replace as needed\n\n\n4.3 Run Theme\nNow there should be a folder inside themes. Now copy all the things inside a folder like exampleSite into the main mysite folder (‘’replace’’).\n\n\n\nCopy the contents in exampleSite into mysite\n\n\nGo to the main mysite folder and remove the original hugo.tomal file, or you can do:\npwd # should be in \"mysite\"\nrm hugo.tomal\nThen (in mysite folder), run:\nhugo server -D\nit will prompt something like:\nBuilt in 865 ms\nEnvironment: \"development\"\nServing pages from disk\nRunning in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender\nWeb Server is available at http://localhost:53844/ (bind address 127.0.0.1) \nPress Ctrl+C to stop\nEnter the link provided (http://localhost:53844/). You are done!"
  },
  {
    "objectID": "posts/create-first-web/index.html#play-around",
    "href": "posts/create-first-web/index.html#play-around",
    "title": "Create your own website using Hugo on MacOS",
    "section": "5 Play Around",
    "text": "5 Play Around\nThis is easy, just compare the contents in each folder and the website and modify things a little."
  },
  {
    "objectID": "posts/create-first-web/index.html#publish-your-website",
    "href": "posts/create-first-web/index.html#publish-your-website",
    "title": "Create your own website using Hugo on MacOS",
    "section": "6 Publish your website",
    "text": "6 Publish your website\nNote the link (http://localhost:53844/) is private and cannot be visited on other devices. So follow these steps to publish it:\n\n6.1 Create GitHub Repository\nGo to your GitHub page and click ‘+’ on the right upper corner, choose New repository and name it like this:\n\n\n\nName of your repository\n\n\ni.e., yourname.github.io, which will be your own domain name.\n\n\n6.2 Push Contents\nRun this:\nhugo --theme=hugo-theme-stack --baseURL=\"https://yourname.github.io/\" --buildDrafts\nThen push the contents in your folder public on it by:\ncd public\ngit init\ngit remote add origin https://github.com/yourname/yourname.github.io.git # change as needed\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\nYou can access your website on https://yourname.github.io/ within several minutes.\n\n\n6.3 Notes\nAfter you change contents locally, the contents on https://yourname.github.io/ will not change automatically, you will have to push it on GitHub again:\ncd public\ngit add .\ngit commit -m \"Add something new\"\ngit push origin main\nAlso the public website will not update instantly, you will have to wait several minutes."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Marcobisky",
    "section": "",
    "text": "University of Electronic Science and Technology of China (UESTC) (Sept 2022 — Present)\n\nStudent, School of Communication Engineering.\n\nUniversity of Glasgow, Scotland, UK (Sept 2022 — present)\n\nStudent, School of Electronic and Computer Engineering."
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Marcobisky",
    "section": "",
    "text": "University of Electronic Science and Technology of China (UESTC) (Sept 2022 — Present)\n\nStudent, School of Communication Engineering.\n\nUniversity of Glasgow, Scotland, UK (Sept 2022 — present)\n\nStudent, School of Electronic and Computer Engineering."
  },
  {
    "objectID": "cv/index.html#engaged-projects",
    "href": "cv/index.html#engaged-projects",
    "title": "Marcobisky",
    "section": "ENGAGED PROJECTS",
    "text": "ENGAGED PROJECTS\nMovable Antenna (MA) for Anti-jamming (Just start)\n\nMain tools: matlab.\nA heuristic investigation into Anti-jamming through stochastic antenna movement.\nConducted under the supervision of Prof. Weidong Mei at UESTC.\n\nComputer Vision (CV) for Quadrotor Aircraft (Just start)\n\nMain tools: matlab, C/C++, python, verilog.\nAutomatic quadrotor aircraft for objection detection, robotic arm manipulation, and closed-loop flight control.\n6-people group.\n\nRV32I CPU Core for Education  (Jan 2025 — Present)\n\n\n\n\nGPIO simulation in Digital\n\n\nMain tools: verilog, VHDL, Digital, Kicad, iCESuger FPGA.\nSimulate an entire RISC-V 32 bit CPU in verilog and Digital Software.\nSupport basic peripherals such as GPIOs, IIC, UART, VGA, etc.\nSimple boot ROM in assembly, minimal interrupt service for running a Linux kernel.\n\nAME Source Coding (Oct 2024 — Nov 2024)\n\n\n\n\nProposed AME coding scheme\n\n\nMain tools: python, matlab.\nFinal project of Information Theory Course.\nSecond-order Markov Adapative Approximation (AME) to source-coding the Game of Thrones.\nPerformance evaluation of Huffman and Fano coding.\n\nCNN for Mbed (Feb 2024 — May 2024)\n\n\n\n\nProposed CNN in L432KC MCU\n\n\nMain tools: python, C++.\nConvolutional Neural Network (CNN) integration into an MCU.\nSmart fall detection, body temperature monitoring and real-time data visualization for patients.\n\nA Study of Generalized Fields and Extension to Higher Dimensions1 (Oct 2023 — Feb 2024)\n1 I submitted this paper to the American Journal of Physics, but it was declined for publication.\n\n\nFields in high dimension can be reduced\n\n\n\nA theoretical study of generalized natural fields and behaviours in higher dimensions.\nLargely motivated by my tutor Mr. Yidong Liu and my friends and completed by myself.\n\nHuman Voice Recognition Smart Car (Sept 2023 — Dec 2023)\n\n\n\n\nVoice-controlled car\n\n\nMain tools: C++, STM32F103C8T6 MCU, etc.\nLeader of a 4-people team.\nEnglish words recognition for car movement controlling.\nBasic operations: Moving forwards and backwards, turning or sliding left and right, etc.\n\nAuto Door Opener for Dormitory (Sept 2023 — Oct 2023)\n\n\n\n\nDoor opener tested on breadboard\n\n\nMain tools: C++, Nucleo L432KC MCU, Mbed library, OLED screen, etc.\nThe final project of the Microelectronic System course.\nOpening the dormitory door by password input.\nBasic functions: Setting up password manually, automatically lock for repeated wrong passwords, OLED message displaying, etc.\n\n“XinTong Cup” Electronic Design Competition: Electronic Keyboard Music Player (Sept 2022 — Oct 2022)\n\nMain tools: Keil C51, STC89C52RC MCU, etc.\nLeader of 3-people team.\nA simplified 8-key music player using register-based development on a 8-bit MCU by ST company.\nFunctionality: Single note playing, chord playing, recording ability, replay and rewind capability, etc."
  },
  {
    "objectID": "cv/index.html#academic-recordfullscore",
    "href": "cv/index.html#academic-recordfullscore",
    "title": "Marcobisky",
    "section": "ACADEMIC RECORD2",
    "text": "ACADEMIC RECORD2\n\nDetailed scores of core courses (GPA: 3.88 out of 4.00)\n\n\n\n\n\n\n\nYear\nSubject\nScore (Full mark: 100)\n\n\n\n\nYear 1\nCalculus I/II   Linear Algebra   C Programming   Physics I  \n91/92   84   95   88  \n\n\nYear 2\nPhysics II   Signal and Systems   Probability and Statistics   Microelectronic Systems   Embedded Processors   Circuit Analysis and Design   Computer Network   Academic English  \n96   91   92   92   95   95   94   89  \n\n\nYear 3\nInformation Theory   Principles of Communication   Digital Circuit Design   Machine Learning   Stochastic Signal Analysis  \n91   95   86   86   82"
  },
  {
    "objectID": "cv/index.html#relevant-skills",
    "href": "cv/index.html#relevant-skills",
    "title": "Marcobisky",
    "section": "RELEVANT SKILLS",
    "text": "RELEVANT SKILLS\n\nIT Skills: Latex, (Quarto) Markdown, Typst, Manim3, Github4, Microsoft Office.\nComputer Programming: C/C++, Matlab, Python.\nEmbedded System Programming: RISCV assembly, STM89C5x (Standard lib), Keil C51.\nMath: Self learned (Abstract Algebra (Harvard E-222)), Point-set Topology, Measure Theory, Complex Analysis (MIT 18.04), Functional Analysis, Elementary Differential Geometry, Lie Groups and Lie Algebras (still learning). I didn’t focus on all epsilons and deltas, but their motivations and application potentials.\nTeam Work: Zoom meeting, Notion team, Microsoft team.\nLanguage: No problem in understanding English lectures, GRE score 317, native Chinese.\n\n3 See this video in which I use manim to explain the relation between the adjoint and dual operator.4 My Github."
  },
  {
    "objectID": "cv/index.html#others",
    "href": "cv/index.html#others",
    "title": "Marcobisky",
    "section": "OTHERS",
    "text": "OTHERS\n\nAwards\n\nFirst Prize in the 7th National College Student Art Exhibition and Performance: Symphony No. 4 in D minor, Op. 120, 4th movement, by Robert Schumann. (In violin section)\nTop Academic Scholarship of UESTC: First-class Scholarship for the past two years.\nChina National Scholarship, 2024: Prestigious national award granted for academic excellence, leadership, and overall achievement.\n\n\n\nInterests\n\nClassical Music Enthusiast🎻: Violin player in UESTC symphony orchestra, votary of legendary composer Gustav Mahler and Johann Sebastian Bach.\nBadminton Lover🏸: Sports always refreshes me at any time.\nLearning Everything🔍: I believe everything is learnable by First Principle Thinking and curiosity.\nVolunteer Work🤝: Enjoy helping others. Over 15 hours of volunteering."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My posts",
    "section": "",
    "text": "Solving Laplace’s Equation using Separation of Variables\n\n\n\n\n\n\nDifferential-Equation\n\n\n\nLearn how to solve stationary heat distribution problem under specific boundary conditions\n\n\n\n\n\nMay 1, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Projective Space and Projective Linear Group 射影空间与射影线性群\n\n\n\n\n\n\nAlgebra\n\n\nDifferential-Geometry\n\n\n\nProjective spaces, unit quaternions, \\(SU(2), SO(3)\\), 3-sphere, Möbius transformations, double cover, etc.\n\n\n\n\n\nApr 21, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nPDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程\n\n\n\n\n\n\nDifferential-Equation\n\n\n\nFirst chapter of PDE series!\n\n\n\n\n\nApr 5, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Isomorphism Theorems for Groups 群同构定理的理解\n\n\n\n\n\n\nAlgebra\n\n\n\nIf you struggle to have a clear mental picture of the isomorphism theorems, I hope this helps.\n\n\n\n\n\nMar 31, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nControl Case Study: LQR for Inverted Pendulum!\n\n\n\n\n\n\nControl-Theory\n\n\n\nComplete guide for building a linear feedback control system for an inverted pendulum in MATLAB.\n\n\n\n\n\nMar 25, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nThree ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解\n\n\n\n\n\n\nDifferential-Geometry\n\n\n\nWhen it comes to \\(a \\cdot (b \\times c)\\), what’s in your head?\n\n\n\n\n\nMar 9, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nEM Chapter I: Maxwell’s First Equation\n\n\n\n\n\n\nPhysics\n\n\n\nUnderstand \\(\\mathbf{\\nabla} \\cdot \\mathbf{E} = \\frac{\\rho}{\\varepsilon_0}\\) in one article!\n\n\n\n\n\nMar 7, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?\n\n\n\n\n\n\nDifferential-Geometry\n\n\n\nWhy \\(e_i\\) is exactly the same as \\(\\frac{\\partial}{\\partial x^i}\\)? How to define tangent space at some point of a manifold?\n\n\n\n\n\nMar 4, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is symmetry? 什么是对称性?\n\n\n\n\n\n\nAlgebra\n\n\n\nSymmetry is nothing but a group acts on an object!\n\n\n\n\n\nMar 3, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nJK Flip-flop not Behavioral (VHDL version)\n\n\n\n\n\n\nHardware\n\n\n\nEasy to write a behavioral description of JK-FF in VHDL, what about writing it in terms of gates?\n\n\n\n\n\nDec 8, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\n在 ios 下载第三方应用\n\n\n\n\n\n\nAwesome-Mac\n\n\n\n会闪退, 散了吧(´･_･`)\n\n\n\n\n\nOct 29, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares as Projection 最小二乘法的投影解释\n\n\n\n\n\n\nAlgebra\n\n\n\nThinking least squares in this way really helps!\n\n\n\n\n\nSep 21, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\njava项目导入Launchpad方案 MacOS\n\n\n\n\n\n\nAwesome-Mac\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nCreate your own website using Hugo on MacOS\n\n\n\n\n\n\nCS\n\n\n\nThis is how I built my first personal website without any prior knowledge of HTML.\n\n\n\n\n\nAug 31, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nRTL Analysis on MacOS under 300MB\n\n\n\n\n\n\nAwesome-Mac\n\n\n\nWanna compile verilog on MacOS but without programing an FPGA? Check this out!\n\n\n\n\n\nAug 31, 2024\n\n\nMarcobisky\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jinming Ren",
    "section": "",
    "text": "Hi! I’m an third-year undergraduate student at University of Electronic Science and Technology of China (UESTC) and University of Glasgow (UofG). I majored in electronic and computer engineering (ECE). I’m a math enthusiast and classical music lover.\nI’m looking for a PhD position in machine learning, computer science or wireless communication in 2026, I love innovation and challenges! See my CV here.\nOn this site I also keep a partial list of my blogs, which are actively updating.\n\n\n\n\n📧 Email: 3191293752@qq.com | marcobisky@outlook.com\n\n📞 Phone: +86 17882004164"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jinming Ren",
    "section": "",
    "text": "Hi! I’m an third-year undergraduate student at University of Electronic Science and Technology of China (UESTC) and University of Glasgow (UofG). I majored in electronic and computer engineering (ECE). I’m a math enthusiast and classical music lover.\nI’m looking for a PhD position in machine learning, computer science or wireless communication in 2026, I love innovation and challenges! See my CV here.\nOn this site I also keep a partial list of my blogs, which are actively updating."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jinming Ren",
    "section": "",
    "text": "📧 Email: 3191293752@qq.com | marcobisky@outlook.com\n\n📞 Phone: +86 17882004164"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html",
    "href": "posts/digital-analysis-mac/index.html",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "",
    "text": "CLAIM: This article synthesizes conclusions and methods from multiple websites and is not pure original. 本文综合了多个网站的结论和方法，并非原创。\nYou may find these websites useful:\n\nWorkflow for FPGA development\nSome open-source tools\nIceStudio for M2 Mac\nFPGA Toolchain\nVHDL compile chain on MacOS\n\n\n\n\n\nYou need these:\n\ngtkwave: main RTL wave creator\nIcarus Verilog, and this manual\nYoSYS: .v to .json\nGHDL: compile, link and simulation tool for VHDL\nnetlistsvg: .json to .svg\n\nNot nessasarily required:\n\nPulseview: Logic Analyzer (Not required though)\nDigital: Visual simulation\n\nUseful vscode plugins:\n\nVerilog Support: vscode verilog language highlighter\nWavetrace: A nice vscode plugin to replace gtkwave"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html#intro-and-overview",
    "href": "posts/digital-analysis-mac/index.html#intro-and-overview",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "",
    "text": "CLAIM: This article synthesizes conclusions and methods from multiple websites and is not pure original. 本文综合了多个网站的结论和方法，并非原创。\nYou may find these websites useful:\n\nWorkflow for FPGA development\nSome open-source tools\nIceStudio for M2 Mac\nFPGA Toolchain\nVHDL compile chain on MacOS\n\n\n\n\n\nYou need these:\n\ngtkwave: main RTL wave creator\nIcarus Verilog, and this manual\nYoSYS: .v to .json\nGHDL: compile, link and simulation tool for VHDL\nnetlistsvg: .json to .svg\n\nNot nessasarily required:\n\nPulseview: Logic Analyzer (Not required though)\nDigital: Visual simulation\n\nUseful vscode plugins:\n\nVerilog Support: vscode verilog language highlighter\nWavetrace: A nice vscode plugin to replace gtkwave"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html#first-project-in-verilog",
    "href": "posts/digital-analysis-mac/index.html#first-project-in-verilog",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "2 First Project in Verilog",
    "text": "2 First Project in Verilog\n\n2.1 Install Icarus Verilog Compiler\nbrew install icarus-verilog\n\n\n2.2 Compilation and Simulation\n\nCreate new folder called Verilog, then create two test files named GatedDLatch.v and GatedDLatch_tb.v. The former is the description file of the circuit, the latter is for testbench. And write the following contents respectively:\n// in GatedDLatch.v\nmodule GatedDLatch (Data, WE, Out, OutBar);\n\n    input Data;\n    input WE;\n\n    output Out;\n    output OutBar;\n\n    // component name(output, input1, input2)\n    wire S;\n    wire R;\n    wire Dbar;\n    nand g1(S, Data, WE);\n    not g2(Dbar, Data);\n    nand g3(R, WE, Dbar);\n    nand g4(Out, S, OutBar);\n    nand g5(OutBar, R, Out);\n    \n\nendmodule\nand\n// in GatedDLatch_tb.v\n`timescale 1ns / 1ns // simulation time, time precision = 1ns\n//Import the main code into the testbench\n`include \"GatedDLatch.v\"\n\nmodule GatedDLatch_tb;\n//Inputs as registers\nreg Data;\nreg WE;\n\n//Outputs as wires\nwire Out;\nwire OutBar;\n\n//Initialisation\nGatedDLatch uut(Data, WE, Out, OutBar);\n\ninitial begin\n    //Name of the graph file that gets generated after we run\n    $dumpfile(\"GatedDLatch_tb.vcd\");\n    $dumpvars(0,GatedDLatch_tb);\n\n    Data = 0;\n    WE = 0;\n    #10;\n\n    Data = 1;\n    #4;\n    WE = 1;\n    #2;\n    WE = 0;\n    #4;\n\n    Data = 0;\n    #4;\n    WE = 1;\n    #2;\n    WE = 0;\n    #4;\n\n    $display(\"Test complete\");\nend\nendmodule\n\nRun this in the terminal:\niverilog -o GatedDLatch_tb.vvp GatedDLatch_tb.v\n\nUse vvp command to convert the binary temperary file GatedDLatch_tb.vvp to GatedDLatch_tb.vcd waveform file:\nvvp GatedDLatch_tb.vvp\n\nInstall Wavetrace in vscode to view the waveform:\n\n\n\nDisplay the waveform in vscode\n\n\n\nYou can also install gtkwave to view the waveform.\nbrew install gtkwave\ngtkwave GatedDLatch_tb.vcd\n\n\n\nBefore imported\n\n\n\nExpand GatedDLatch_tb list to display the waveform:\n\n\n\nDisplaying waveform\n\n\n\n\n2.3 Synthesis and Visualization\n\nWe can also visualize the circuit topology (called generating schematics). First, use YoSYS to convert the verilog code into gate-level netlist. Of course you should install the command line tool YoSYS:\nbrew install yosys\nyosys -V # Verify Yosys installation\n\nYoSYS will first convert the circuit structure description file GatedDLatch.v into a json file:\nyosys -p \"prep -top GatedDLatch; write_json GatedDLatch.json\" GatedDLatch.v\n\nThen we install another tool called netlistsvg:\n# Install Node.js (if not already installed)\nbrew install node\n\n# Install netlistsvg globally using npm\nnpm install -g netlistsvg\n\n# Verify netlistsvg installation\nnetlistsvg --version\n\nUsing the netlistsvg tool to convert GatedDLatch.json to GatedDLatch.svg:\nnetlistsvg GatedDLatch.json -o GatedDLatch.svg\n\nPreviewing GatedDLatch.svg will give you the circuit schematic:\n\n\n\nGatedDLatch.svg\n\n\n\n\n2.4 Makefile Work Flow\n\nThe entire workflow can be divided into two major independent parts:\n\n\nCompilation and Simulation:\n\niverilog (Compilation)\nvvp (Simulation)\n\nSynthesis and Circuit Structure Visualization:\n\nyosys (Synthesis)\nnetlistsvg (Visualization)\n\n\n\nWe use a Makefile to automate this process (ensure that Make and related components are installed): Create a Makefile file in the previously created Verilog folder and add the following content:\n# Description: Makefile for GatedDLatch\nCIRCUIT_STRUCT = GatedDLatch\n\n# Directories\nBUILD_DIR = build\n\n# Ensure the build directory exists\n$(BUILD_DIR):\n    mkdir -p $(BUILD_DIR)\n\n# Compilation: iverilog compilation\niverilog: $(CIRCUIT_STRUCT).v $(CIRCUIT_STRUCT)_tb.v | $(BUILD_DIR)\n    iverilog -o $(BUILD_DIR)/$(CIRCUIT_STRUCT)_tb.vvp $(CIRCUIT_STRUCT)_tb.v\n\n# Simulation: generate waveform (.vcd)\nvvp: $(BUILD_DIR)/$(CIRCUIT_STRUCT)_tb.vvp\n    vvp $(BUILD_DIR)/$(CIRCUIT_STRUCT)_tb.vvp\n\n# Synthesis: generate circuit structure configuration file (.json)\nYOSYS: $(CIRCUIT_STRUCT).v | $(BUILD_DIR)\n    yosys -p \"prep -top $(CIRCUIT_STRUCT); write_json $(BUILD_DIR)/$(CIRCUIT_STRUCT).json\" $(CIRCUIT_STRUCT).v\n\n# Visualization: generate human readable (.svg) from .json\nNETLISTSVG: $(BUILD_DIR)/$(CIRCUIT_STRUCT).json | $(BUILD_DIR)\n    netlistsvg $(BUILD_DIR)/$(CIRCUIT_STRUCT).json -o $(BUILD_DIR)/$(CIRCUIT_STRUCT).svg\n\n# Schematic diagram only: Synthesis then Visualization\nschematic: YOSYS NETLISTSVG\n\n# Run all steps\nrun_all: iverilog vvp schematic\n\n# Clean build directory\nclean:\n    rm -rf $(BUILD_DIR)\n    rm -f $(CIRCUIT_STRUCT)_tb.vcd\n\nAfter modifying the files, simply execute:\nmake clean\nmake run_all\n\nThis will generate all the relevant files:\n\n\n\nProject File Structure"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html#first-project-in-vhdl",
    "href": "posts/digital-analysis-mac/index.html#first-project-in-vhdl",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "3 First Project in VHDL",
    "text": "3 First Project in VHDL\n\n3.1 Install GHDL Compiler\n\nSimilar to Verilog, VHDL is also a hardware description language. Compiling it requires another tool: GHDL. Installing it on macOS can be tricky. The following steps have been tested on an M2 Mac (as of 2024-09-01):\n\nInstall vhdl using brew:\nbrew install vhdl\n\nvhdl has two versions: LLVM and mcode. The LLVM version has some issues on macOS, and the brew-installed version uses LLVM, so we manually download the mcode version from here. I downloaded ghdl-macos-11-mcode.tgz.\nExtract it by double-clicking, and you will get three files:\n\n\n\nGHDL-mcode package\n\n\n\nCopy and paste these three files to the following path: /opt/homebrew/Caskroom/ghdl/4.1.0:\n\n\n\nCopy and Paste\n\n\nIn the terminal, type:\nghdl --version\n\nIf you encounter security prompts, go to System Settings &gt; Privacy & Security to allow access:\n\n\n\nAllow Access in Privacy & Security\n\n\n\n\n3.2 Compilation, Linking and Simulation\n\nUnlike Verilog, VHDL requires an additional Linking step, which connects the component declarations with their implementation files (testbench). Why doesn’t Verilog require this? Because the testbench file in Verilog includes the declaration contents (include \"GatedDLatch.v\"), so it links automatically.\n\nIn Verilog, we used two separate tools (iverilog and vvp) for compilation and simulation. However, for VHDL, we only need one tool: GHDL.\n\nCreate a new folder VHDLDemo, and within it, create two files: demo.vhdl and demo_tb.vhdl. The former describes the circuit structure, and the latter serves as the testbench file (you can also use .vhd as the suffix). Add the following content to the respective files:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity demo is\n    port (\n        A : in  STD_LOGIC;\n        B : in  STD_LOGIC;\n        O : out STD_LOGIC\n    );\nend demo;\n\narchitecture Behavioral of demo is\nbegin\n    O &lt;= not (A and B); -- NAND gate\nend Behavioral;\nand\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity demo_tb is\nend demo_tb;\n\narchitecture Behavioral of demo_tb is\n    signal A : STD_LOGIC := '0';\n    signal B : STD_LOGIC := '0';\n    signal O : STD_LOGIC;\n\n    -- Instantiate the unit under test (UUT)\n    component demo\n        port (\n            A : in  STD_LOGIC;\n            B : in  STD_LOGIC;\n            O : out STD_LOGIC\n        );\n    end component;\nbegin\n    UUT: demo port map (\n        A =&gt; A,\n        B =&gt; B,\n        O =&gt; O\n    );\n\n    -- Test process\n    process\n    begin\n        -- Test case 1: A = 0, B = 0\n        A &lt;= '0';\n        B &lt;= '0';\n        wait for 10 ns;\n        \n        -- Test case 2: A = 0, B = 1\n        A &lt;= '0';\n        B &lt;= '1';\n        wait for 10 ns;\n\n        -- Test case 3: A = 1, B = 0\n        A &lt;= '1';\n        B &lt;= '0';\n        wait for 10 ns;\n\n        -- Test case 4: A = 1, B = 1\n        A &lt;= '1';\n        B &lt;= '1';\n        wait for 10 ns;\n\n        -- End of simulation\n        wait;\n    end process;\nend Behavioral;\n\nCreate a Makefile to automate the process:\n# Description: Makefile for VHDLDemo\nCIRCUIT = demo\nTB = demo_tb\nBUILD_DIR = build\n\n# Ensure the build directory exists\n$(BUILD_DIR):\n    mkdir -p $(BUILD_DIR)\n\n# Compilation: compile the design and testbench\nghdl_compile: $(BUILD_DIR)\n    ghdl -a --workdir=$(BUILD_DIR) $(CIRCUIT).vhdl\n    ghdl -a --workdir=$(BUILD_DIR) $(TB).vhdl\n\n# Linking: Elaborate the design and testbench\nghdl_elab: ghdl_compile\n    ghdl -e --workdir=$(BUILD_DIR) $(TB)\n\n# Simulation: simulate the testbench\nghdl_simulate: ghdl_elab\n    ghdl -r --workdir=$(BUILD_DIR) $(TB) --vcd=$(BUILD_DIR)/$(TB).vcd\n\n### These cannot work for now##############################################\n# # Synthesis: generate circuit structure configuration file (.json), you should have ghdl plugin installed for yosys, but I have error: \"ERROR: No such command: ghdl\" or \"dyld[5264]: missing symbol called\", possible solution could be to install yosys from source, but not sure\n# YOSYS: $(CIRCUIT).vhdl | $(BUILD_DIR)\n#   yosys -p \"ghdl $(CIRCUIT); prep -top $(CIRCUIT); write_json -compat-int $(BUILD_DIR)/$(CIRCUIT).json\" $(CIRCUIT).vhdl\n\n# # Visualization: generate human readable (.svg) from .json\n# NETLISTSVG: $(BUILD_DIR)/$(CIRCUIT).json  | $(BUILD_DIR)\n#   netlistsvg $(BUILD_DIR)/$(CIRCUIT).json -o $(BUILD_DIR)/$(CIRCUIT).svg\n\n# # Schematic diagram only: Synthesis then Visualization\n# schematic: YOSYS NETLISTSVG\n\n# # Run all steps\n# run_all: ghdl_compile ghdl_elab ghdl_simulate YOSYS NETLISTSVG\n### These cannot work for now##############################################\n\n# Run compilation, linking and simulation\nrun_cls: ghdl_compile ghdl_elab ghdl_simulate\n\n# Clean build directory\nclean:\n    rm -rf $(BUILD_DIR)\n    rm -f $(TB)\n\n# Experiment: Run testbench without the design file\nrun_tb_only: $(BUILD_DIR)\n    ghdl -a --workdir=$(BUILD_DIR) $(TB).vhdl\n    ghdl -r --workdir=$(BUILD_DIR) $(TB) --vcd=$(BUILD_DIR)/$(TB)_no_design.vcd\n\nThe running process is very clear. In the command line, execute:\nmake clean\nmake run_cls\n\nThis will complete the process.\n\nIf you want to see what happens if you skip compiling and linking the design file (demo.vhdl), run:\nmake clean # Cannot omitted!\nmake run_tb_only\n\nYou can compare the two .vcd files generated in VSCode or using GTKWave; they are different as expected.\n\n\n3.3 No Synthesis and Visualization Plan\n\nNote that this Makefile does not include steps to generate a schematic diagram because yosys requires a ghdl plugin. Currently, the integration is not very stable. You can refer to ghdl-yosys-plugin and building-ghdl for more details. However, the suggested methods have been tested on M2 Mac and result in errors:\n /----------------------------------------------------------------------------\\\n |  yosys -- Yosys Open SYnthesis Suite                                       |\n |  Copyright (C) 2012 - 2024  Claire Xenia Wolf &lt;claire@yosyshq.com&gt;         |\n |  Distributed under an ISC-like license, type \"license\" to see terms        |\n \\----------------------------------------------------------------------------/\n Yosys 0.44 (git sha1 80ba43d26, clang++ 15.0.0 -fPIC -O3)\n\n-- Running command `ghdl demo_tb.vhdl -e demo_tb; prep -top demo_tb; write_json demo.json' --\n\n1. Executing GHDL.\ndyld[5264]: missing symbol called\nzsh: abort      yosys -m ghdl -p\n\nThis issue has also been mentioned in the Issues section of ghdl-yosys-plugin, but there is no solution yet.\n\nPossible solutions might include compiling and installing yosys from source, ensuring the correct versions of yosys and ghdl, or checking if any component of the FPGA Toolchain is missing. Alternatively, you could try converting VHDL to Verilog using some tool (like GPT) and then synthesizing the schematics."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html",
    "href": "posts/inverted-pendulum/index.html",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "",
    "text": "Control block diagram\n\n\nThe objective of this article is to provide a self-contained guide to building a linear feedback control system for the classical inverted pendulum problem in MATLAB, i.e., to design a controller by applying a force to the cart \\(M\\) to balance the pendulum at the upright position (shown in Figure 1). We will use the following standard equations of a control system: \\[\n\\begin{aligned}\n    \\dot{\\mathbf{x}} &= A \\mathbf{x} + B \\mathbf{u} \\\\\n    \\mathbf{y} &= C \\mathbf{x} + D \\mathbf{u} \\\\\n    \\mathbf{u} &= -K \\mathbf{y}.\n\\end{aligned}\n\\]\nA brief description of the symbols and their physical interpretation is given in the following table. If you have difficulty understanding their meanings, don’t worry, feel free to scan the next section.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nThis Article\n\n\n\n\n\\(\\mathbf{x} \\in TM\\)\nSystem state vector in the tangent bundle of dimension \\(n\\).\n\\(\\mathbf{x} = [x, \\dot{x}, \\theta, \\dot{\\theta}]^t \\in T(\\mathbb{R}^1 \\times \\mathbb{S}^2)\\)\n\n\n\\(\\mathbf{u} \\in TM_c\\)\nThe values control knobs (“actuators”) in a space of dimension \\(q\\) (\\(q &lt; n\\)).\nForce on the cart \\(\\mathbf{u} = \\mathbf{F} \\in \\mathbb{R}\\).\n\n\n\\(\\mathbf{y} \\in TM_o\\)\nThe observable output vector of the system (the values that we can measure) in a space of dimension \\(p\\) (\\(p &lt; n\\)).\nAll dimension of \\(\\mathbf{x}\\) is observable \\(\\mathbf{y} = \\mathbf{x}\\).\n\n\n\\(A^{n \\times n}\\)\nThe linear infinitesimal generator of the system.\nModelling and linearization needed, see Equation 5.\n\n\n\\(B^{n \\times q}\\)\nHow the control knobs affect the state vector.\nForce analyzing needed, see Equation 6.\n\n\n\\(C^{p \\times n}\\)\nConvert the state vector to what we can actually measure.\n\\(C = I\\).\n\n\n\\(D^{p \\times q}\\)\nSometimes the control knobs affect the system observable output directly.\nApplying the force has no direct effect on the observable \\(D = 0\\).\n\n\n\\(K^{q \\times p}\\)\nThe linear feedback matrix. The observable \\(\\mathbf{y}\\) is mapped linearly to the control knobs.\nWe use LQR to optimize this matrix."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#intro",
    "href": "posts/inverted-pendulum/index.html#intro",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "",
    "text": "Control block diagram\n\n\nThe objective of this article is to provide a self-contained guide to building a linear feedback control system for the classical inverted pendulum problem in MATLAB, i.e., to design a controller by applying a force to the cart \\(M\\) to balance the pendulum at the upright position (shown in Figure 1). We will use the following standard equations of a control system: \\[\n\\begin{aligned}\n    \\dot{\\mathbf{x}} &= A \\mathbf{x} + B \\mathbf{u} \\\\\n    \\mathbf{y} &= C \\mathbf{x} + D \\mathbf{u} \\\\\n    \\mathbf{u} &= -K \\mathbf{y}.\n\\end{aligned}\n\\]\nA brief description of the symbols and their physical interpretation is given in the following table. If you have difficulty understanding their meanings, don’t worry, feel free to scan the next section.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nThis Article\n\n\n\n\n\\(\\mathbf{x} \\in TM\\)\nSystem state vector in the tangent bundle of dimension \\(n\\).\n\\(\\mathbf{x} = [x, \\dot{x}, \\theta, \\dot{\\theta}]^t \\in T(\\mathbb{R}^1 \\times \\mathbb{S}^2)\\)\n\n\n\\(\\mathbf{u} \\in TM_c\\)\nThe values control knobs (“actuators”) in a space of dimension \\(q\\) (\\(q &lt; n\\)).\nForce on the cart \\(\\mathbf{u} = \\mathbf{F} \\in \\mathbb{R}\\).\n\n\n\\(\\mathbf{y} \\in TM_o\\)\nThe observable output vector of the system (the values that we can measure) in a space of dimension \\(p\\) (\\(p &lt; n\\)).\nAll dimension of \\(\\mathbf{x}\\) is observable \\(\\mathbf{y} = \\mathbf{x}\\).\n\n\n\\(A^{n \\times n}\\)\nThe linear infinitesimal generator of the system.\nModelling and linearization needed, see Equation 5.\n\n\n\\(B^{n \\times q}\\)\nHow the control knobs affect the state vector.\nForce analyzing needed, see Equation 6.\n\n\n\\(C^{p \\times n}\\)\nConvert the state vector to what we can actually measure.\n\\(C = I\\).\n\n\n\\(D^{p \\times q}\\)\nSometimes the control knobs affect the system observable output directly.\nApplying the force has no direct effect on the observable \\(D = 0\\).\n\n\n\\(K^{q \\times p}\\)\nThe linear feedback matrix. The observable \\(\\mathbf{y}\\) is mapped linearly to the control knobs.\nWe use LQR to optimize this matrix."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#physics-model",
    "href": "posts/inverted-pendulum/index.html#physics-model",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "2 Physics Model",
    "text": "2 Physics Model\nIn this section, we will derive the equations of motion for the cart-pendulum system using Lagrangian Mechanics. I hope Figure 1 will be enough to explain the notations used in this article.\n\n\n\n\n\n\nFigure 1: Cart-pendulum Model\n\n\n\nThe dynamics of the system can be computed using Lagrangian Mechanics:\n\\[\n\\boxed{\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\partial L}{\\partial q_i} - \\frac{\\partial L}{\\partial q_i} = Q_i^{\\text{non-conservative}},\n}\n\\tag{1}\\] where \\(q_i\\) is either \\(x\\) or \\(\\theta\\) in our case. The lagrangian \\(L\\) is system kinetic energy minus potential energy: \\[\nL = T - V.\n\\]\n\n2.1 Kinetic Energy\nThe total kinetic energy of the system is the sum of the kinetic energy of the cart and the kinetic energy of the pendulum: \\[\nT = T_M + T_m.\n\\]\nFirst we compute the position of \\(m\\): \\[\n\\mathbf{r}_m = \\begin{bmatrix} x + \\ell \\sin \\theta \\\\ -\\ell \\cos \\theta \\end{bmatrix}.\n\\]\nTherefore, \\[\n\\begin{aligned}\n    T_m &= \\frac{1}{2} m \\dot{\\mathbf{r}}_m^T \\dot{\\mathbf{r}}_m \\\\\n    &= \\frac{1}{2} m \\left[(\\dot{x} + \\ell \\dot{\\theta} \\cos \\theta)^2 + (\\ell \\dot{\\theta} \\sin \\theta)^2\\right] \\\\\n    &= \\frac{1}{2} m \\left[ \\dot{x}^2 + \\ell^2 \\dot{\\theta}^2 + 2 \\ell \\dot{x} \\dot{\\theta} \\cos \\theta \\right].\n\\end{aligned}\n\\]\nThe kinetic energy of \\(M\\) is easy: \\[\nT_M = \\frac{1}{2} M \\dot{x}^2.\n\\]\nThe total kinetic energy is: \\[\nT = \\frac{1}{2} (M + m) \\dot{x}^2 + \\frac{1}{2} m \\ell^2 \\dot{\\theta}^2 + m \\ell \\dot{x} \\dot{\\theta} \\cos \\theta.\n\\]\n\n\n2.2 Potential Energy\nOnly the pendulum has potential energy: \\[\nV = - m g \\ell \\cos \\theta.\n\\]\n\n\n2.3 Euler-Lagrange Equation\nThe lagrangian of the system is: \\[\n\\begin{aligned}\n    L &= T - V \\\\\n    &= \\frac{1}{2} (M + m) \\dot{x}^2 + \\frac{1}{2} m \\ell^2 \\dot{\\theta}^2 + m \\ell \\dot{x} \\dot{\\theta} \\cos \\theta + m g \\ell \\cos \\theta.\n\\end{aligned}\n\\]\n\n2.3.1 Euler-Lagrange in \\(x\\)\nFor \\(q_1 = x\\): \\[\n\\begin{aligned}\n    \\frac{\\partial L}{\\partial \\dot{x}} &= (M + m) \\dot{x} + m \\ell \\dot{\\theta} \\cos \\theta \\\\\n    \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\frac{\\partial L}{\\partial \\dot{x}} \\right) &= (M + m) \\ddot{x} + m \\ell (-\\sin \\theta \\dot{\\theta}^2 + \\cos \\theta \\ddot{\\theta}) \\\\\n    \\frac{\\partial L}{\\partial x} &= 0 \\\\\n    Q_x^{\\text{non-conservative}} &= F - b \\dot{x}.\n\\end{aligned}\n\\]\nBy Equation 1, we have: \\[\n(M + m) \\ddot{x} + m \\ell (-\\sin \\theta \\dot{\\theta}^2 + \\cos \\theta \\ddot{\\theta}) = F - b \\dot{x}.\n\\tag{2}\\]\n\n\n2.3.2 Euler-Lagrange in \\(\\theta\\)\nFor \\(q_2 = \\theta\\): \\[\n\\begin{aligned}\n    \\frac{\\partial L}{\\partial \\dot{\\theta}} &= m \\ell^2 \\dot{\\theta} + m \\ell \\dot{x} \\cos \\theta \\\\\n    \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\frac{\\partial L}{\\partial \\dot{\\theta}} \\right) &= m \\ell^2 \\ddot{\\theta} + m \\ell (\\ddot{x} \\cos \\theta - \\dot{x} \\sin \\theta \\dot{\\theta}) \\\\\n    \\frac{\\partial L}{\\partial \\theta} &= - m g \\ell \\sin \\theta - m \\ell \\dot{x} \\sin \\theta \\dot{\\theta} \\\\\n    Q_{\\theta}^{\\text{non-conservative}} &= 0.\n\\end{aligned}\n\\]\nBy Equation 1, we have: \\[\nm \\ell \\ddot{x} \\cos \\theta + m \\ell^2 \\ddot{\\theta} + m g \\ell \\sin \\theta = 0.\n\\tag{3}\\]\n\n\n2.3.3 Matrix Form\nWrite Equation 2 and Equation 3 in matrix form: \\[\n\\begin{bmatrix}\n    M + m & m \\ell \\cos \\theta \\\\\n    m \\ell \\cos \\theta & m \\ell^2\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\ddot{x} \\\\ \\ddot{\\theta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    F - b \\dot{x} + m \\ell \\sin \\theta \\dot{\\theta}^2 \\\\\n    - m g \\ell \\sin \\theta\n\\end{bmatrix}.\n\\tag{4}\\]\n\n\n\n2.4 Matlab Code\nThe following function computes the state-space model of the inverted pendulum:\n\n\ninvpend.m\n\n% x -&gt; state vector (x, xdot, theta, thetadot)\n% m -&gt; mass of pendulum\n% M -&gt; mass of cart\n% L -&gt; length of pendulum\n% g -&gt; gravity\n% b -&gt; friction = -b*xdot\n% F -&gt; force applied to cart\n% dx -&gt; derivative of state vector\n\nfunction dx = invpend(x, m, M, L, g, b, F)\n    % Unpack state variables\n    x1 = x(1);     % cart position\n    x2 = x(2);     % cart velocity (xdot)\n    x3 = x(3);     % pendulum angle (theta)\n    x4 = x(4);     % angular velocity (thetadot)\n    \n    % Precompute useful terms\n    sin_theta = sin(x3);\n    cos_theta = cos(x3);\n    theta_dot = x4;\n    \n    % Mass matrix\n    D = [M + m,         m * L * cos_theta;\n         m * L * cos_theta,   m * L^2];\n\n    % Right-hand side (forces/accelerations)\n    RHS = [F - b * x2 + m * L * sin_theta * theta_dot^2;\n           -m * g * L * sin_theta];\n\n    % Solve for accelerations\n    accel = D \\ RHS;  % Equivalent to inv(D) * RHS, but more stable\n    \n    % Return time derivative of state vector\n    dx = zeros(4, 1);\n    dx(1) = x2;         % xdot\n    dx(2) = accel(1);   % xddot\n    dx(3) = x4;         % thetadot\n    dx(4) = accel(2);   % thetaddot\nend\n\nThe following function simulates the motion of the system without damping and external force:\n\n\nsimulation_invpend.m\n\nclear all; close all; clc;\n\n% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 4;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 30;          % damping coefficient\ntime = 0:.1:10; % Time samples\n\n% Initial conditions\nx0 = [0; 0; .2; 0]; % x, xdot, theta, thetadot\n\n% Solve ODE\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, 0), time, x0);\n\n% Animation\nfor k = 1:length(t)\n    invpend_plot(x(k, :), L, true, 'simulation_invpend.gif');\nend\n\n\n\n\n\n\n\n\ninvpend_plot() function\n\n\n\n\n\nfunction invpend_plot(x, L, saveGif, gifFileName)\n    % Extract state variables\n    cart_x = x(1);    % Cart position\n    theta = x(3);     % Pendulum angle\n    \n    % Compute pendulum position\n    pend_x = cart_x + L * sin(theta);\n    pend_y = L * cos(theta);\n    \n    % Figure setup\n    clf; hold on; axis equal; grid on;\n    xlim([-2 2]); ylim([-1.5 1.5]);\n    \n    % Draw cart\n    cart_w = 0.4; cart_h = 0.2;\n    rectangle('Position', [cart_x - cart_w/2, -cart_h/2, cart_w, cart_h], ...\n              'Curvature', 0.1, 'FaceColor', [0.5 0.5 0.5]);\n    \n    % Draw pendulum\n    plot([cart_x, pend_x], [0, pend_y], 'k-', 'LineWidth', 2); % Rod\n    plot(pend_x, pend_y, 'ro', 'MarkerSize', 10, 'MarkerFaceColor', 'r'); % Mass\n    \n    % Draw ground\n    plot([-2 2], [0, 0], 'k', 'LineWidth', 2);\n    \n    % If saveGif is true, save the current frame to a GIF file\n    persistent gifFile frameCount currentGifName\n    if nargin &lt; 3\n        saveGif = false;\n    end\n    \n    if saveGif\n        % Set default filename if not provided\n        if nargin &lt; 4 || isempty(gifFileName)\n            gifFileName = 'invpend_animation.gif';\n        end\n        \n        % Reset frame count if this is a new gif file\n        if isempty(currentGifName) || ~strcmp(currentGifName, gifFileName)\n            frameCount = 1;\n            currentGifName = gifFileName;\n        else\n            frameCount = frameCount + 1;\n        end\n        \n        % Capture the current figure as an image\n        frame = getframe(gcf);\n        im = frame2im(frame);\n        [imind, cm] = rgb2ind(im, 256);\n        \n        % Write to the GIF file\n        if frameCount == 1\n            imwrite(imind, cm, gifFileName, 'gif', 'Loopcount', inf, 'DelayTime', 0.1);\n        else\n            imwrite(imind, cm, gifFileName, 'gif', 'WriteMode', 'append', 'DelayTime', 0.1);\n        end\n    end\n    \n    drawnow;\nend\n\n\n\n\n\n\n\nSimulation result of simulation_invpend.m"
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#linearized-phase-space-model",
    "href": "posts/inverted-pendulum/index.html#linearized-phase-space-model",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "3 Linearized Phase Space Model",
    "text": "3 Linearized Phase Space Model\n\n3.1 Linearization of the original dynamics\nIn this section, we build the linear version of the original system (without control) in the phase space.\nFirst of all, the state of the system can be described in a vector: \\[\n\\mathbf{x} = \\begin{bmatrix}\nx \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\n\\end{bmatrix}\n\\in T (\\mathbb{R}^1 \\times \\mathbb{S}^1)\n\\]\nNow we want to use a linear model \\(\\dot{\\mathbf{x}} = \\mathbf{Ax}\\) to analyze the system at \\(\\theta = 0\\), i.e., we want to find matrix \\(A \\in \\mathbb{C}^{4 \\times 4}\\) s.t., \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\n\\begin{bmatrix}\n    x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\n\\end{bmatrix}\n=\n\\overbrace{\n\\begin{bmatrix}\n    \\dot{x} \\\\ \\ddot{x} \\\\ \\dot{\\theta} \\\\ \\ddot{\\theta}\n\\end{bmatrix}\n}^{\\dot{\\mathbf{x}}}\n=\n\\overbrace{\n\\begin{bmatrix}\n    0 & 1 & 0 & 0 \\\\\n    ? & ? & ? & ? \\\\\n    0 & 0 & 0 & 1 \\\\\n    ? & ? & ? & ?\n\\end{bmatrix}\n}^{A}\n\\overbrace{\n\\begin{bmatrix}\n    x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\n\\end{bmatrix}\n}^{\\mathbf{x}}\n\\]\nNote the the first and third row are trivial identities, we only need to derive the second and fourth row of \\(A\\). We already solved Equation 4 in invpend.m. But now we have to solve it explicitly to derive the partial derivatives for the jacobian: \\[\n\\begin{bmatrix}\n    \\ddot{x} \\\\ \\ddot{\\theta}\n\\end{bmatrix}\n=\n\\phi (x, \\dot{x}, \\theta, \\dot{\\theta})\n=\n\\begin{bmatrix}\n    \\frac{F - b \\dot{x} + m \\ell \\sin \\theta \\dot{\\theta}^2 - mg \\cos \\theta \\sin \\theta}{M + m \\sin^2 \\theta} \\\\\n    \\frac{- \\cos \\theta \\left(F - b \\dot{x} + m \\ell \\sin \\theta \\dot{\\theta}^2\\right) + (M+m) g \\ell \\sin \\theta}{M \\ell + m \\ell \\sin^2 \\theta}\n\\end{bmatrix}.\n\\]\nNow obviously the function \\(\\phi\\) is non-linear. Its jacobian1 at phase point \\(\\mathbf{x}_{\\text{up}} = [x, 0, 0, 0]^t\\) is: \\[\n\\begin{aligned}\n    [J]\n    &=\n    \\begin{bmatrix}\n    \\frac{\\partial \\phi_1}{\\partial x} & \\frac{\\partial \\phi_1}{\\partial \\dot{x}} & \\frac{\\partial \\phi_1}{\\partial \\theta} & \\frac{\\partial \\phi_1}{\\partial \\dot{\\theta}} \\\\\n    \\frac{\\partial \\phi_2}{\\partial x} & \\frac{\\partial \\phi_2}{\\partial \\dot{x}} & \\frac{\\partial \\phi_2}{\\partial \\theta} & \\frac{\\partial \\phi_2}{\\partial \\dot{\\theta}}\n    \\end{bmatrix}_{\\mathbf{x} = \\mathbf{x}_{\\text{up}}} \\\\\n    &=\n    \\begin{bmatrix}\n    0 & -\\frac{b}{M} & -\\frac{mg}{M} & 0 \\\\\n    0 & -\\frac{b}{M \\ell} & -\\frac{(m + M)g}{M \\ell} & 0\n    \\end{bmatrix}.\n\\end{aligned}\n\\]\n1 Just to refresh, the Jacobian \\(J\\) of a (non-linear) function \\(\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at point \\(p \\in \\mathbb{R}^n\\) can be viewed as the local transformation from the neighborhood of \\(p\\) to the neighborhood of \\(\\phi(p)\\). \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\) are not viewed as vector spaces but manifolds. In our case it’s a little weird to compute the jacobian of \\(A\\) since \\(A\\) defines a vector field, not mapping between manifolds. Or it is? \\(A\\) is indeed a mapping between manifolds! Because vector field themselves are a section of the tangent bundle. By the way, this jacobian actually takes me a lot of effort to compute.Therefore, \\[\nA =\n\\begin{bmatrix}\n    0 & 1 & 0 & 0 \\\\\n    0 & -\\frac{b}{M} & -\\frac{mg}{M} & 0 \\\\\n    0 & 0 & 0 & 1 \\\\\n    0 & -\\frac{b}{M \\ell} & -\\frac{(m + M)g}{M \\ell} & 0\n\\end{bmatrix}.\n\\tag{5}\\]\n\n\n3.2 Deriving matrix \\(B\\) for control\nThe only control knob is \\(\\mathbf{u} = \\mathbf{F}\\), the force on the cart.\n\n\n\nThe control knob \\(\\mathbf{F}\\) affects \\(\\dot{x}\\) and \\(\\dot{\\theta}\\)\n\n\nBy Newton’s second law, it’s obvious that \\[\nB \\mathbf{u} =\n\\begin{bmatrix}\n    0 \\\\ \\frac{1}{M} \\\\ 0 \\\\ -\\frac{1}{M \\ell}\n\\end{bmatrix} F.\n\\tag{6}\\]\nEach entry of \\(B\\) in Equation 6 is explained in the following table:\n\n\n\n\n\n\n\nEntry\nExplanation\n\n\n\n\nB(1,1) = 0\nThe velocity \\(\\dot{x}\\) of the cart \\(M\\) will not suddenly change due to the force \\(F\\).\n\n\nB(2,1) = \\(\\frac{1}{M}\\)\nThe acceleration \\(\\ddot{x}\\) flash to \\(\\frac{F}{M}\\) due to \\(F\\).\n\n\nB(3,1) = 0\nThe angular velocity \\(\\dot{\\theta}\\) of the pendulum \\(m\\) will not suddenly change due to the force \\(F\\).\n\n\nB(4,1) = \\(-\\frac{1}{M \\ell}\\)\nThe angular acceleration \\(\\ddot{\\theta}\\) of the pendulum \\(m\\) flash to \\(-\\frac{F}{M \\ell}\\) due to \\(F\\). The pendulum is moving backward relative to the cart, hence the negative sign."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#controllability-of-the-system",
    "href": "posts/inverted-pendulum/index.html#controllability-of-the-system",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "4 Controllability of the system",
    "text": "4 Controllability of the system\n\n4.1 Original system stability\nFrom intuition, the system where \\(\\theta = 0\\) is unstable. How to know that from Equation 5? We know that the system \\(\\dot{\\mathbf{x}} = A \\mathbf{x}\\) is stable iff all the eigenvalues of \\(A\\) are rigorously negative. The instability of the system can be verified by the following Matlab code:\n\n\noriginal_system_stability.m\n\nclear all; close all; clc;\n\n% Simulation parameters\nm = 3;          % Mass of pendulum\nM = 5;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 5;          % damping coefficient\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\neig(A)\n\n\n\n\n\n\n\n\nResult of original_system_stability.m\n\n\n\n\n\nans =\n         0\n   -4.1883\n   -0.6157\n    3.8040\nSince there are two non-negative eigenvalues, the system is unstable.\n\n\n\n\n\n\n4.2 Controllability\nOne great thing about feedback is that we can change the fundamental dynamics of the system, changing its eigenvalues2 to make it stable (as shown in Figure 2).\n2 Also called poles for historical reasons.\n\n\n\n\n\nFigure 2: Feedback control change the underlying dynamics of the system\n\n\n\nThe system with linear feedback is: \\[\n\\begin{aligned}\n    \\dot{\\mathbf{x}} &= A \\mathbf{x} + B \\mathbf{u} \\\\\n    \\mathbf{u} &= -K \\mathbf{x},\n\\end{aligned}\n\\] or \\[\n\\dot{\\mathbf{x}} = (A - B K) \\mathbf{x},\n\\] whose dynamics could be very different from the original system \\(\\dot{\\mathbf{x}} = A \\mathbf{x}\\).\nWe want to take full control of the system, i.e., does there exist some \\(\\mathbf{F} = \\mathbf{u}(t)\\) to drive the system state point to anywhere in the phase space? In other words, is the system controllable?\n\n\n\n\n\n\n\nControllability\n\n\n\n\nTheorem 1 The following statements are equivalent:\n\nThe system is controllable.\nIts controllability matrix \\(\\mathcal{C}\\) is full rank, \\[\n\\mathcal{C} :=\n\\begin{bmatrix}\nB & AB & A^2 B & \\cdots & A^{n-1} B\n\\end{bmatrix}.\n\\]\nThe poles of the system can be placed arbitrarily3, i.e., the matrix \\(A - B K\\) could have arbitrary eigenvalues.\n\n\nThe reachability space \\(\\mathcal{R}\\) is the full phase space, \\[\n\\mathcal{R} := \\{\\mathbf{\\xi} \\in TM: \\exists \\text{ input } \\mathbf{u}(t) \\text{ s.t. } \\mathbf{x}(t) = \\mathbf{\\xi}\\}\n\\]\n\n\n\n\n\n3 In fact, there is a built-in Matlab function K = place(A, B, desired_eigs_vec) to help you place the poles of the system to any desired locations.By Theorem 1, the system is controllable iff ctrb(A, B) has rank \\(4\\). In fact, the following Matlab code can verify this.\n\n\nsys_controllability.m\n\nclear all; close all; clc;\n\n% Simulation parameters\nm = 3;          % Mass of pendulum\nM = 5;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 5;          % damping coefficient\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\nrank(ctrb(A, B))\n\n\n\n\n\n\n\n\nResult of sys_controllability.m\n\n\n\n\n\nans =\n     4\nSince \\(\\mathcal{C}=\\) ctrb(A, B) has full rank \\(4\\), the system \\(\\dot{\\mathbf{x}} = (A - B K) \\mathbf{x}\\) is controllable."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#finding-the-feedback-matrix-k",
    "href": "posts/inverted-pendulum/index.html#finding-the-feedback-matrix-k",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "5 Finding the feedback matrix \\(K\\)",
    "text": "5 Finding the feedback matrix \\(K\\)\n\n5.1 Random pole placement\nFor controllable systems, Theorem 1 also guarantees that we can place the poles of the system to any desired locations, say desired_eigs_vec = [-3; -4; -5; -6], just randomly some numbers in the left half of the complex plane, to ensure the stability of the system. We use place() in Matlab to find such \\(K\\) and use that \\(K\\) to simulate the control effect of the system.\n\n\nsimulation_linear_control.m\n\nclear all; close all; clc;\n\n%% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 10;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 2;          % damping coefficient\ntime = 0:.1:6; % Time samples\n\n%% Initial conditions\nx0 = [0; 0; -.4; 0]; % x, xdot, theta, thetadot\n\n%% pole placement\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\ndesired_eigs_vec = [-3; -4; -5; -6];\nK = place(A, B, desired_eigs_vec);\n\n% Just to verify that the poles are where we want them\neig(A - B*K)\n\n%% Solve ODE\ndesired_state_vec = [1; 0; 0; 0];\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, -K * (x - desired_state_vec)), time, x0);\n\n%% Animation\nfor k = 1:length(t)\n    invpend_plot(x(k, :), L, true, 'simulation_linear_control.gif');\nend\n\n\n\n\n\n\n\nFigure 3: Simulation result of simulation_linear_control.m\n\n\n\nWe can see in Figure 3, the inverted pendulum is able to be stabilized at the status \\(\\mathbf{x} = [1, 0, 0, 0]^t\\).\nBy changing the desired eigenvalues, we can adjust the convergent speed of the system.\n\n\n\n\n\n\n\nMatlab code for changing the desired eigenvalues\n\n\n\n\n\nclear all; close all; clc;\n\n%% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 10;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 2;          % damping coefficient\ntime = 0:.1:12; % Time samples\n\n%% Initial conditions\nx0 = [0; 0; -.4; 0]; % x, xdot, theta, thetadot\n\n%% pole placement\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\ndesired_eigs_vec_1 = [-1; -2; -3; -4];\ndesired_eigs_vec_2 = [-2; -3; -4; -5];\ndesired_eigs_vec_3 = [-3; -4; -5; -6];\ndesired_eigs_vec_4 = [-4; -5; -6; -7];\n\nK = place(A, B, desired_eigs_vec_4);\n\n% Just to verify that the poles are where we want them\neig(A - B*K)\n\n%% Solve ODE\ndesired_state_vec = [1; 0; 0; 0];\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, -K * (x - desired_state_vec)), time, x0);\n\n%% Animation\nfor k = 1:length(t)\n    % invpend_plot(x(k, :), L, true, 'convergence_speed_4.gif');\n    invpend_plot(x(k, :), L, false);\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\ndesired_eigs_vec_1 = [-1; -2; -3; -4]\n\n\n\n\n\n\n\ndesired_eigs_vec_2 = [-2; -3; -4; -5]\n\n\n\n\n\n\n\n\n\ndesired_eigs_vec_3 = [-3; -4; -5; -6]\n\n\n\n\n\n\n\ndesired_eigs_vec_4 = [-4; -5; -6; -7]\n\n\n\n\n\nWe can see that more negative eigenvalues lead to faster convergence, at the risk of breaking the linearity limit and making the system less robust. So we need to make a trade-off between the convergence speed and the robustness (convergence speed is usually limited by the power of the control). Also, we may want to change the convergence style: whether to make the cart move as quickly as possible, or save as much energy as much as possible as long as the pendulum is inverted? Thankfully there is a way to find the required and “optimal” (in a sense) eigenvalues – the Linear Quadratic Regulator (LQR).\n\n\n5.2 Linear Quadratic Regulator (LQR)\nThe idea is to define a metric (“loss function” in optimization jargon) to measure the badness of the eigenvalues (either too less robust or cost a lot to control), and then minimize that metric. Someone came up with this weird Equation 7:\n\\[\n\\tilde{J} := \\int_0^\\infty (\\mathbf{x}^t Q \\mathbf{x} + \\mathbf{u}^t R \\mathbf{u}) \\ \\mathrm{d}t,\n\\tag{7}\\] where \\(Q^{n \\times n}\\) and \\(R^{q \\times q}\\) are symmetric, positive-definite matrices, which typically are diagonal. In our example, we could choose \\[\nQ =\n\\begin{bmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 5 & 0 \\\\\n    0 & 0 & 0 & 70\n\\end{bmatrix},\n\\quad\nR = 0.5,\n\\] which we put bigger penalties on \\(\\theta\\) and \\(\\dot{\\theta}\\) because we want them to converge to their expected value quickly. For the position \\(x\\) and velocity \\(\\dot{x}\\), we don’t care much, so we put a small penalty on them. Also we put a penalty on the control input \\(F\\) since our control device is able to provide large enough instantaneous force and we don’t care the expenditure of energy.\nTherefore, \\(\\tilde{J}\\) will be: \\[\n\\tilde{J} = \\int_0^\\infty (x^2 + \\dot{x}^2 + 5 \\theta^2 + 70 \\dot{\\theta}^2 + 0.5 F^2) \\  \\mathrm{d}t.\n\\]\nNow we are using Linear feedback to Regulate a system to minimize a Quadratic loss function, hence the name Linear Quadratic Regulator (LQR).\nThe solution to this LQR problem deserves a separate lecture, the solution is given by the Algebraic Riccati Equation. But in Matlab, there is a one-line command we can use to solve the corresponding \\(K\\) matrix:\nK = lqr(A, B, Q, R)\n\n\n5.3 Simulation by changing Q matrix\nWe simulate the dynamics result with different \\(Q\\) matrices and fixed \\(R = 0.01\\):\n\n\n\n\n\n\n\nMatlab code for comparing different Q matrices\n\n\n\n\n\nclear all; close all; clc;\n\n%% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 10;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 2;          % damping coefficient\ntime = 0:.4:100; % Time samples\n\n%% Initial conditions\nx0 = [0; 0; -.4; 0]; % x, xdot, theta, thetadot\n\n%% LQR pole placement\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\n% Define penalty Q and R matrix\n% Desired: Move the cart to the desired position as fast as possible\n% Q = [400, 0,  0,  0;\n%      0, 50,  0,  0;\n%      0, 0,  1,  0;\n%      0, 0,  0,  1];\n\n% Desired: Move the cart as smooth as possible\nQ = [20, 0,  0,  0;\n     0, 400,  0,  0;\n     0, 0,  1,  0;\n     0, 0,  0,  1];\n\n% Desired: Don't change the angle so much\n% Q = [4, 0,  0,  0;\n%      0, 4,  0,  0;\n%      0, 0,  500,  0;\n%      0, 0,  0,  50];\n\n% Desired: Save energy\nR = .01;\n\nK = lqr(A, B, Q, R)\n\n% Just to verify the stability of the closed loop system\neig(A - B*K)\n\n%% Solve ODE\ndesired_state_vec = [1; 0; 0; 0];\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, -K * (x - desired_state_vec)), time, x0);\n\n%% Animation\nfor k = 1:length(t)\n    invpend_plot(x(k, :), L, true, 'simulation_lqr_4.gif');\n    % invpend_plot(x(k, :), L, false);\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Q = \\operatorname{diag}(400, 50, 1, 1)\\): Fast lock in\n\n\n\n\n\n\n\n\\(Q = \\operatorname{diag}(20, 400, 1, 1)\\): Smooth cart movement\n\n\n\n\n\n\n\n\\(Q = \\operatorname{diag}(4, 4, 500, 50)\\): Keep straight up\n\n\n\n\n\n\n\n5.4 Simulation by changing R values\nWe simulate the dynamics result with fixed \\(Q - \\operatorname{diag} (20, 400, 1, 1)\\) matrix and different \\(R\\) values:\n\n\n\n\n\n\n\n\n\n\\(R = 0.001\\): More control power\n\n\n\n\n\n\n\n\n\n\\(R = 1.5\\): Median control power\n\n\n\n\n\n\n\n\n\n\\(R = 5\\): Least control power"
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#references",
    "href": "posts/inverted-pendulum/index.html#references",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "References",
    "text": "References\n\n\nÅström, Karl Johan, and Richard M Murray. 2021. Feedback Systems. Princeton University Press.\n\n\nBrunton, Steven L, and Jose Nathan Kutz. 2019. Data-Driven Science and Engineering : Machine Learning, Dynamical Systems, and Control. Cambridge University Press.\n\n\nPowell, J David. 2012. Feedback Control of Dynamic Systems : International Version/Matlab & Simulink. Pearson Education Limited."
  },
  {
    "objectID": "posts/java-to-launchpad/index.html",
    "href": "posts/java-to-launchpad/index.html",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "",
    "text": "我想在launchpad上面启动一个 java 项目，但是application只支持启动 .app 文件，怎么办？"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#intro",
    "href": "posts/java-to-launchpad/index.html#intro",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "",
    "text": "我想在launchpad上面启动一个 java 项目，但是application只支持启动 .app 文件，怎么办？"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#解决方案",
    "href": "posts/java-to-launchpad/index.html#解决方案",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "2 解决方案",
    "text": "2 解决方案\nspotlight 搜索 Automator，打开 Automator，按 command+W 关闭弹出的窗口：\n\n在导航栏中再次打开 Automator，选择 Application：\n\n搜索栏中搜索 Run AppleScript，拖拽到右侧的空白区域:\n\n加入以下内容：\non run {input, parameters}\n    set p to POSIX path of (path to me)\n    do shell script \"java -jar \" & p & \"/Contents/Java/YOURJARFILE.jar\"\n \nend run\n记得替换 YOURJARFILE.jar 为你的 jar 文件名。但是此时 \"/Contents/Java/ 这个路径是不存在的，而且 YOURJARFILE.jar 也没有在这个路径下，所以以后我们需要创建这个路径。但是首先我们先保存(Command+S)这个文件为一个 .app 文件，路径为 /Application，文件名为你期待这个app的名字，如：YOURJARFILE.app。\n进入 /Application/YOURJARFILE.app，右键点击 Show Package Contents，在 Contents 文件夹下创建 java 文件夹，将你的 YOURJARFILE.jar 拷贝放入这个文件夹。\n\n然后你就可以通过 launchpad .app 文件启动你的 java 项目了。"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#改图标",
    "href": "posts/java-to-launchpad/index.html#改图标",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "3 改图标",
    "text": "3 改图标\n还是在 Contents 文件夹下，把这个 .icns 文件换成你自己的图标文件，比如可以把 png 文件转换成 icns 文件，然后替换这个文件(文件名不变)。\n\n然后 Refresh the Icon Cache (if necessary):\ntouch /path/to/YOURJARFILE.app\nkillall Dock\nFinish!"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#references",
    "href": "posts/java-to-launchpad/index.html#references",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "4 References",
    "text": "4 References\n\nHow to convert .jar to .app on Mac - a Java tutorial\nLaunching a jar file as an app on Mac (from the dock)"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html",
    "href": "posts/least-squares-as-projection/index.html",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "",
    "text": "The goal is to find the linear model \\(y = \\beta_0 + \\beta_1 x\\) such that the sum of squared errors between the predicted values and the actual data is minimized."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#introduction",
    "href": "posts/least-squares-as-projection/index.html#introduction",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "",
    "text": "The goal is to find the linear model \\(y = \\beta_0 + \\beta_1 x\\) such that the sum of squared errors between the predicted values and the actual data is minimized."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#linear-model",
    "href": "posts/least-squares-as-projection/index.html#linear-model",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "2 Linear Model",
    "text": "2 Linear Model\nThe form of the linear model is:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nwhere \\(y_i\\) is the observed value, \\(x_i\\) is the independent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\epsilon_i\\) is the error term.\nWe wish to find \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted values \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\) minimize the sum of squared errors between \\(\\hat{y}_i\\) and the observed values \\(y_i\\)."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#design-matrix-and-observation-vector",
    "href": "posts/least-squares-as-projection/index.html#design-matrix-and-observation-vector",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "3 Design Matrix and Observation Vector",
    "text": "3 Design Matrix and Observation Vector\nTo make the problem more convenient, we represent it using vectors and matrices.\n\n3.1 Design Matrix\nDefine the design matrix \\(\\mathbf{X}\\) as:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n\\]\nThe first column contains only 1s, representing the constant term \\(\\beta_0\\), and the second column contains the values of the independent variable \\(x_i\\).\n\n\n3.2 Observation Vector\nDefine the observation vector \\(\\mathbf{y}\\) as:\n\\[\n\\mathbf{y} = \\begin{bmatrix}\n2 \\\\\n3 \\\\\n5 \\\\\n7\n\\end{bmatrix}\n\\]\nThis vector contains all the observed values \\(y_i\\).\n\n\n3.3 Parameter Vector\nDefine the parameter vector \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\)."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#sum-of-squared-errors-objective-function",
    "href": "posts/least-squares-as-projection/index.html#sum-of-squared-errors-objective-function",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "4 Sum of Squared Errors Objective Function",
    "text": "4 Sum of Squared Errors Objective Function\nIn regression, our goal is to find the parameters \\(\\boldsymbol{\\beta}\\) such that the predicted values \\(\\hat{\\mathbf{y}} = \\mathbf{X} \\boldsymbol{\\beta}\\) are as close as possible to the observed values \\(\\mathbf{y}\\), by minimizing the sum of squared errors (SSE):\n\\[\nS(\\beta_0, \\beta_1) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n\\]"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#deriving-the-normal-equation",
    "href": "posts/least-squares-as-projection/index.html#deriving-the-normal-equation",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "5 Deriving the Normal Equation",
    "text": "5 Deriving the Normal Equation\nThe key idea of least squares is to find \\(\\boldsymbol{\\beta}\\) such that the residual \\(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\) is minimized. Geometrically, this means that the residual should be orthogonal to the column space of the design matrix \\(\\mathbf{X}\\), which leads to the normal equation:\n\\[\n\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}) = 0\n\\]\nExpanding this:\n\\[\n\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\n\\]\nThis is the normal equation, which can be solved to find the least squares estimate \\(\\hat{\\boldsymbol{\\beta}}\\)."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#solving-the-normal-equation",
    "href": "posts/least-squares-as-projection/index.html#solving-the-normal-equation",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "6 Solving the Normal Equation",
    "text": "6 Solving the Normal Equation\nNow, let’s compute the parts of the normal equation.\n\n6.1 Compute \\(\\mathbf{X}^T \\mathbf{X}\\)\n\\[\n\\mathbf{X}^T \\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n= \\begin{bmatrix}\n4 & 10 \\\\\n10 & 30\n\\end{bmatrix}\n\\]\n\n\n6.2 Compute \\(\\mathbf{X}^T \\mathbf{y}\\)\n\\[\n\\mathbf{X}^T \\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\\n3 \\\\\n5 \\\\\n7\n\\end{bmatrix}\n= \\begin{bmatrix}\n17 \\\\\n50\n\\end{bmatrix}\n\\]\n\n\n6.3 Solve the Normal Equation\nNow we solve the normal equation:\n\\[\n\\begin{bmatrix}\n4 & 10 \\\\\n10 & 30\n\\end{bmatrix} \\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n17 \\\\\n50\n\\end{bmatrix}\n\\]\nTo solve this, we first compute the inverse of \\(\\mathbf{X}^T \\mathbf{X}\\):\n\\[\n(\\mathbf{X}^T \\mathbf{X})^{-1} = \\frac{1}{(4)(30) - (10)(10)} \\begin{bmatrix}\n30 & -10 \\\\\n-10 & 4\n\\end{bmatrix} = \\frac{1}{20} \\begin{bmatrix}\n30 & -10 \\\\\n-10 & 4\n\\end{bmatrix}\n\\]\nNext, we compute \\(\\hat{\\boldsymbol{\\beta}}\\):\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n\\]\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\frac{1}{20} \\begin{bmatrix}\n30 & -10 \\\\\n-10 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n17 \\\\\n50\n\\end{bmatrix}\n= \\frac{1}{20} \\begin{bmatrix}\n(30)(17) + (-10)(50) \\\\\n(-10)(17) + (4)(50)\n\\end{bmatrix}\n\\]\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\frac{1}{20} \\begin{bmatrix}\n510 - 500 \\\\\n-170 + 200\n\\end{bmatrix}\n= \\frac{1}{20} \\begin{bmatrix}\n10 \\\\\n30\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.5 \\\\\n1.5\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#least-squares-estimate",
    "href": "posts/least-squares-as-projection/index.html#least-squares-estimate",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "7 Least Squares Estimate",
    "text": "7 Least Squares Estimate\nBy solving the normal equation, we find \\(\\hat{\\beta}_0 = 0.5\\) and \\(\\hat{\\beta}_1 = 1.5\\). Thus, the regression model is:\n\\[\n\\hat{y} = 0.5 + 1.5x\n\\]"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#conclusion",
    "href": "posts/least-squares-as-projection/index.html#conclusion",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nUsing the projection approach, we see that the least squares estimate is the projection of the observation vector \\(\\mathbf{y}\\) onto the space spanned by the columns of the design matrix \\(\\mathbf{X}\\). By solving the normal equation, we found the parameters \\(\\hat{\\beta}_0 = 0.5\\) and \\(\\hat{\\beta}_1 = 1.5\\), which minimize the sum of squared errors."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#problem-formulation",
    "href": "posts/pde-solve-laplace-equation/index.html#problem-formulation",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "1 Problem Formulation",
    "text": "1 Problem Formulation\nSuppose we have a rectangular sheet of steel sized \\(p \\times q\\). We somehow force the temperature on the boundary of the sheet to be some deterministic function \\(f, g, h, w\\) shown in Figure 1. When the system settles stablely, what is the temperature distribution \\(v(x, y)\\) inside the sheet?\n\n\n\n\n\n\nFigure 1: How to solve the temperature distribution given boundary condition?\n\n\n\nSince the steady heat distribution is governed by the Laplace’s equation. The problem is basically to:\n\nSolve the Laplace’s equation given a (Dirichlet) boundary condition1 (temperature functions):\\[ \\nabla^2 v = 0. \\]\n1 See here for different types of boundary conditions."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#existence-and-uniqueness-of-solutions",
    "href": "posts/pde-solve-laplace-equation/index.html#existence-and-uniqueness-of-solutions",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "2 Existence and uniqueness of solutions",
    "text": "2 Existence and uniqueness of solutions\nIt’s good habit to always check the existence and uniqueness of a mathematical object.\n\nExistence: Does the solution exist?\nIn this case, one can guess by life experience that the solution exists since no matter how we force the temperature on the boundary, there must be a temperature distribution inside the sheet.\nUniqueness: Does the temperature distribution settles to a unique solution?\nThis is not immediately obvious2. Could there exist two different stable temperature distributions? The answer is no.\n\n2 Not every PDE has a unique solution. For example, the uniqueness of the famous Navier-Stokes equation is still unsolved."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#linearity-simplifies-the-problem",
    "href": "posts/pde-solve-laplace-equation/index.html#linearity-simplifies-the-problem",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "3 Linearity simplifies the problem",
    "text": "3 Linearity simplifies the problem\nStare Figure 2 for a while, you will find we just need to solve the steady heat distribution \\(F(x, y)\\) with an easier boundary condition: \\[\n\\begin{align}\nF(0, y) &= 0, & y \\in [0, q] \\tag{BC1} \\\\\nF(0, y) &= 0, & y \\in [0, q] \\tag{BC2} \\\\\nF(x, 0) &= 0, & x \\in [0, p] \\tag{BC3} \\\\\nF(x, q) &= f(x), & x \\in [0, p] \\tag{BC4}\n\\end{align}\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 2: Linearity of Laplace’s equation simplifies the boundary condition\n\n\n\n\n\n\nSimulate the subproblem"
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#separation-of-variables-sov",
    "href": "posts/pde-solve-laplace-equation/index.html#separation-of-variables-sov",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "4 Separation of variables (SoV)",
    "text": "4 Separation of variables (SoV)\nSuppose \\(F(x,y)\\) could be written as: \\[\nF(x,y) = A(x)B(y)\n\\tag{2}\\]\n\n\n\n\n\n\n\nUniqueness legitimize seperation of variables!\n\n\n\n\nDefinition 1 Equation 2 is such a bold and unreasonable assumption. Why is that??\n\nIt literally doesn’t make any sense, until it indeed gives a solution!\n\nThe seperation of variables is just a guess. You will later find that if we continue reasoning using this assumption, we will eventually arrive at a solution that satisfies both the Laplace’s equation and the boundary condition.\nHere is the point: The uniqueness theorem guarantees that the solution is unique!, which means the solution we just got is the only solution, though we have no fluent logic to derive it! This is sort of like:\nImagine your primary school teacher ask you to solve the equation: \\[\n2x + 3 = 1.\n\\tag{3}\\]\nYou have no idea how to solve it, but you guess that \\(x\\) satisfies: \\[\nx + 1 = 0,\n\\] and you get \\(x = -1\\), which indeed satisfies Equation 3. By Fundamental theorem of algebra, Equation 3 has and only has one solution. Therefore, its solution is \\(x = -1\\). This is a valid reasoning!!\n\n\n\n\nThen we plugin Equation 2 into the Laplace’s equation: \\[\n\\begin{align*}\n&\\nabla^2 (AB) = 0 \\\\\n\\implies\\quad &\\frac{\\partial^2 (AB)}{\\partial x^2} + \\frac{\\partial^2 (AB)}{\\partial y^2} = 0 \\\\\n\\implies\\quad &A_{xx}(x)B(y) + A(x)B_{yy}(y) = 0 \\\\\n\\implies\\quad &\\underbrace{\\frac{A_{xx}(x)}{A(x)}}_{\\text{function of }x} = \\underbrace{-\\frac{B_{yy}(y)}{B(y)}}_{\\text{function of }y}, \\quad \\forall (x, y) \\in [0, p] \\times [0, q].\n\\end{align*}\n\\]\nNow here is a very important step, for all point in the rectangle, a function of \\(x\\) equals a function of \\(y\\), always. This means they must be constant functions as shown in Figure 3.\n\\[\n\\boxed{\n\\frac{A_{xx}(x)}{A(x)} = -\\frac{B_{yy}(y)}{B(y)} = \\text{const} \\equiv \\lambda.\n}\n\\tag{4}\\]\nIf we denote the constant as \\(\\lambda\\), this is actually problematic because \\(\\lambda\\) could be different numbers as long as \\(A_{xx}(x)/A(x)\\) and \\(-B_{yy}(y)/B(y)\\) are equal. We will later see that there are countably \\(\\lambda\\) values that are available and we shall use series to represent the solution.\n\n\n\n\n\n\nFigure 3: A function of \\(x\\) equals a function of \\(y\\) pointwise forces them constant functions\n\n\n\nNow we get two ODEs from Equation 4: \\[\n\\begin{cases} A_{xx} = \\lambda A \\\\\nB_{yy} = -\\lambda B\n\\end{cases}\n\\tag{5}\\]\nRefer to Section 7 for the solution of this kind of ODE."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#boundary-conditions-applied",
    "href": "posts/pde-solve-laplace-equation/index.html#boundary-conditions-applied",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "5 Boundary conditions applied",
    "text": "5 Boundary conditions applied\n\n5.1 Determine the sign of \\(\\lambda\\)\n\n\n\n\n\n\n\nClaim\n\n\n\n\nProposition 1 We claim that: \\[\n\\lambda \\le 0.\n\\]\n\nThis is because if \\(\\lambda &gt; 0\\), \\[\nA(x) \\in \\operatorname{span}_\\mathbb{R}\\{\\cosh(\\sqrt{\\lambda} x), \\sinh(\\sqrt{\\lambda} x)\\}.\n\\] But hyperbolic functions look like the right graph of Figure 5, there is no element in \\(\\operatorname{span}_\\mathbb{R}\\{\\cosh(\\sqrt{\\lambda} x), \\sinh(\\sqrt{\\lambda} x)\\}\\) that equals \\(0\\) at both ends of the rectangle. Only periodic sinusoidal functions can do that.\n\n\n\nFrom Proposition 1, Equation 5 implies \\[\n\\begin{cases} A(x) \\in \\operatorname{span}_\\mathbb{R}\\{\\cos(\\omega x), \\sin(\\omega x)\\} \\cup \\{\\alpha_1 x + \\beta_1\\} \\\\\nB(y) \\in \\operatorname{span}_\\mathbb{R}\\{\\cosh(\\omega y), \\sinh(\\omega y)\\} \\cup \\{\\alpha_2 y + \\beta_2\\}\n\\end{cases}\n\\tag{6}\\] where \\(-\\lambda = \\omega^2\\)\n\n\n5.2 Determine valid \\(\\lambda\\) values\n\n5.2.1 Left and right boundary conditions\nLet’s start with the easier boundary conditions @BC1 and @BC2.\n\\(A(x)\\) in Equation 6 are just sinusoidal functions or linear functions. If we force it to be zero on the edges, all of a sudden \\(\\alpha_1\\) and \\(\\beta_1\\) should be both zero, and the coefficients before \\(\\cos(\\omega x)\\) also must be zero, otherwise the function will never be zero at the line \\(x=0\\). Also the horizontal length \\(p\\) must be integer multiple of half period of \\(\\sin(\\omega x)\\), i.e., \\[\nn \\cdot \\frac{\\pi}{\\omega} = p, \\quad n = 1, 2, 3, \\cdots.\n\\]\nTherefore, there are only countably many valid \\(\\omega\\), hence countably many valid \\(\\lambda\\): \\[\n\\lambda = -\\omega^2 \\in \\left\\{ -\\frac{n^2 \\pi^2}{p^2} \\right\\}_{n=1}^\\infty.\n\\]\nSo \\(A(x)\\) in Equation 6 is in a subspace: \\[\n\\boxed{\nA(x) \\in \\operatorname{span}_\\mathbb{R}\\left\\{\\sin\\left(\\frac{n \\pi}{p} x\\right)\\right\\}}\n\\]\nAs shown in Figure 4, \\(A(x)\\) is like the harmonics on a string.\n\n\n\n\n\n\nFigure 4: Mental picture for \\(A(x)\\) and \\(B(y)\\)\n\n\n\n\n\n5.2.2 Top and bottom boundary conditions\nOn top and bottom edges, we have the boundary condition (BC3) and (BC4) in Equation 1.\n\\(B(y)\\) in Equation 6 must obey these boundary conditions. (BC3) in Equation 1 forces \\(\\beta_2 = 0\\), and the coefficients before \\(\\cosh(\\omega y)\\) to be zero. So \\(B(y)\\) in Equation 6 is in a subspace: \\[\n\\boxed{\nB(y) \\in \\operatorname{span}_\\mathbb{R}\\left\\{\\sinh\\left(\\frac{n \\pi}{p} y\\right)\\right\\} \\cup \\{\\alpha_2 y\\}}\n\\]\n\n\n\n5.3 Fourier series for the undetermined coefficients\nWe have not used the boundary condition (BC4) in Equation 1 yet. Actually, it leads to the introduction to Fourier series! Let’s write what does \\(F(x,y)\\) looks like up to now: \\[\n\\begin{aligned}\nF(x, y) &= A(x)B(y) \\\\\n&= c \\sin\\left(\\frac{n \\pi}{p} x\\right) \\sinh\\left(\\frac{n \\pi}{p} y\\right) \\\\\n\\text{or } &= c' 0 \\cdot y\n\\end{aligned}\n\\tag{7}\\] where \\(c\\) depends on the choice of \\(n\\), \\(n\\) can be any positive integer. Now we have a very important claim:\n\n\n\n\n\n\n\nClaim\n\n\n\n\nProposition 2 If we add these valid \\(F\\) together, the sum is still valid: \\[\n\\boxed{\nF(x,y) = \\sum_{n=1}^\\infty c_n \\sin\\left(\\frac{n \\pi}{p} x\\right) \\sinh\\left(\\frac{n \\pi}{p} y\\right).}\n\\tag{8}\\]\n\nThink about why!?\nEach solution in Equation 7 is a solution to the Laplace’s equation, right? By the superposition principle, their sum is also a solution to the Laplace’s equation!\n\n\n\nHere is the most beautiful part: We plugin (BC4) in Equation 1 into Equation 8: \\[\nF(x, q) = \\sum_{n=1}^\\infty \\underbrace{c_n \\sinh\\left(\\frac{n \\pi}{p} q\\right)}_{\\mathclap{\\scriptsize \\text{Fourier coefficients of }f(x)}} \\sin\\left(\\frac{n \\pi}{p} x\\right) = f(x).\n\\tag{9}\\]\nThe \\(c_n \\sinh(n \\pi q/p)\\) are just constants, which is exactly the Fourier coefficients of the function \\(f(x)\\)!\n\n\n\n\n\n\n\nDeriving \\(c_n\\) through Fourier analysis\n\n\n\n\n\nWe know that a real-valued function \\(f(x)\\) of period \\(T\\) could be decomposed as: \\[\nf(x) = a_0 + \\sum_{n=1}^\\infty a_n \\cos(n \\omega_0 x) + \\sum_{n=1}^\\infty b_n \\sin(n \\omega_0 x),\n\\tag{10}\\] where \\[\n\\begin{cases}\na_n &= \\frac{2}{T} \\langle f, \\cos(n \\omega_0 x) \\rangle \\\\\nb_n &= \\frac{2}{T} \\langle f, \\sin(n \\omega_0 x) \\rangle \\\\\na_0 &= \\frac{1}{T} \\langle f, 1 \\rangle.\n\\end{cases}\n\\tag{11}\\]\nWe consider the function \\(f(x)\\) to be a periodic function with period \\(T=2p\\) (not \\(p\\)) since we need \\[\n\\omega_0 = \\frac{\\pi}{p}.\n\\]\nNow we know that all \\(a_n = 0\\). Compare Equation 11 with Equation 9, \\[\nb_n = \\frac{2}{2p} \\left\\langle f, \\sin\\left(\\frac{n \\pi}{p}x\\right) \\right\\rangle = c_n \\sinh\\left(\\frac{n \\pi}{p} q\\right).\n\\] Solve for \\(c_n\\): \\[\n\\begin{aligned}\nc_n &= \\frac{1}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\left\\langle f, \\sin\\left(\\frac{n \\pi}{p}x\\right) \\right\\rangle \\\\\n&= \\frac{1}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\int_0^{2p} f(x) \\sin\\left(\\frac{n \\pi}{p}x\\right) \\mathrm{d}x \\\\\n&= \\frac{2}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\int_0^{p} f(x) \\sin\\left(\\frac{n \\pi}{p}x\\right) \\mathrm{d}x.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#conclusion",
    "href": "posts/pde-solve-laplace-equation/index.html#conclusion",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThe solution to the simplified boundary condition subproblem is: \\[\nF(x,y) = \\sum_{n=1}^\\infty c_n \\sin\\left(\\frac{n \\pi}{p} x\\right) \\sinh\\left(\\frac{n \\pi}{p} y\\right),\n\\tag{12}\\]\nwhere \\(c_n\\) satisfies: \\[\nc_n = \\frac{2}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\int_0^{p} f(x) \\sin\\left(\\frac{n \\pi}{p}x\\right) \\mathrm{d}x.\n\\]\nBy the superposition principle (shown in Figure 2), the solution to the original problem is the linear combination of the solutions in the form of Equation 12."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#sec-2nd-order-ode-refresh",
    "href": "posts/pde-solve-laplace-equation/index.html#sec-2nd-order-ode-refresh",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "7 2nd-order ODE Refresh",
    "text": "7 2nd-order ODE Refresh\n\n\n7.1 Complex domain solution\nConsider the following 2nd-order ODE: \\[\n\\ddot{x} = \\lambda x,\n\\tag{13}\\] where \\(x(t) \\in \\mathbb{C}, \\lambda \\in \\mathbb{R}\\). What function satisfies this special property that if we differentiate it twice, we get itself up to a constant? Exponentials! So we guess: \\[\nx(t) = e^{rt}\n\\] with \\[\nr = \\pm \\sqrt{\\lambda}.\n\\]\nTherefore, the general solution to Equation 13 is: \\[\nx(t) = c_1 e^{\\sqrt{\\lambda} t} + c_2 e^{-\\sqrt{\\lambda} t},\n\\tag{14}\\] where \\(c_1, c_2 \\in \\mathbb{C}\\) are constants. Done!\nWe can use different notation according to the sign of \\(\\lambda\\):\n\n\\(\\lambda &lt; 0\\): Let \\(-\\lambda = \\omega^2\\), then Equation 14 can also be written as: \\[\nx(t) = c_1' \\cos(\\omega t) + c_2' \\sin(\\omega t) \\quad (c_1', c_2' \\in \\mathbb{C})\n  \\tag{15}\\] since \\[\n\\operatorname{span}_\\mathbb{C}\\{e^{i\\omega t}, e^{-i\\omega t}\\} = \\operatorname{span}_\\mathbb{C}\\{\\cos(\\omega t), \\sin(\\omega t)\\}\n\\]\n\\(\\lambda &gt; 0\\): Let \\(\\lambda = a^2\\), then Equation 14 can also be written as: \\[\nx(t) = c_1'' \\cosh(at) + c_2'' \\sinh(at) \\quad (c_1'', c_2'' \\in \\mathbb{C})\n  \\tag{16}\\] since \\[\n\\operatorname{span}_\\mathbb{C}\\{e^{at}, e^{-at}\\} = \\operatorname{span}_\\mathbb{C}\\{\\cosh(at), \\sinh(at)\\}.\n\\]\n\\(\\lambda = 0\\):\n\n\\[\n\\begin{align*}\n&\\ddot{x} = 0 \\\\\n\\implies\\quad &\\dot{x} = \\alpha \\\\\n\\implies\\quad &x(t) = \\alpha x + \\beta\n\\end{align*}\n\\]\n\n\n\n\n\n\nFigure 5: The graph of trigonometric and hyperbolic functions\n\n\n\n\n\n7.2 Real domain solution\nThe solution of Equation 13 when \\(x(t)\\) is confined to \\(\\mathbb{R}\\) is very simple. One just need to confine \\(c_1', c_2'\\) in Equation 15 and \\(c_1'', c_2''\\) in Equation 16 to \\(\\mathbb{R}\\): \\[\nx(t) = c_1 \\cos(\\omega t) + c_2 \\sin(\\omega t) \\quad (\\lambda &lt; 0, -\\lambda = \\omega^2, c_1, c_2 \\in \\mathbb{R})\n\\] and \\[\nx(t) = c_1 \\cosh(at) + c_2 \\sinh(at) \\quad (\\lambda &gt; 0, \\lambda = a^2, c_1, c_2 \\in \\mathbb{R}).\n\\]"
  },
  {
    "objectID": "posts/side-ios/index.html",
    "href": "posts/side-ios/index.html",
    "title": "在 ios 下载第三方应用",
    "section": "",
    "text": "CLAIM: 更新(2-22-2025), SideStore在iPhone上会闪退!! 以下操作无法保证稳定运行!"
  },
  {
    "objectID": "posts/side-ios/index.html#说明",
    "href": "posts/side-ios/index.html#说明",
    "title": "在 ios 下载第三方应用",
    "section": "1 说明",
    "text": "1 说明\n下面的操作已在欧版iphone 16 pro ios 18.1上测试成功，但我个人感觉这个操作跟欧版iPhone没有什么关系，其他版本的iPhone大概率应该也可以，但是不保证。\n本文参考链接：\n\nlist of tools\nsideloadly\nsidestore"
  },
  {
    "objectID": "posts/side-ios/index.html#what-is-side-downloading",
    "href": "posts/side-ios/index.html#what-is-side-downloading",
    "title": "在 ios 下载第三方应用",
    "section": "2 What is side-downloading",
    "text": "2 What is side-downloading\nios系统不像安卓系统，只能在iOS store里面下载app。为了打破这点，有两种方法：越狱(jail breaking)或使用side-downloading。本文介绍后者。\nSide-downloading 指通过不越狱的方法将 iOS 应用程序直接安装到 Apple 设备上，而不需要通过 App Store。有很多工具可以实现这点，具体参考list of tools。\n其中 Scarlet 和 Signulous 的方法我没试过，另外三种 AltStore 需要你手机是欧版的、app地区是欧洲、你人也要在欧洲，所以没戏。最后两种 Sideloadly 和 Sidestore，后者下载软件需要连电脑，并且一般 side-downloading 的软件是要“续约(renew)”的，一般每周都要，而且 Sideloadly 这种方法还要连电脑 renew，不方便。所以我们采用最后一种 SideStore（只需要第一次的时候连电脑，后面在手机上下载和 renew）。当然我们也会用 sideloadly 下几个看看，毕竟这种比较简单。"
  },
  {
    "objectID": "posts/side-ios/index.html#sideloadly",
    "href": "posts/side-ios/index.html#sideloadly",
    "title": "在 ios 下载第三方应用",
    "section": "3 Sideloadly",
    "text": "3 Sideloadly\n在 windows 或 mac 上下载安装 sideloadly，图标如下：\n\n（可能要在 settings 中授权这个应用）\n这个只是个将软件下载到 iPhone 中的工具，而软件在哪里获取呢？我们需要 .ipa 后缀的文件，可以在这里搜索你要的软件，如果没有就在google上搜，最好注意下安全性问题。\n然后打开 sideloadly, 左侧选择你的 .ipa 文件，iDevice选择你的 iPhone（要用线将手机连到mac），输入你的Apple id（要记住这个ID，以后都用这个）和密码，点击start。（start 左边那个按钮是用来 renew 的，将来软件过期的时候就用同样的操作来 renew）"
  },
  {
    "objectID": "posts/side-ios/index.html#sidestore",
    "href": "posts/side-ios/index.html#sidestore",
    "title": "在 ios 下载第三方应用",
    "section": "4 Sidestore",
    "text": "4 Sidestore\n直接按照这里的指示下载安装即可：sidestore。\n其中有一步要产生一个配对文件，记得要将mac连上手机后再运行。如果mac上一直运行不出来，在 terminal 当前文件夹中输入：\nsudo ./jitterbugpair\n输入密码即可生成"
  },
  {
    "objectID": "posts/vector-derivation-relation/index.html",
    "href": "posts/vector-derivation-relation/index.html",
    "title": "Q&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?",
    "section": "",
    "text": "In differential geometry, we usually see a vector \\(v\\) is written as: \\[\nv = v^i \\frac{\\partial }{\\partial x^i} \\bigg\\rvert_p.\n\\]\nWhy does a vector naturally relates to partial derivatives?\n\n\n\n\n\n\n\nOne-line Solution\n\n\n\n\n\n\\[\nT_p (\\mathbb{R}^n) \\cong \\text{Der}_p (C^\\infty(\\mathbb{R}^n))\n\\]"
  },
  {
    "objectID": "posts/vector-derivation-relation/index.html#question",
    "href": "posts/vector-derivation-relation/index.html#question",
    "title": "Q&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?",
    "section": "",
    "text": "In differential geometry, we usually see a vector \\(v\\) is written as: \\[\nv = v^i \\frac{\\partial }{\\partial x^i} \\bigg\\rvert_p.\n\\]\nWhy does a vector naturally relates to partial derivatives?\n\n\n\n\n\n\n\nOne-line Solution\n\n\n\n\n\n\\[\nT_p (\\mathbb{R}^n) \\cong \\text{Der}_p (C^\\infty(\\mathbb{R}^n))\n\\]"
  },
  {
    "objectID": "posts/vector-derivation-relation/index.html#solution-from-derivative-to-derivation",
    "href": "posts/vector-derivation-relation/index.html#solution-from-derivative-to-derivation",
    "title": "Q&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?",
    "section": "2 Solution – From Derivative to Derivation",
    "text": "2 Solution – From Derivative to Derivation\n\n2.1 Directional derivative\nWe know from multivariable calculus that in high dimensions, we could not say the “derivative”, but the directional derivative of a function1. The directional derivative is a measure of how quickly the function value vary when we step a tiny nudge along a vector \\(v\\). Imagine we are at \\(p\\) in \\(\\mathbb{R}^3\\) and temperature is different everywhere. We are curiously about how this temperature field \\(f\\) changes in different directions. we move a tiny proportion2 along \\(v\\) (say \\(\\epsilon = 0.01 \\%\\)) and we feel the temperature changes by \\(\\Delta f = f(p+\\epsilon v)-f(p)\\). So we define the directional derivative of \\(f\\) along \\(v\\) is \\[\nD_{v} f |_p := \\lim_{\\epsilon \\to 0} \\frac{\\Delta f}{\\epsilon}.\n\\]\n1 “Scalar field” in fancier term. A scalar field in \\(\\mathbb{R}^n\\) is a map from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\).2 This is important! We are NOT moving a tiny bit but a tiny proportion, which means the length of \\(v\\) matters. Because if we move \\(0.01 \\%\\) on \\(v\\) and \\(2 v\\), \\(f\\) will vary \\(\\Delta f\\) and \\(2\\Delta f\\) and therefore the directional derivative of \\(f\\) along \\(2v\\) would be doubled! In some books, you will see we force \\(v\\) to be unit length, so we will not have this problem. But for me it’s unnecessary.3 We use upper indices to represent coordinate components and lower indices to represent basis vectors, so Equation 1 in usually notation is just \\[\nD_v f = \\langle \\frac{\\partial f}{\\partial x} \\hat{\\imath} + \\frac{\\partial f}{\\partial y} \\hat{\\jmath} +\\frac{\\partial f}{\\partial z} \\hat{k}, v_1 \\hat{\\imath} + v_2 \\hat{\\jmath} + v_3 \\hat{k} \\rangle.\n\\]It turns out that there is an explicit formula for directional derivatives: \\[\nD_{v} f = \\langle\\nabla f, v\\rangle,\n\\] i.e., the inner product between the gradient of \\(f\\) and \\(v\\). The direction of the \\(\\nabla f\\) is the steepest ascend of \\(f\\) at \\(p\\). In \\(\\mathbb{R}^3\\), this can be written as3 \\[\n\\begin{aligned}\n    D_v f &= \\langle \\frac{\\partial f}{\\partial x^1} e_1 + \\frac{\\partial f}{\\partial x^2} e_2 +\\frac{\\partial f}{\\partial x^3} e_3, v^1 e_1 + v^2 e_2 + v^3 e_3 \\rangle \\\\\n    &= v^1 \\frac{\\partial f}{\\partial x^1} + v^2 \\frac{\\partial f}{\\partial x^2} + v^3 \\frac{\\partial f}{\\partial x^3} \\\\\n    &= \\sum_i v^i \\frac{\\partial f}{\\partial x^i} \\\\\n    &=: v^i \\frac{\\partial f}{\\partial x^i}.\n\\end{aligned}\n\\tag{1}\\]\nThe last step in Equation 1 where we drop the summation notation is a convention called Einstein notation.\nWe could view \\(D_v f\\) as \\(v\\) acts on \\(f\\). Some textbook uses \\(v[f]\\) to represent this action, i.e., \\[\nv[f] := D_v f.\n\\]\n\n\n2.2 Derivation\nWe know a normal derivative satisfy so-called chain rule: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x}(fg) = \\frac{\\mathrm{d}f}{\\mathrm{d}x} g + f \\frac{\\mathrm{d}g}{\\mathrm{d}x}.\n\\]\nWe extract this property and define abstractly the derivation operator on an algebra as follows:\n\n\n\n\n\n\n\nDerivation on an Algebra\n\n\n\n\nDefinition 1 Let \\(A\\) be an algebra over field \\(\\mathbb{F}\\), a derivation is a linear map \\(D: A \\to A\\) s.t., \\[\nD(ab) = D(a)b + aD(b).\n\\]\n\n\n\n\nIt’s obvious that every \\(v\\) induces such a derivation on the algebra \\(C^\\infty_p\\) by a map \\(\\phi: v \\mapsto D_v\\). The question is: Does every derivation necessarily induced by a vector?\n\n\n\n\n\n\n\nVectors are Derivations\n\n\n\n\nTheorem 1 The space of all vectors emanating at \\(p\\) is isomorphic to the space of all derivations \\[\nT_p (\\mathbb{R}^n) \\cong \\text{Der}_p (C^\\infty(\\mathbb{R}^n)).\n\\]\n\n\n\n\nIn other words, every possible derivations on the algebra \\(C^\\infty(\\mathbb{R}^n)\\) is some directional derivative along \\(v \\in T_p (\\mathbb{R}^n)\\). Under this isomorphism, the basis vectors \\(e_i\\) is mapped to the partial derivative operator \\(\\frac{\\partial }{\\partial x^i}\\)!\nIn a general manifold \\(M\\), we actually use derivations to define tangent vectors on a manifold4. Because the concept of derivations are just functions that satisfy certain property, which is easy to define. While vectors seem exclusively belongs to Euclidean space. So :\n\n4 Tu’s book is a very good book of differential geometry for beginners, check it out!\n\n\n\n\n\nTangent Vector in a manifold\n\n\n\n\nDefinition 2 A tangent vector at a point \\(p\\) in a manifold \\(M\\) is a derivation at \\(p\\).\n\n\n\n\nThis is common in mathematics. We call this “stereotyping”, ah sorry, “abstraction”. We find two similar concepts (e.g. vectors and directional derivatives) on some object (euclidean space). But one of them (directional derivative) can be easily generalized to another objects (“manifold”). So then Mathematicians use some of its properties back to define itself axiomatically and called it the same name just to confuse people (“tangent vectors”)5. Or invent another name (e.g. topological space) just to be intimidating. Anyway, you will feel comfortable once you get used to them.\n\n\n5 Other examples include topological spaces, groups, \\(\\sigma\\)-algebra, “measurable spaces”, etc. These are just abstraction of open sets, closed stuff, events, volumes, etc."
  }
]
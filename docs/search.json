[
  {
    "objectID": "posts/vector-operation/index.html",
    "href": "posts/vector-operation/index.html",
    "title": "Three ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解",
    "section": "",
    "text": "This property of vector operation bother me for a loooooong time:\nMixed product1: \\[\n\\boxed{\na \\cdot (b \\times c) = b \\cdot (c \\times a) = c \\cdot (a \\times b).\n}\n\\tag{1}\\]\n1 Also known as “scalar triple product”.This says nothing but the mixed product is unchanged under a circular shift.\nYou will understand it in this article!"
  },
  {
    "objectID": "posts/vector-operation/index.html#hello",
    "href": "posts/vector-operation/index.html#hello",
    "title": "Three ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解",
    "section": "",
    "text": "This property of vector operation bother me for a loooooong time:\nMixed product1: \\[\n\\boxed{\na \\cdot (b \\times c) = b \\cdot (c \\times a) = c \\cdot (a \\times b).\n}\n\\tag{1}\\]\n1 Also known as “scalar triple product”.This says nothing but the mixed product is unchanged under a circular shift.\nYou will understand it in this article!"
  },
  {
    "objectID": "posts/vector-operation/index.html#three-approaches-to-the-mixed-product",
    "href": "posts/vector-operation/index.html#three-approaches-to-the-mixed-product",
    "title": "Three ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解",
    "section": "2 Three Approaches to the Mixed Product",
    "text": "2 Three Approaches to the Mixed Product\n\n\n\n\n\n\n\nLemma\n\n\n\n\nLemma 1 Let \\(a, b, c \\in \\mathbb{R}^3\\), then \\[\na \\cdot (b \\times c) =\n\\begin{vmatrix}\n| & | & | \\\\\na & b & c \\\\\n| & | & |\n\\end{vmatrix}\n\\]\n\n\n\n\nOnce this is established, we can use the fact that: \\[\n\\begin{vmatrix}\n| & | & | \\\\\na & b & c \\\\\n| & | & |\n\\end{vmatrix} =\n\\begin{vmatrix}\n| & | & | \\\\\nb & c & a \\\\\n| & | & |\n\\end{vmatrix} =\n\\begin{vmatrix}\n| & | & | \\\\\nc & a & b \\\\\n| & | & |\n\\end{vmatrix}\n\\] to prove Equation 1.\nNow let’s understand Lemma 1 in three different ways!\n\n2.1 Geometric Approach\nSee Figure 1, dot product make the slanted black box straight but maintains its volume.\n\n\n\n\n\n\nFigure 1: Geometric proof of Lemma 1\n\n\n\n\n\n2.2 3b1b Approach\nI called this “3b1b Approach” because this method is inspired by a great mathematician Grant Sanderson. “3b1b” is the name of his Youtube channel.\nIn a video created by him, there is a very interesting function \\(f: \\mathbb{R}^3 \\to \\mathbb{R}\\): \\[\nf(x) =\n\\begin{vmatrix}\n| & | & | \\\\\nx & v & w \\\\\n| & | & |\n\\end{vmatrix}\n\\tag{2}\\] which takes in a vector in \\(\\mathbb{R}^3\\) and output a number in \\(\\mathbb{R}\\) (\\(v\\) and \\(w\\) are predefined and fixed).2 Now I claim that:\n\n2 Functions from a vector space to a scalar is often referred to as a functional.\n\n\n\n\n\nClaim\n\n\n\n\nTheorem 1 The functional \\(f\\) in Equation 2 is linear, i.e., \\[\nf(x + y) = f(x) + f(y), \\forall x, y \\in \\mathbb{R}^3\n\\] and \\[\nf(\\alpha x) = \\alpha f(x), \\forall \\alpha \\in \\mathbb{R}.\n\\]\n\n\n\n\nThis is trivial.\nThere is another theorem3:\n\n3 There is a video created by myself that explains this in detail.\n\n\n\n\n\nRiesz Representation Theorem (reduced version)\n\n\n\n\nTheorem 2 Every linear functional in \\(\\mathbb{R}^n\\) induces a vector \\(p \\in \\mathbb{R}^n\\) such that \\[\nf(x) = x \\cdot p, \\forall x \\in \\mathbb{R}^n.\n\\]\n\n\n\n\nTherefore, Equation 2 becomes \\[\n\\boxed{\nx \\cdot p =\n\\begin{vmatrix}\n| & | & | \\\\\nx & v & w \\\\\n| & | & |\n\\end{vmatrix}\n}\n\\tag{3}\\]\n\n\n\n\n\n\n\nMnemonic device inspired by Equation 3\n\n\n\n\n\nThe Equation 3 gives us a way to write the cross product in a more “dot-product” way: \\[\nv \\times w =\n\\begin{vmatrix}\n\\hat{\\imath} & v_1 & w_1 \\\\\n\\hat{\\jmath} & v_2 & w_2 \\\\\n\\hat{k} & v_3 & w_3\n\\end{vmatrix}\n\\] because the result \\(v \\times w = (v_2w_3 - v_3w_2)\\hat{\\imath} + (v_3w_1 - v_1w_3)\\hat{\\jmath} + (v_1w_2 - v_2w_1)\\hat{k}\\) is very much like a dot product! Remember \\(v_2w_3 - v_3w_2, v_3w_1 - v_1w_3, v_1w_2 - v_2w_1\\) are just three numbers unless you you associate each number with a direction.\nIf you feel uncomfortable with this notation, there is more: \\[\n\\operatorname{curl} F\n\\equiv \\nabla \\times F =\n\\begin{vmatrix}\n\\hat{\\imath} & \\frac{\\partial}{\\partial x} & F_x \\\\\n\\hat{\\jmath} & \\frac{\\partial}{\\partial y} & F_y \\\\\n\\hat{k} & \\frac{\\partial}{\\partial z} & F_z\n\\end{vmatrix}\n\\] where we treat \\(\\nabla\\) like a vector! The first column is just an indicator that the result should be interpreted as a vector not a dot product. I took long time to think \\(\\nabla\\) as a kind of special vector, but I failed. Feel free to ignore the notation \\(\\nabla \\times F\\) if you don’t like it!\n\n\n\n\nWhat is \\(p\\) then? Well, \\(p\\) is a special vector in \\(\\mathbb{R}^3\\) such that the dot product with any vector \\(x\\) gives a number that is equal to the volume of a box spaned by \\(x\\), \\(v\\), and \\(w\\). This is a little bit mouthful, but we can see immediately from Figure 1 that \\(p\\) is just the blue vector, which is \\(v \\times w\\) in this case! So Equation 3 becomes: \\[\nx \\cdot (v \\times w) = \\begin{vmatrix}\n| & | & | \\\\\nx & v & w \\\\\n| & | & |\n\\end{vmatrix}\n\\]\nWe are done!\n\n\n2.3 Tensor Approach\nNow this explanation requires some knowledge about tensors4. But once you understand it, you will completely change your view on determinants! (If you did not respect them at all in the past anyway.)\n\n4 Just a quick joke, tensors are exactly linear functionals but we allow multiple vectors to be the input (instead of one).\n\n\n\n\n\nProposition\n\n\n\n\nProposition 1 The mapping \\(g(a, b, c) := a \\cdot (b \\times c)\\) and \\(\\det\\) are both alternating 3-tensor, i.e., \\[\ng, \\det \\in \\bigwedge\\nolimits^{\\!3} \\left(\\mathbb{R}^3\\right).\n\\tag{4}\\]\n\n\n\n\nWe also have this theorem:\n\n\n\n\n\n\n\nUniqueness of volume form\n\n\n\n\nTheorem 3 The dimension of the vector space of \\(k\\)-tensors on \\(\\mathbb{R}^n\\) is \\({n \\choose k}\\): \\[\n\\operatorname{dim} \\bigwedge\\nolimits^{\\!k} \\left(\\mathbb{R}^n\\right) = {n \\choose k}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDemonstration of Theorem 3\n\n\n\n\n\nPick \\(n = 3\\) and \\(k = 2\\). This is because in order to get the result of a \\(2\\)-tensor5 \\(B\\) acting on any two vectors of dimension \\(3\\), we would only need to specify the values of \\(B\\) acting on any \\(2\\) of the three basis vectors (\\(e_x, e_y, e_z\\)). How many numbers do we need? Well, only \\({3 \\choose 2} = 3\\) numbers: \\[\nB(e_x, e_y) = a_{12}, B(e_y, e_z) = a_{23}, B(e_z, e_x) = a_{31}.\n\\]\nThese are the “basis” of the space \\(\\bigwedge\\nolimits^{\\!2} \\left(\\mathbb{R}^3\\right)\\). So its dimension is \\(3\\). Then we just use the bilinear and alternating property of \\(B\\) to calculate the result of any two input vectors say \\(x = 2e_x + e_y - e_z\\) and \\(y = -e_x + 3e_z\\): \\[\n\\begin{aligned}\nB(x, y)\n&= B(2e_x + e_y - e_z, -e_x + 3e_z) \\\\\n&= B(2e_x + e_y - e_z, -e_x) + B(2e_x + e_y - e_z, 3e_z) \\\\\n&= -2B(e_x, e_x) - B(e_y, e_x) + B(e_z, e_x) \\\\\n&+ 6B(e_x, e_z) + 3B(e_y, e_z) - 3B(e_z, e_z) \\\\\n&= 0 - (-a_{12}) + a_{31} \\\\\n&+ 6(-a_{31}) + 3a_{23} - 0 \\\\\n&= a_{12} + 3a_{23} - 5a_{31}.\n\\end{aligned}\n\\]\n\n\n\n\n5 This is also called an alternating bilinear form.6 This type of tensor is also called “volume form”.As a special case6 when \\(n = k = 3\\), we have: \\[\n\\boxed{\n\\operatorname{dim} \\bigwedge\\nolimits^{\\!3} \\left(\\mathbb{R}^3\\right) = {3 \\choose 3} = 1.\n}\n\\]\nLet’s think about what this means. The determinant is a very special object that every volume form in \\(n\\) dimension is just a scalar multiple of it! In other words, every alternating \\(n\\)-tensor in \\(\\mathbb{R}^n\\) must be the determinant (up to a scalar)! So with Equation 4, both \\(g\\) and \\(\\det\\) are volume forms! So \\(g\\) must be a scalar multiple of \\(\\det\\): \\[\ng = k \\det.\n\\]\nWe can evaluate \\(k\\) by choosing a very special set of \\(a, b, c \\in \\mathbb{R}^3\\), say \\(a = e_x, b = e_y, c = e_z\\): \\[\ng(e_x, e_y, e_z) = e_x \\cdot (e_y \\times e_z) = e_x \\cdot e_x = 1,\n\\] \\[\n\\det(e_x, e_y, e_z) =\n\\begin{vmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{vmatrix} = 1.\n\\]\nTherefore, \\(k = 1\\) and \\(g = \\det\\). We are done!"
  },
  {
    "objectID": "posts/symmetry/index.html",
    "href": "posts/symmetry/index.html",
    "title": "What is symmetry? 什么是对称性?",
    "section": "",
    "text": "You may have noticed these concepts:\n\n\n\n\n\n\n\n\nEven/odd complex-valued function\n\n\n\n\nDefinition 1 A function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) is called\n\nconjugate symmetric \\(:\\iff f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff -f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\nSpecial cases of Definition 1\n\n\n\n\n\n\n\n\n\nEven/odd real function\n\n\n\n\nDefinition 2 A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-x) = f(x), \\forall x \\in \\mathbb{R}\\),\nodd \\(:\\iff -f(-x) = f(x), \\forall x \\in \\mathbb{R}\\)\n\n\n\n\n\n\n\n\n\n\nEven/odd multivariate real function\n\n\n\n\nDefinition 3 A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nodd \\(:\\iff -f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposition Property\n\n\n\n\nTheorem 1 Any function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) can be decomposed1 into a symmetric part \\(Sf\\) and a anti-symmetric part \\(Af\\): \\[\nf = \\frac{Sf + Af}{2},\n\\] \\[\nSf := f(\\mathbf{v})+\\overline{f(-\\mathbf{v})},\n\\] \\[\nAf := f(\\mathbf{v})-\\overline{f(-\\mathbf{v})}.\n\\]\nIn fancier language, \\[\n\\mathbb{C}^{\\mathbb{R}^n} = S\\mathbb{C}^{\\mathbb{R}^n} \\oplus A\\mathbb{C}^{\\mathbb{R}^n}.\n\\]\n\n\n\n1 The reason why I do NOT define \\(Sf = (f(\\mathbf{v})+\\overline{f(-\\mathbf{v})})/2\\) will be clear later.\n\n\n\n\n\nNote\n\n\n\nAs a special case of Theorem 1, any function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is a sum of an even and an odd function: \\[\nf = \\frac{(f(x) + f(-x))+(f(x) - f(-x))}{2}.\n\\]\n\n\n\n\n\nThere are also multiplicative version of Definition 1 and Theorem 1:\n\n\n\n\n\n\nMultiplicative version of Definition 1\n\n\n\n\nDefinition 4 A function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) is called2\n\nMultiplicative conjugate symmetric \\(:\\iff f(\\frac{1}{\\mathbf{v}}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff \\frac{1}{f(\\frac{1}{\\mathbf{v}})} = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n,\\)\n\nwhere \\(\\frac{1}{\\mathbf{v}}\\) is another vector in \\((\\mathbb{R}^{\\times})^n\\) whose components are the reciprocal of those of \\(\\mathbf{v}\\).\n\n\n\n2 \\(\\mathbb{R}^{\\times} := \\mathbb{R} \\backslash \\{0\\}\\).\n\n\n\n\n\nMultiplicative version of Decomposition Property\n\n\n\n\nTheorem 2 Any function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) can be decomposed into a symmetric part \\(S^{\\bullet}f\\) and a anti-symmetric part \\(A^{\\bullet}f\\): \\[\nf = \\sqrt{S^{\\bullet}f \\cdot A^{\\bullet}f},\n\\] \\[\nS^{\\bullet}f := f(\\mathbf{v}) \\cdot \\overline{f(\\mathbf{v}^{-1})},\n\\] \\[\nA^{\\bullet}f := \\frac{f(\\mathbf{v})}{\\overline{f(\\mathbf{v}^{-1})}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSymmetric/Alternating Tensor\n\n\n\n\nDefinition 5 A symmetric rank-\\(k\\) tensor \\(f: V^k \\to \\mathbb{R}\\) is symmetric iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\nIt is alternating iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = (\\operatorname{sgn} \\sigma) f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\n\n\n\nThough generally we cannot decompose an arbitrary tensor into a symmetric and alternating part, we could build them by introducing two operators:\n\n\n\n\n\n\nSymmetric/Alternating Operator for Tensors\n\n\n\n\nDefinition 6 Given \\(\\forall f: V^k \\to \\mathbb{R}\\), the operator \\(S\\) and \\(A\\) defined below always give a symmetric and alternating tensor3: \\[\nSf := \\sum_{\\sigma \\in S_k} \\sigma f,\n\\] \\[\nAf := \\sum_{\\sigma \\in S_k} \\operatorname{sgn}(\\sigma) \\sigma f.\n\\]\n\n\n\n3 \\(\\sigma f\\) is defined by \\((\\sigma f)(v_1, v_2, \\ldots, v_k) := f(v_{\\sigma(1)}, v_{\\sigma(2)}, \\ldots, v_{\\sigma(k)}).\\)\n\n\n\n\n\n\n\n\nSelf-adjoint and Skew-adjoint Matrices\n\n\n\n\nDefinition 7 A linear operator \\(\\phi \\in \\operatorname{Hom}(V)\\) is called self-adjoint iff \\[\n\\phi^H = \\phi,\n\\] and skew-adjoint iff \\[\n\\phi^H = -\\phi.\n\\]"
  },
  {
    "objectID": "posts/symmetry/index.html#sec-sec1",
    "href": "posts/symmetry/index.html#sec-sec1",
    "title": "What is symmetry? 什么是对称性?",
    "section": "",
    "text": "You may have noticed these concepts:\n\n\n\n\n\n\n\n\nEven/odd complex-valued function\n\n\n\n\nDefinition 1 A function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) is called\n\nconjugate symmetric \\(:\\iff f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff -f(-\\mathbf{v}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\nSpecial cases of Definition 1\n\n\n\n\n\n\n\n\n\nEven/odd real function\n\n\n\n\nDefinition 2 A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-x) = f(x), \\forall x \\in \\mathbb{R}\\),\nodd \\(:\\iff -f(-x) = f(x), \\forall x \\in \\mathbb{R}\\)\n\n\n\n\n\n\n\n\n\n\nEven/odd multivariate real function\n\n\n\n\nDefinition 3 A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is called\n\neven \\(:\\iff f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nodd \\(:\\iff -f(-\\mathbf{v}) = f(\\mathbf{v}), \\forall \\mathbf{v} \\in \\mathbb{R}^n\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposition Property\n\n\n\n\nTheorem 1 Any function \\(f: \\mathbb{R}^n \\to \\mathbb{C}\\) can be decomposed1 into a symmetric part \\(Sf\\) and a anti-symmetric part \\(Af\\): \\[\nf = \\frac{Sf + Af}{2},\n\\] \\[\nSf := f(\\mathbf{v})+\\overline{f(-\\mathbf{v})},\n\\] \\[\nAf := f(\\mathbf{v})-\\overline{f(-\\mathbf{v})}.\n\\]\nIn fancier language, \\[\n\\mathbb{C}^{\\mathbb{R}^n} = S\\mathbb{C}^{\\mathbb{R}^n} \\oplus A\\mathbb{C}^{\\mathbb{R}^n}.\n\\]\n\n\n\n1 The reason why I do NOT define \\(Sf = (f(\\mathbf{v})+\\overline{f(-\\mathbf{v})})/2\\) will be clear later.\n\n\n\n\n\nNote\n\n\n\nAs a special case of Theorem 1, any function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) is a sum of an even and an odd function: \\[\nf = \\frac{(f(x) + f(-x))+(f(x) - f(-x))}{2}.\n\\]\n\n\n\n\n\nThere are also multiplicative version of Definition 1 and Theorem 1:\n\n\n\n\n\n\nMultiplicative version of Definition 1\n\n\n\n\nDefinition 4 A function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) is called2\n\nMultiplicative conjugate symmetric \\(:\\iff f(\\frac{1}{\\mathbf{v}}) = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n\\),\nconjugate anti-symmetric \\(:\\iff \\frac{1}{f(\\frac{1}{\\mathbf{v}})} = \\overline{f(\\mathbf{v})}, \\forall \\mathbf{v} \\in \\mathbb{R}^n,\\)\n\nwhere \\(\\frac{1}{\\mathbf{v}}\\) is another vector in \\((\\mathbb{R}^{\\times})^n\\) whose components are the reciprocal of those of \\(\\mathbf{v}\\).\n\n\n\n2 \\(\\mathbb{R}^{\\times} := \\mathbb{R} \\backslash \\{0\\}\\).\n\n\n\n\n\nMultiplicative version of Decomposition Property\n\n\n\n\nTheorem 2 Any function \\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\) can be decomposed into a symmetric part \\(S^{\\bullet}f\\) and a anti-symmetric part \\(A^{\\bullet}f\\): \\[\nf = \\sqrt{S^{\\bullet}f \\cdot A^{\\bullet}f},\n\\] \\[\nS^{\\bullet}f := f(\\mathbf{v}) \\cdot \\overline{f(\\mathbf{v}^{-1})},\n\\] \\[\nA^{\\bullet}f := \\frac{f(\\mathbf{v})}{\\overline{f(\\mathbf{v}^{-1})}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSymmetric/Alternating Tensor\n\n\n\n\nDefinition 5 A symmetric rank-\\(k\\) tensor \\(f: V^k \\to \\mathbb{R}\\) is symmetric iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\nIt is alternating iff \\[\nf(v_{\\sigma(1)}, \\dots, v_{\\sigma(k)}) = (\\operatorname{sgn} \\sigma) f(v_1, \\dots, v_k)\n\\] for all permutations \\(\\sigma \\in S_k\\).\n\n\n\nThough generally we cannot decompose an arbitrary tensor into a symmetric and alternating part, we could build them by introducing two operators:\n\n\n\n\n\n\nSymmetric/Alternating Operator for Tensors\n\n\n\n\nDefinition 6 Given \\(\\forall f: V^k \\to \\mathbb{R}\\), the operator \\(S\\) and \\(A\\) defined below always give a symmetric and alternating tensor3: \\[\nSf := \\sum_{\\sigma \\in S_k} \\sigma f,\n\\] \\[\nAf := \\sum_{\\sigma \\in S_k} \\operatorname{sgn}(\\sigma) \\sigma f.\n\\]\n\n\n\n3 \\(\\sigma f\\) is defined by \\((\\sigma f)(v_1, v_2, \\ldots, v_k) := f(v_{\\sigma(1)}, v_{\\sigma(2)}, \\ldots, v_{\\sigma(k)}).\\)\n\n\n\n\n\n\n\n\nSelf-adjoint and Skew-adjoint Matrices\n\n\n\n\nDefinition 7 A linear operator \\(\\phi \\in \\operatorname{Hom}(V)\\) is called self-adjoint iff \\[\n\\phi^H = \\phi,\n\\] and skew-adjoint iff \\[\n\\phi^H = -\\phi.\n\\]"
  },
  {
    "objectID": "posts/symmetry/index.html#symmetry-as-group-action",
    "href": "posts/symmetry/index.html#symmetry-as-group-action",
    "title": "What is symmetry? 什么是对称性?",
    "section": "2 Symmetry as Group Action",
    "text": "2 Symmetry as Group Action\n\n2.1 Problem\nIs there any way to unify these seemingly “symmetric” concepts? What kind of mathematical object can be symmetrize and and alternate? When does the object itself expressible by only its symmetrized and and alternated ones?\n\n\n2.2 Important Observation\nThe common thing of the above examples in Section 1 is that the domain of the objects (functions, tensors, matrices4) could be manipulated by some kind of actions:\n4 This is left as an exercise.\n\\(f: \\mathbb{R}^n \\to \\mathbb{C}\\): additive inversion,\n\\(f: (\\mathbb{R}^{\\times})^n \\to \\mathbb{C}\\): multiplicative inversion,\n\\(f: V^k \\to \\mathbb{R}\\): permutation.\n\nThe first two can be viewed as the 2-element group \\(S_2\\) acts on the domain of \\(f\\), where \\(S_2\\) is the group generated by the operation of “taking inverse”: \\[\nS_2 := \\langle\\cdot^{-1}\\rangle = \\{e, \\cdot^{-1}\\},\n\\] or equivalently, the permutation group on two letters: \\[\nS_2 = \\{e, (12)\\}.\n\\]\nTherefore, in the first two cases, we could define a \\(S_2\\)-action: \\[\n(\\sigma f)(\\mathbf{v}) := \\overline{f(\\mathbf{v}^{-1})},\n\\] where \\(\\mathbf{v}^{-1}\\) is either \\(-\\mathbf{v}\\) (additive inverse) or \\(1/\\mathbf{v}\\) (multiplicative inverse).\nTherefore, the definition of the operator \\(S\\) and \\(A\\) in Definition 6 also applies for the first two cases: \\[\nSf\n:= \\sum_{\\sigma \\in S_2} \\sigma f\n= f(\\mathbf{v}) + \\overline{f(-\\mathbf{v})} \\quad (\\text{or } f(\\mathbf{v})\\cdot \\overline{f(-\\mathbf{v})}),\n\\] \\[\nAf\n:= \\sum_{\\sigma \\in S_k} \\operatorname{sgn}(\\sigma) \\sigma f\n= f(\\mathbf{v}) - \\overline{f(-\\mathbf{v})} \\quad (\\text{or } \\frac{f(\\mathbf{v})}{\\overline{f(\\mathbf{v}^{-1})}}).\n\\]\n\n\n2.3 When Decomposable?\nIn the first two cases, \\(f\\) can be expressed purely by \\(Sf\\) and \\(Af\\): \\[\nf = \\frac{Sf + Af}{2} \\quad (\\text{or } \\sqrt{Sf \\cdot Af}),\n\\] which is just the average of them! (Arithmetic average and geometric average respectively)\nBut we don’t have this relationship for tensors, i.e., not every rank \\(k\\) tensor can be purely expressed using \\(Sf\\) and \\(Af\\) – apart from the case when \\(k = 2\\): \\[\nf(v_1, v_2) = \\frac{(f(v_1, v_2)+f(v_2, v_1))+(f(v_1, v_2)-f(v_2, v_1))}{2} = \\frac{Sf + Af}{2}.\n\\]\nWhat happened when \\(k \\ge 3\\)?\nLet \\(f: V^3 \\to \\mathbb{R}\\), we have \\[\nSf = f(v_1, v_2, v_3) + f(v_2, v_3, v_1) + f(v_3, v_1, v_2) + f(v_2, v_1, v_3) + f(v_1, v_3, v_2) + f(v_3, v_2, v_1),\n\\] \\[\nAf = f(v_1, v_2, v_3) + f(v_2, v_3, v_1) + f(v_3, v_1, v_2) - f(v_2, v_1, v_3) - f(v_1, v_3, v_2) - f(v_3, v_2, v_1).\n\\]\n\n\n\nVisualize group action\n\n\nThe result \\[\n\\frac{Sf + Af}{2} = f(v_1, v_2, v_3) + f(v_2, v_3, v_1) + f(v_3, v_1, v_2) = \\sum_{\\sigma \\in A_3} \\sigma f \\neq f,\n\\] where \\(A_3\\) is the alternating group (the group of even permutations) on three letters.\n\n\n2.4 Try Yourself!\n\nExercise 1 (\\(S\\) and \\(A\\) operator for matrices \\(\\phi\\)) Let \\(\\phi \\in \\operatorname{End} (\\mathbb{C}^n)\\), derive the definition of \\(S\\phi\\) and \\(A \\phi\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\nS \\phi := \\frac{\\phi + \\phi^H}{2},\n\\] \\[\nA \\phi := \\frac{\\phi - \\phi^H}{2}.\n\\]\nWe also have \\[\n\\phi = \\frac{S \\phi + A \\phi}{2}.\n\\]"
  },
  {
    "objectID": "posts/side-ios/index.html",
    "href": "posts/side-ios/index.html",
    "title": "在 ios 下载第三方应用",
    "section": "",
    "text": "CLAIM: 更新(2-22-2025), SideStore在iPhone上会闪退!! 以下操作无法保证稳定运行!"
  },
  {
    "objectID": "posts/side-ios/index.html#说明",
    "href": "posts/side-ios/index.html#说明",
    "title": "在 ios 下载第三方应用",
    "section": "1 说明",
    "text": "1 说明\n下面的操作已在欧版iphone 16 pro ios 18.1上测试成功，但我个人感觉这个操作跟欧版iPhone没有什么关系，其他版本的iPhone大概率应该也可以，但是不保证。\n本文参考链接：\n\nlist of tools\nsideloadly\nsidestore"
  },
  {
    "objectID": "posts/side-ios/index.html#what-is-side-downloading",
    "href": "posts/side-ios/index.html#what-is-side-downloading",
    "title": "在 ios 下载第三方应用",
    "section": "2 What is side-downloading",
    "text": "2 What is side-downloading\nios系统不像安卓系统，只能在iOS store里面下载app。为了打破这点，有两种方法：越狱(jail breaking)或使用side-downloading。本文介绍后者。\nSide-downloading 指通过不越狱的方法将 iOS 应用程序直接安装到 Apple 设备上，而不需要通过 App Store。有很多工具可以实现这点，具体参考list of tools。\n其中 Scarlet 和 Signulous 的方法我没试过，另外三种 AltStore 需要你手机是欧版的、app地区是欧洲、你人也要在欧洲，所以没戏。最后两种 Sideloadly 和 Sidestore，后者下载软件需要连电脑，并且一般 side-downloading 的软件是要“续约(renew)”的，一般每周都要，而且 Sideloadly 这种方法还要连电脑 renew，不方便。所以我们采用最后一种 SideStore（只需要第一次的时候连电脑，后面在手机上下载和 renew）。当然我们也会用 sideloadly 下几个看看，毕竟这种比较简单。"
  },
  {
    "objectID": "posts/side-ios/index.html#sideloadly",
    "href": "posts/side-ios/index.html#sideloadly",
    "title": "在 ios 下载第三方应用",
    "section": "3 Sideloadly",
    "text": "3 Sideloadly\n在 windows 或 mac 上下载安装 sideloadly，图标如下：\n\n（可能要在 settings 中授权这个应用）\n这个只是个将软件下载到 iPhone 中的工具，而软件在哪里获取呢？我们需要 .ipa 后缀的文件，可以在这里搜索你要的软件，如果没有就在google上搜，最好注意下安全性问题。\n然后打开 sideloadly, 左侧选择你的 .ipa 文件，iDevice选择你的 iPhone（要用线将手机连到mac），输入你的Apple id（要记住这个ID，以后都用这个）和密码，点击start。（start 左边那个按钮是用来 renew 的，将来软件过期的时候就用同样的操作来 renew）"
  },
  {
    "objectID": "posts/side-ios/index.html#sidestore",
    "href": "posts/side-ios/index.html#sidestore",
    "title": "在 ios 下载第三方应用",
    "section": "4 Sidestore",
    "text": "4 Sidestore\n直接按照这里的指示下载安装即可：sidestore。\n其中有一步要产生一个配对文件，记得要将mac连上手机后再运行。如果mac上一直运行不出来，在 terminal 当前文件夹中输入：\nsudo ./jitterbugpair\n输入密码即可生成"
  },
  {
    "objectID": "posts/projective-space/index.html",
    "href": "posts/projective-space/index.html",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "",
    "text": "The projective space of a vector space is the set of all lines1 through the origin with some extra structures.\n1 Or “1-dimensional subspaces”.\n\n\n\nThis can be formally defined by identifying some elements in a vector space \\(V\\), or classifying the vectors in \\(V\\) according to the spaces they span. This is exactly the idea of “quotients” in math.\n\n\n\n\n\n\n\nDefinition of Projective Space\n\n\n\n\nDefinition 1 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\), define a equivalence relation \\(\\sim\\) on \\(V\\) as2: \\[\n\\forall x, y \\in V, x \\sim y :\\iff \\exists \\lambda \\in \\mathbb{F}^*, x = \\lambda y.\n\\] Then we can define projective space \\(\\mathbb{P}(V)\\) as: \\[\n\\mathbb{P}(V) := \\frac{V - \\{0_V\\}}{\\sim}.\n\\]\n\n\n\n\n\n2 \\(\\mathbb{F}^* := \\mathbb{F} - \\{0_\\mathbb{F}\\}\\), i.e., the non-zero elements of the field \\(\\mathbb{F}\\).\n\n\n\n\n\nProjective space as a special case of the Grassmannian manifold\n\n\n\n\n\nAll \\(1\\)-dimensional subspaces form the projective space. What about \\(n\\)-dimensional subspaces? They form the so-called Grassmannian manifold!\n\n\n\n\n\n\nFigure 1: Projective spaces are special case of Grassmannian manifolds\n\n\n\n\n\n\n\n\n\n\nThere is a (surjective) canonical projection \\[\n\\begin{aligned}\n\\pi: V - \\{0_V\\} &\\twoheadrightarrow \\mathbb{P}(V) \\\\\nx &\\mapsto [x]_{\\sim}\n\\end{aligned}\n\\] that maps each non-zero vector to its equivalence class (as shown in Figure 2).\n\n\n\n\n\n\nFigure 2: The mental picture of \\(\\pi\\) for \\(V = \\mathbb{R}^3\\)\n\n\n\nIn the following post, we will only consider the case where \\(\\mathbb{F} \\in \\{\\mathbb{R}, \\mathbb{C}\\}\\) and \\(V = \\mathbb{F}^n\\) since they are the most common cases that we will encounter. It’s necessary to have a clear mental picture of each of the example in the following sections."
  },
  {
    "objectID": "posts/projective-space/index.html#projective-space",
    "href": "posts/projective-space/index.html#projective-space",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "",
    "text": "The projective space of a vector space is the set of all lines1 through the origin with some extra structures.\n1 Or “1-dimensional subspaces”.\n\n\n\nThis can be formally defined by identifying some elements in a vector space \\(V\\), or classifying the vectors in \\(V\\) according to the spaces they span. This is exactly the idea of “quotients” in math.\n\n\n\n\n\n\n\nDefinition of Projective Space\n\n\n\n\nDefinition 1 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\), define a equivalence relation \\(\\sim\\) on \\(V\\) as2: \\[\n\\forall x, y \\in V, x \\sim y :\\iff \\exists \\lambda \\in \\mathbb{F}^*, x = \\lambda y.\n\\] Then we can define projective space \\(\\mathbb{P}(V)\\) as: \\[\n\\mathbb{P}(V) := \\frac{V - \\{0_V\\}}{\\sim}.\n\\]\n\n\n\n\n\n2 \\(\\mathbb{F}^* := \\mathbb{F} - \\{0_\\mathbb{F}\\}\\), i.e., the non-zero elements of the field \\(\\mathbb{F}\\).\n\n\n\n\n\nProjective space as a special case of the Grassmannian manifold\n\n\n\n\n\nAll \\(1\\)-dimensional subspaces form the projective space. What about \\(n\\)-dimensional subspaces? They form the so-called Grassmannian manifold!\n\n\n\n\n\n\nFigure 1: Projective spaces are special case of Grassmannian manifolds\n\n\n\n\n\n\n\n\n\n\nThere is a (surjective) canonical projection \\[\n\\begin{aligned}\n\\pi: V - \\{0_V\\} &\\twoheadrightarrow \\mathbb{P}(V) \\\\\nx &\\mapsto [x]_{\\sim}\n\\end{aligned}\n\\] that maps each non-zero vector to its equivalence class (as shown in Figure 2).\n\n\n\n\n\n\nFigure 2: The mental picture of \\(\\pi\\) for \\(V = \\mathbb{R}^3\\)\n\n\n\nIn the following post, we will only consider the case where \\(\\mathbb{F} \\in \\{\\mathbb{R}, \\mathbb{C}\\}\\) and \\(V = \\mathbb{F}^n\\) since they are the most common cases that we will encounter. It’s necessary to have a clear mental picture of each of the example in the following sections."
  },
  {
    "objectID": "posts/projective-space/index.html#real-projective-space-mathbbrpn",
    "href": "posts/projective-space/index.html#real-projective-space-mathbbrpn",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "2 Real Projective Space \\(\\mathbb{RP}^n\\)",
    "text": "2 Real Projective Space \\(\\mathbb{RP}^n\\)\nWhen \\(V = \\mathbb{R}^n\\), we will denote \\(\\mathbb{P}(\\mathbb{R}^n)\\) by \\(\\mathbb{RP}^{n-1}\\), where the upper index indicates the dimension of the projective space (actually a manifold of dimension \\(n-1\\)).\nThe “shape” of \\(\\mathbb{RP}^n\\) can be understood by Equation 1.\n\\[\n\\boxed{\n\\mathbb{RP}^n \\simeq \\frac{\\mathbb{S}^n}{\\mathbb{Z}_2} \\simeq \\frac{H^n}{\\sim} \\simeq \\frac{D^n}{\\sim}.\n}\n\\tag{1}\\]\n\n\n\n\n\n\n\nNote for Equation 1\n\n\n\n\n\n\nThis is a diffeomorphism in the category of smooth manifolds.\nThe notations in Equation 1 are defined below:\n\n\\(\\mathbb{S}^n\\) is the \\(n\\)-dimensional sphere: \\[\n\\mathbb{S}^n := \\{x \\in \\mathbb{R}^{n+1} : \\|x\\| = 1\\}.\n\\]\n\\(H^n\\) is the upper hemisphere: \\[\nH^n := \\{x \\in \\mathbb{R}^{n+1} : \\|x\\| = 1, x_j \\geq 0\\},\n\\] where \\(j\\) can be any coordinate index of \\(x\\).\n\\(D^n\\) is the closed \\(n\\)-disk: \\[\nD^n := \\{x \\in \\mathbb{R}^{n} : \\|x\\| \\leq 1\\}.\n\\]\n\n\\(\\mathbb{S}^n/\\mathbb{Z}_2\\) means:\n\\[\n\\mathbb{S}^n/\\text{\\{antipodal points\\}}\n\\] Let the group \\(\\mathbb{Z}/2\\mathbb{Z} = \\{\\bar{0}, \\bar{1}\\}\\) acts on \\(\\mathbb{S}^n\\) by taking the antipodal point: \\[\n\\bar{1} \\cdot x = -x, \\quad \\forall x \\in \\mathbb{S}^n.\n\\] This action defines the equivalence relation (See wiki here): \\[\n\\forall x \\in \\mathbb{S}^n, x \\sim y :\\iff \\exists \\lambda \\in \\mathbb{Z}_2, y = \\lambda x.\n\\]\nThe equivalence relation \\(\\sim\\) on \\(H^n\\) is defined as: \\[\n\\forall x, y \\in H^n, x \\sim y :\\iff x = y \\text{ or }\n\\begin{cases} x, y \\in \\partial H^n = \\mathbb{S}^{n-1}, \\\\ x=-y.  \\end{cases}\n\\]\nThe equivalence relation \\(\\sim\\) on \\(D^n\\) is defined as: \\[\n\\forall x, y \\in D^n, x \\sim y :\\iff x = y \\text{ or }\n\\begin{cases} x, y \\in \\partial D^n = \\mathbb{S}^{n-1}, \\\\ x=-y.  \\end{cases}\n\\]\n\n\n\n\n\nThis is explained in detail below.\n\n2.1 Real Projective line3 \\(\\mathbb{RP}^1\\)\nLet \\(V = \\mathbb{R}^2\\), then4: \\[\n\\mathbb{RP}^1 \\simeq \\frac{\\mathbb{S}^1}{\\mathbb{Z}_2} \\simeq \\frac{H^1}{\\sim} \\simeq \\frac{D^1}{\\sim} \\simeq \\mathbb{S}^1 \\simeq \\mathbb{\\hat{R}}.\n\\tag{2}\\]\n4 \\(\\mathbb{\\hat{R}} := \\mathbb{R} \\cup \\{\\infty\\}\\) is called the projectively extended real line. Distinguish it from the extended real line \\(\\overline{\\mathbb{R}} := \\mathbb{R} \\cup \\{-\\infty, +\\infty\\}\\).5 The blue curly arrow in Figure 3 indicates that the two points are equivalent. One can be immediately transported to the other side by this arrow.Visually speaking5,\n\n\n\n\n\n\nFigure 3: Visual demonstration of Equation 2\n\n\n\nNote that in Equation 2, \\(D^1/{\\sim}\\) happens to be \\(\\mathbb{S}^1\\) again, which is very special and not general for \\(n &gt; 1\\).\nInstead of let a line intersects the unit circle at the origin (as shown in the first isomorphism in Equation 2), there are different graphical representations of \\(\\mathbb{RP}^1\\):\n\n\n\n\n\n\nFigure 4: Visual demonstration of \\(\\mathbb{RP}^1 \\simeq \\mathbb{\\hat{R}}\\)\n\n\n\nThe horizontal line in Figure 4 does not intersect the blue line. We thus define it belongs to the equivalent class \\(\\{\\infty\\}\\).\n\n3 The projective space for \\(V = \\mathbb{R}^1\\) is too trivial, since everything in \\(\\mathbb{R}\\) is in one equivalence class. Therefore, \\(\\mathbb{RP}^0 = \\{e\\}\\).\n2.2 Real Projective plane \\(\\mathbb{RP}^2\\)\nLet \\(V = \\mathbb{R}^3\\), then: \\[\n\\mathbb{RP}^2 \\simeq \\frac{\\mathbb{S}^2}{\\mathbb{Z}_2} \\simeq \\frac{H^2}{\\sim} \\simeq \\frac{D^2}{\\sim}.\n\\tag{3}\\]\nVisually speaking,\n\n\n\n\n\n\nFigure 5: Visual demonstration of Equation 3\n\n\n\nNote that we cannot glue the opposite points on \\(H^2\\) nor \\(D^2\\) in \\(\\mathbb{R}^3\\) without a self-intersection. So we ended up at \\(D^2/{\\sim}\\). This structure is often expressed using fundamental polygon. Figure 6 [1] is what you get if you force to glue the opposite points on the equator of \\(H^2\\) together.\n\n\n\n\n\n\nFigure 6: \\(H^2/{\\sim}\\) immersed as a cross-cap in \\(\\mathbb{R}^3\\)\n\n\n\nInstead of let a line intersects the unit sphere at the origin (as shown in the first isomorphism in Equation 3), there are a different graphical representation of \\(\\mathbb{RP}^2\\):\n\n\n\n\n\n\nFigure 7: Infinity in \\(\\mathbb{RP}^2\\) is not unique\n\n\n\nThe lines in the \\(xy\\) plane in Figure 7 do not intersect the blue plane. We thus define them to “intersect with the circle at infinity”. The right figure of Figure 7 is NOT true since there is only one infinity, so it cannot tell the difference of the lines in the \\(xy\\) plane. So notably, \\[\n\\begin{aligned}\n\\mathbb{RP}^2 &\\simeq \\mathbb{R}^2 \\cup \\{\\text{circle at infinity}\\} \\\\\n&\\not\\simeq \\mathbb{R}^2 \\cup \\{\\infty\\}.\n\\end{aligned}\n\\]\n\n\n2.3 Real Projective space \\(\\mathbb{RP}^3\\)\nLet \\(V = \\mathbb{R}^4\\), one could prove that Equation 1 is still symbolically true: \\[\n\\mathbb{RP}^3 \\simeq \\frac{\\mathbb{S}^3}{\\mathbb{Z}_2} \\simeq \\frac{H^3}{\\sim} \\simeq \\frac{D^3}{\\sim}.\n\\tag{4}\\]\nHowever this time, the only object in Equation 4 that can be visualized is \\(D^3/{\\sim}\\):\n\n\n\n\n\n\nFigure 8: Every point on this solid ball (with antipodal points glued) represents an element in \\(\\mathbb{RP}^3\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbb{RP}^3, \\mathbb{S}^3, SO(3), SU(2)\\) and the unit quaternions \\(U(\\mathbb{H})\\)\n\n\n\n\n\nTheir relations are: \\[\n\\boxed{\n\\text{Spin}(3) \\simeq SU(2) \\simeq \\mathbb{S}^3 \\simeq U(\\mathbb{H}) \\overset{2:1} \\twoheadrightarrow \\mathbb{RP}^3 \\simeq SO(3).\n}\n\\]\n\n\\(SU(2) \\simeq \\mathbb{S}^3\\): Rotation in \\(\\mathbb{C}^2\\) sits bijectively on the surface of the 3-sphere \\(S^3\\).\n\n\n\n\n\n\n\n\nProof 1\n\n\n\n\n\nAn element in \\(SU(2)\\) has the form: \\[\nU = \\begin{pmatrix} \\alpha & \\gamma \\\\ \\beta & \\delta \\\\\\end{pmatrix} \\in SU(2)\n\\] with constraints: \\[\nU^H = U^{-1}, \\det U = 1.\n\\] Therefore, \\[\n\\begin{pmatrix} \\bar{\\alpha} & \\bar{\\beta} \\\\ \\bar{\\gamma} & \\bar{\\delta} \\\\\\end{pmatrix}\n=\n\\frac{1}{\\det U} \\begin{pmatrix} \\delta & -\\gamma \\\\ -\\beta & \\alpha \\\\\\end{pmatrix}\n=\n\\begin{pmatrix} \\delta & -\\gamma \\\\ -\\beta & \\alpha \\\\\\end{pmatrix}.\n\\] Thus, \\(U\\) must look like: \\[\n\\begin{align*}\n& U = \\begin{pmatrix} \\alpha & -\\beta \\\\ \\beta & \\bar{\\alpha} \\\\\\end{pmatrix}, \\text{ with } \\det U = 1 \\\\\n\\implies\\quad & \\alpha \\bar{\\alpha} + \\beta \\bar{\\beta} = 1 \\\\\n\\implies\\quad & |\\alpha|^2 + |\\beta|^2 = 1 \\\\\n\\implies\\quad & |a + bi|^2 + |c + di|^2 = 1 \\\\\n\\implies\\quad & a^2 + b^2 + c^2 + d^2 = 1.\n\\end{align*}\n\\] So, \\[\n\\forall U \\in SU(2), U \\in \\mathbb{S}^3.\n\\]\n\n\n\n\n\n\\(\\mathbb{S}^3 \\simeq U(\\mathbb{H})\\): Unit quaternions live on the 3-sphere.\n\n\n\n\n\n\n\n\nProof 2\n\n\n\n\n\nThis is easy to see from the definition of unit quaternions: \\[\n\\begin{aligned}\nU(\\mathbb{H}) &:= \\{ q \\in \\mathbb{H} : |q| = 1 \\} \\\\\n&\\simeq \\{ (a, b, c, d) \\in \\mathbb{R}^4 : a^2 + b^2 + c^2 + d^2 = 1 \\} \\\\\n&=: \\mathbb{S}^3\n\\end{aligned}\n\\] where \\(q = a + b\\mathbf{i} + c \\mathbf{j} + d \\mathbf{k}\\).\n\n\n\n\n\n\\(\\mathbb{RP}^3 \\simeq SO(3)\\): If you look around in \\(\\mathbb{R}^4\\), what you see is rotations in \\(\\mathbb{R}^3\\)!\n\n\n\n\n\n\n\n\nProof 3 (Not rigorous)\n\n\n\n\n\nAn element \\(R \\in SO(3)\\) is specified by the rotation axis and the rotation angle. We could encode these two information by a single vector in a solid ball of radius \\(\\pi\\), where the length of \\(V\\) is the rotation angle and the direction of \\(V\\) is the rotation axis. However, we soon realize that antipodal points on the surface of \\(D^3\\) represent the same rotation since rotating by \\(\\pi\\) does not care about the rotation direction. So we end up with the quotient space \\(D^3/{\\sim}\\) (shown in Figure 9).\n\n\n\n\n\n\nFigure 9: How elements in \\(SO(3)\\) live in \\(D^3/{\\sim}\\)\n\n\n\nBut Figure 9 is also the space of \\(\\mathbb{RP}^3\\), so we get: \\[\n\\mathbb{RP}^3 \\simeq SO(3).\n\\]\n\n\n\n\n\n\\(\\mathbb{S}^3 \\overset{2:1}{\\twoheadrightarrow} SO(3)\\): A rotation in \\(\\mathbb{R}^3\\) is represented by two points on the 3-sphere.\n\n\n\n\n\n\n\n\nProof 4\n\n\n\n\n\nThis is easy since we know: \\[\n\\mathbb{S}^3 \\overset{2:1}{\\twoheadrightarrow} \\frac{\\mathbb{S}^3}{\\mathbb{Z}_2},\n\\] and Equation 4 gives: \\[\n\\mathbb{RP}^3 \\simeq \\frac{\\mathbb{S}^3}{\\mathbb{Z}_2}.\n\\] We’ve just shown: \\[\n\\mathbb{RP}^3 \\simeq SO(3).\n\\]\n\n\n\n\n\n\\(\\text{Spin}(3) \\simeq \\mathbb{S}^3\\): Spin group double covers the rotation group.\n\n\n\n\n\n\n\n\nProof 5\n\n\n\n\n\n\\(\\text{Spin}(3)\\) could be defined to be the universal cover of \\(SO(3)\\), which is \\(\\mathbb{S}^3\\)."
  },
  {
    "objectID": "posts/projective-space/index.html#complex-projective-space-mathbbcpn",
    "href": "posts/projective-space/index.html#complex-projective-space-mathbbcpn",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "3 Complex Projective Space \\(\\mathbb{CP}^n\\)",
    "text": "3 Complex Projective Space \\(\\mathbb{CP}^n\\)\nWe don’t have a formula like Equation 1 for \\(\\mathbb{CP}^n\\) by the way. We will only focus on one case where \\(V = \\mathbb{C}^2\\).\n\n3.1 Complex projective line6 \\(\\mathbb{CP}^1\\)\nLet \\(V = \\mathbb{C}^2\\). Although it’s impossible to have a mental picture of \\(\\mathbb{C}^2\\), we could have a legit imagination for \\(\\mathbb{CP}^1\\) using homogeneous coordinates: \\[\n\\mathbb{C}^2 - \\{0\\} \\ni \\begin{pmatrix} z_1 \\\\ z_2 \\\\\\end{pmatrix}\n\\xmapsto{\\pi} \\left[\\frac{z_1}{z_2} : 1\\right]\n\\equiv \\left[a+bi : 1\\right]\n\\simeq \\mathbb{C}.\n\\tag{5}\\]\nAccording to Equation 5, one would need one single complex number \\(a+bi\\) to specify an element in \\(\\mathbb{CP}^1\\). Does it implies that \\(\\mathbb{CP}^1\\) is just the complex line7? No! We missed the case where \\(z_2 = 0\\)! Similar to Section 2.1, we define the line \\(\\begin{pmatrix} z_1 \\\\ 0 \\\\\\end{pmatrix} \\in \\mathbb{C}^2 - \\{0\\}\\) belongs to the equivalence class denoted by the symbol \\(\\infty\\): \\[\n\\begin{pmatrix} z_1 \\\\ 0 \\\\\\end{pmatrix}\n:\\xmapsto{\\pi} \\left[1 : 0\\right]\n\\equiv \\infty\n\\]\n7 \\(\\mathbb{C}\\) has different dimension as a real or complex vector space: \\[\\operatorname{dim}_{\\mathbb{R}} \\mathbb{C} = 2, \\operatorname{dim}_{\\mathbb{C}} \\mathbb{C} = 1.\\] We consider \\(\\mathbb{C}\\) as a complex vector space here, so we call it the complex line instead of the complex plane.Therefore, we would need one complex number plus only one extra symbol \\(\\infty\\) (together called the projectively extended complex number \\(\\mathbb{\\hat{C}}\\)) to specify an element in \\(\\mathbb{CP}^1\\): \\[\n\\mathbb{CP}^1 \\simeq \\mathbb{C} \\cup \\{\\infty\\} \\equiv \\mathbb{\\hat{C}}.\n\\]\nWe could also put a 2-sphere across or above \\(\\mathbb{\\hat{C}}\\) (as shown in Figure 10) and construct a bijective correspondence (called the Stereographic projection) between the points on the sphere and the points on \\(\\mathbb{\\hat{C}}\\). This sphere is a compact, connected, 1-dimensional complex manifold, called the Riemann sphere. We have the isomorphism: \\[\n\\mathbb{CP}^1 \\simeq \\mathbb{S}^2.\n\\]\n\n\n\n\n\n\nFigure 10: Two ways of stereographic projection both show that \\(\\mathbb{CP}^1 \\simeq \\mathbb{S}^2\\)\n\n\n\n\n6 The projective space for \\(V = \\mathbb{C}^1\\) is too trivial, since everything in \\(\\mathbb{C}\\) is in one equivalence class. Therefore, \\(\\mathbb{CP}^0 = \\{e\\}\\)."
  },
  {
    "objectID": "posts/projective-space/index.html#homography",
    "href": "posts/projective-space/index.html#homography",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "4 Homography",
    "text": "4 Homography\nWhenever mathematicians define a new object, they will soon talk about the morphisms between them, i.e., the mappings that preserve the structures of the objects. We are interested specifically in bijective morphisms between projective spaces, which are called homography. Just like the general linear group \\(\\text{GL}(V)\\), all homographies form a group called the projective linear group \\(\\text{PGL}(V)\\).\n\nHomography is an isomorphism between projective spaces induced by bijective linear transformations of vector spaces. \\[\\text{PGL}(V) \\equiv \\operatorname{Aut}(\\mathbb{P}(V)).\\]\n\nThe canonical projection \\(\\pi\\) in Section 1.3 not only project vector spaces but also the linear transformations between vector spaces. So it could be viewed as a functor from the category of vector spaces (v.s.) to the category of projective spaces (p.s.):\n\n\n\n\n\n\nFigure 11: Canonical projection \\(\\pi\\) as a functor\n\n\n\nWe first give two examples of homography in the case of \\(V \\in \\{\\mathbb{R}^2, \\mathbb{C}^2\\}\\) before dealing with the case of abstract \\(V\\).\n\n4.1 Homography on \\(\\mathbb{RP}^1\\)\nLet \\(V = \\mathbb{R}^2, A \\in \\text{GL}(V)\\), what is \\(A\\) under the functor \\(\\pi\\)?\n\n\n\n\n\n\nFigure 12: Homography on \\(\\mathbb{RP}^1\\)\n\n\n\n\\(A\\) could be represented by a \\(2 \\times 2\\) non-singular matrix multiplied by a non-zero vector \\(v\\): \\[\n\\begin{aligned}\nA: \\mathbb{R}^2 &\\xrightarrow[]{\\sim} \\mathbb{R}^2 \\\\\n\\begin{pmatrix} x_0 \\\\ y_0 \\\\\\end{pmatrix}\n&\\mapsto\n\\begin{pmatrix} a & c \\\\ b & d \\\\\\end{pmatrix}\n\\begin{pmatrix} x_0 \\\\ y_0 \\\\\\end{pmatrix}\\\\\n\\begin{pmatrix} x \\\\ 1 \\\\\\end{pmatrix}\n&\\mapsto\n\\begin{pmatrix} a & c \\\\ b & d \\\\\\end{pmatrix}\n\\begin{pmatrix} x \\\\ 1 \\\\\\end{pmatrix}\n=\n\\begin{pmatrix} ax + c \\\\ bx + d \\\\\\end{pmatrix}\n\\end{aligned}\n\\] where \\(x \\in \\mathbb{\\hat{R}}\\), \\(x_0, y_0, a, b, c, d \\in \\mathbb{R}\\) and \\(ad-bc \\neq 0\\). By definition, \\(\\pi(A)\\) should map the equivalence class \\([x : 1]\\) to \\([ax + c : bx + d]\\): \\[\n\\begin{aligned}\n\\pi(A): \\mathbb{RP}^1 &\\xrightarrow[]{\\sim} \\mathbb{RP}^1 \\\\\n[x : 1] &\\mapsto \\left[\\frac{ax+c}{bx+d} : 1\\right] \\\\\n\\end{aligned}\n\\tag{6}\\]\nNote that \\(\\pi(A)\\) maps \\(\\infty\\) to \\(\\frac{a}{b}\\).\n\n\n4.2 Möbius transformations – Homography on \\(\\mathbb{CP}^1\\)\n\nHomographies on \\(\\mathbb{CP}^1\\) are called Möbius transformations. The object \\(\\text{PGL}(\\mathbb{C}^2)\\) is called the Möbius group.\n\nLet \\(V = \\mathbb{C}^2, A \\in \\text{GL}(V)\\). We could simply write Equation 6 as its complex version: \\[\n\\begin{aligned}\n\\pi(A): \\mathbb{CP}^1 &\\xrightarrow[]{\\sim} \\mathbb{CP}^1 \\\\\n[z : 1] &\\mapsto \\left[\\frac{az+c}{bz+d} : 1\\right] \\\\\n\\end{aligned}\n\\] where \\(z \\in \\mathbb{\\hat{C}}\\), \\(a, b, c, d \\in \\mathbb{C}\\) and \\(ad-bc \\neq 0\\). Also, \\(\\pi(A)\\) maps \\(\\infty\\) to \\(\\frac{a}{b}\\).\nThere is an astounding fact [2] that the elements in the Möbius group correspond bijectively to the rigid motions of the Riemann sphere.\n\n\n\n\n\n\nFigure 13: Möbius transformations are rigid motions of the Riemann sphere [3]\n\n\n\nHowever, this does NOT mean that \\(\\text{PGL}(\\mathbb{C}^2) \\simeq SO(3)\\) as groups. In fact, one can show that the Möbius group is isomorphic to the so-called Lorenz group: \\[\n\\text{PGL}(\\mathbb{C}^2) \\simeq SO^+(1, 3).\n\\]"
  },
  {
    "objectID": "posts/projective-space/index.html#projective-linear-group",
    "href": "posts/projective-space/index.html#projective-linear-group",
    "title": "Basics of Projective Space and Projective Linear Group 射影空间与射影线性群",
    "section": "5 Projective Linear group",
    "text": "5 Projective Linear group\nThe projective space is a classification of vectors in a vector space. Similarly, we will define the projective linear group \\(\\text{PGL}(V)\\) as a classification of isomorphisms in the general linear group \\(\\text{GL}(V)\\) in this section.\n\n5.1 Definition\nIt’s obvious that some linear transformations are equivalent when we view them projectively. For example in Figure 14, a matrix \\(A \\in \\text{GL}(\\mathbb{R}^2)\\) looks the same as its scalar multiples \\(kA\\) for \\(k \\in \\mathbb{R}^*\\) under the functor \\(\\pi\\):\n\n\n\n\n\n\nFigure 14: A matrix is equivalent to its scalar multiples projectively\n\n\n\nIt turns out its converse is also true: All equivalent matrices are scalar multiples of each other! We have:\n\n\n\n\n\n\n\nEquivalent matrices \\(\\iff\\) Scalar multiples\n\n\n\n\nTheorem 1 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\) and \\(A, B \\in \\text{GL}(V)\\), then: \\[\n\\pi(A) = \\pi(B) \\iff \\exists \\lambda \\in \\mathbb{F}^*, A = \\lambda B.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof of Theorem 1\n\n\n\n\n\n\\((\\impliedby)\\) is trivial. We only show the direction \\((\\implies)\\) here:\n\\(\\pi(A) = \\pi(B)\\) implies: \\[\n\\forall v \\in V - \\{0\\}, \\pi(Av) = \\pi(Bv)\n\\]\nBy the definition of projective space, \\(Av\\) and \\(Bv\\) must be scalar multiple \\(\\lambda\\) of each other and that scalar may depend on \\(v\\) (We will show that \\(\\lambda\\) is actually independent of \\(v\\) later): \\[\n\\exists \\lambda(v) \\in \\mathbb{F}^*, Av = \\lambda(v)Bv.\n\\tag{7}\\]\nPick a basis \\(\\{e_1, e_2, \\cdots, e_n\\}\\) of \\(V\\) and take \\(v\\) to be each of the basis: \\[\nAe_i = \\lambda(e_i)Be_i, \\forall i = 1, 2, \\cdots, n.\n\\tag{8}\\]\n\\(\\forall v \\in V\\) is a linear combination of the basis: \\(v = \\sum \\alpha_i e_i\\). Multiply \\(A\\) both sides and from Equation 8, we have: \\[\n\\begin{aligned}\nAv &= \\sum \\alpha_i Ae_i = \\sum \\alpha_i \\underbrace{\\lambda(e_i) Be_i}_{Ae_i} \\\\\n&= \\sum \\lambda(e_i) \\alpha_i Be_i\n\\end{aligned}\n\\tag{9}\\]\nBut Equation 7 tells us: \\[\nAv = \\lambda(v)Bv = \\lambda(v) \\sum \\alpha_i Be_i.\n\\tag{10}\\]\nCompare Equation 9 and Equation 10, all \\(\\lambda\\) s must be the same: \\[\n\\lambda(v) = \\lambda(e_1) = \\lambda(e_2) = \\cdots = \\lambda(e_n) \\equiv \\lambda = \\text{const}.\n\\]\nTherefore, Equation 7 implies: \\[\n\\forall v \\in V - \\{0\\}, Av = \\lambda Bv \\implies A = \\lambda B.\n\\]\n\n\n\n\nWe can define the projective linear group \\(\\text{PGL}(V)\\) according to the classification of matrices “up to scalar”:\n\n\n\n\n\n\n\nDefinition of \\(\\text{PGL}(V)\\)\n\n\n\n\nDefinition 2 Let \\(V\\) be a vector space over field \\(\\mathbb{F}\\), then the projective linear group \\(\\text{PGL}(V)\\) is defined as: \\[\n\\text{PGL}(V) := \\frac{\\text{GL}(V)}{\\mathbb{F}^* I},\n\\] where \\(I\\) is the identity in \\(\\text{GL}(V)\\).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{PGL}(V)\\) as the inner automorphism group of \\(\\text{GL}(V)\\)\n\n\n\n\n\nWe know from here that the center of \\(\\text{GL}(V)\\) is exactly \\(\\mathbb{F}^* I\\): \\[\n\\text{ZGL}(V) \\equiv Z(\\text{GL}(V)) = \\mathbb{F}^* I.\n\\]\nWe know from this post that for any group \\(G\\), \\[\n1 \\xrightarrow[]{} Z(G) \\hookrightarrow G \\twoheadrightarrow \\operatorname{Inn}(G) \\xrightarrow[]{} 1,\n\\] where \\(\\text{Inn}(G)\\) = \\(G/{Z(G)}\\). Let \\(G = \\text{GL}(V)\\), then we have: \\[\n1 \\xrightarrow[]{} \\text{ZGL}(V) \\hookrightarrow \\text{GL}(V) \\twoheadrightarrow \\operatorname{PGL}(V) \\xrightarrow[]{} 1,\n\\] where \\[\n\\text{Inn}(\\text{GL}(V)) = \\frac{\\text{GL}(V)}{\\text{ZGL}(V)} = \\text{PGL}(V)\n\\]\n\n\n\n\n\n\n5.2 Grid of short exact sequence about projective linear groups\nThe general linear group \\(\\text{GL}(V)\\), special linear group \\(\\text{SL}(V)\\), their centers \\(\\text{ZGL}(V)\\) and \\(\\text{ZSL}(V)\\), and their projective linear groups \\(\\text{PGL}(V)\\) and \\(\\text{PSL}(V)\\) form the following grid of short exact sequence:\n\n\n\n\n\n\n\n\nFigure 15\n\n\n\nEach row and column in Figure 15 is a short exact sequence8. Everything would be clear if you stare at the cartoon in Figure 16 for a moment.\n8 \\((\\mathbb{C}^{\\times})^n\\) here means \\(\\{z^n: z \\in \\mathbb{C}^{\\times }\\}\\), not \\(\\mathbb{C}^{\\times} \\times \\cdots \\times \\mathbb{C}^{\\times}\\). \\(\\mathbb{C}^{\\times}\\) is the same as \\(\\mathbb{C}^*\\), which represents the multiplicative group of non-zero complex numbers.\n\n\n\n\n\nFigure 16: Illustration of the commutative diagram in Figure 15"
  },
  {
    "objectID": "posts/pde-derivation/index.html",
    "href": "posts/pde-derivation/index.html",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "",
    "text": "Conservation law seems underratedly important.\nOscillation and overshooting comes from second order time derivative. Smooth exponential decay comes from first order time derivative.\n\\(a_{n+1} + a_{n-1} - 2a_n\\) is the second order spatial derivative, the mental picture of it is the difference between current value and the average of its two neighbors.\nForcing terms are usually comes with second order spacial derivative. This is because first order spacial derivative only represent force from one side, and second order spacial derivative is the net force of the neighboring particles acting on the particle we analyze."
  },
  {
    "objectID": "posts/pde-derivation/index.html#takeaways",
    "href": "posts/pde-derivation/index.html#takeaways",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "",
    "text": "Conservation law seems underratedly important.\nOscillation and overshooting comes from second order time derivative. Smooth exponential decay comes from first order time derivative.\n\\(a_{n+1} + a_{n-1} - 2a_n\\) is the second order spatial derivative, the mental picture of it is the difference between current value and the average of its two neighbors.\nForcing terms are usually comes with second order spacial derivative. This is because first order spacial derivative only represent force from one side, and second order spacial derivative is the net force of the neighboring particles acting on the particle we analyze."
  },
  {
    "objectID": "posts/pde-derivation/index.html#welcome",
    "href": "posts/pde-derivation/index.html#welcome",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "1 Welcome!",
    "text": "1 Welcome!\nThis is the first chapter of my PDE series. You will learn how to derive two of the most important PDEs in physics: Wave equation and heat equation, with nearly no prerequisites. The goal of this article is to make these two equations as obvious as possible1 to you:\n1 We avoid using the notation \\(\\Delta\\) for \\(\\nabla^2\\). \\(\\Delta\\) in this article just mean “difference in” blablabla. We also use \\(\\xi_{t}\\) and \\(\\xi_{tt}\\) to represent \\(\\frac{\\partial \\xi}{\\partial t}\\) and \\(\\frac{\\partial^2 \\xi}{\\partial t^2}\\).\nWave equation: \\(\\xi_{tt} = c^2 \\nabla^2 \\xi\\).\nHeat equation: \\(T_t = \\alpha \\nabla^2 T\\).\n\nLet’s do it!"
  },
  {
    "objectID": "posts/pde-derivation/index.html#wave-equation",
    "href": "posts/pde-derivation/index.html#wave-equation",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "2 Wave Equation",
    "text": "2 Wave Equation\n\n2.1 1D Wave Equation\nThere are two kinds of one-dimensional wave: longitudinal and transverse.\n\n\n\nLongitudinal wave and transverse wave cartoon\n\n\nThey look totally different, but they share exactly the same properties and could be described by the same wave PDE! Let’s have a look at longitudinal wave first.\n\n2.1.1 Longitudinal Wave\nThis kind of wave can be considered as little balls connected by springs:\n\n\n\n\n\n\nFigure 1: Balls at equilibrium position\n\n\n\nAs shown in Figure 1, each small ball is in a tranquil state called “equilibrium position” where all resultant force acting on it is zero. Now we want to describe the system when there are perturbations using PDEs. So there should be a variable that depends on time and space. What is that variable? Well, we should define it. Let’s make it the displacement value \\(\\xi(x,t)\\) of each ball from its equilibrium position.\n\n\n\n\n\n\nFigure 2: Zoom in at one of the balls\n\n\n\nConsider the scenario shown in Figure 2, we zoom in at the \\(n\\)-th ball. In order to find the equation of motion for this ball, we need to define some parameters:\n\n\\(k\\): Microscopic string constant.\n\\(m\\): Mass of each ball.\n\nThe net force acting on the \\(n\\)-th ball is the sum of the forces from the two adjacent balls: \\[\n\\begin{aligned}\nF_n &= F_L + F_R \\\\\n&= (-F_0 - k(\\xi_n - \\xi_{n-1})) + (F_0 + k(\\xi_{n+1} - \\xi_n)) \\\\\n&= k((\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})),\n\\end{aligned}\n\\] where \\(F_0\\) is the force from both left and right balls acting on the \\(n\\)-th ball at equilibrium position (they balanced out at equilibrium position). According to Newton’s second law2 \\(F_n = m \\ddot \\xi_n\\), we have: \\[\nk((\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})) = m \\ddot \\xi_n.\n\\tag{1}\\]\n2 \\(\\ddot \\xi_n\\) means \\(\\frac{\\partial^2 \\xi}{\\partial t^2}\\) in the article.Now Equation 1 should be it! But in reality, we do not know \\(m\\) and \\(k\\) since they are microscopic parameters. We now define some new parameters to connect the quantities that we can measure with the microscopic parameters:\n\n\\(N\\): Total number of balls.\n\\(\\Delta l\\): Microscopic distance between adjacent balls.\n\\(L\\): Total length, \\(L = N \\Delta l\\).\n\\(M\\): Total mass of the system, \\(M = Nm\\).\n\\(\\rho\\): Mass density, \\(\\rho = \\frac{M}{L}\\).\n\\(K\\): Total spring constant3, \\(K = \\frac{k}{N}\\).\n\n3 The spring constant of serial spring is smaller than that of individual spring by a factor of \\(N\\).Now Equation 1 becomes: \\[\n\\begin{align*}\n&k \\frac{(\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})}{(\\Delta l)^2} = \\frac{\\rho \\ddot \\xi_n}{\\Delta l} \\\\\n\\implies\\quad &k \\xi_{xx} = \\frac{\\rho \\ddot \\xi_n}{\\Delta l} \\\\\n\\implies\\quad &\\ddot \\xi_n = \\frac{k \\Delta l}{\\rho} \\xi_{xx} = \\frac{NK \\frac{L}{N}}{\\frac{M}{L}} \\xi_{xx} \\\\\n\\implies\\quad &\\ddot \\xi_n = \\frac{KL^2}{M} \\xi_{xx}\n\\end{align*}\n\\]\n\\(n\\) is arbitrary, so we got: \\[\n\\boxed{\n\\ddot \\xi = \\frac{KL^2}{M} \\xi_{xx}.\n}\n\\]\n\n\n2.1.2 Transverse wave\nTo derive the wave equation for transverse wave, we need to make some basic assumptions:\n\nEvery particle could only move in a fixed line perpendicular to the direction of wave propagation, which means the horizontal resultant force is zero for each particle.\nThe vibration amplitude is extremely small.\nFollowing above, the tension (density) is constant throughout the string.\n\n\n\n\n\n\nOnly verticle movement of each particle\n\n\n\n\n\nVibration amplitude of each particle is small\n\n\n\n\n\nTension is asssumed to be invariant\n\n\nNow we define some parameters of the string:\n\n\\(T\\): Tension of the string.\n\\(\\lambda\\): Line density of the string (mass per unit length).\n\nWe focus on an element of the string shown in Figure 3.\n\n\n\n\n\n\nFigure 3: Line segment of the string\n\n\n\nSuppose the mass of that element is \\(\\Delta m\\), the displacement is \\(\\xi\\). The resultant force \\[\n\\begin{align*}\n& \\Sigma F = \\Delta m \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad & T \\sin \\theta_2 - T \\sin \\theta_1 = \\lambda \\frac{\\Delta x}{\\cos \\theta_1} \\cdot \\ddot{\\xi}\n\\end{align*}\n\\]\nSince \\(\\theta_1, \\theta_2 \\to 0, \\cos \\theta_1 \\approx 1, \\sin \\theta \\approx \\tan \\theta\\), we have: \\[\n\\begin{align*}\n&T(\\tan \\theta_2 - \\tan \\theta_1) = \\lambda \\Delta x \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &T \\left( \\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x+\\Delta x} - \\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x}  \\right) = \\lambda \\Delta x \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &T \\frac{\\partial^2 \\xi}{\\partial x^2} \\Delta x = \\lambda \\Delta x \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &\\frac{T}{\\lambda} \\xi_{xx} = \\ddot{\\xi}.\n\\end{align*}\n\\]\nDone! This is the wave equation for transverse wave. We can also see the famous result that the wave speed is the square root of tension divide by density: \\[\nv = \\sqrt{\\frac{T}{\\lambda}}.\n\\]\n\n\n\n\n\n\n\nSpatial Second order derivative\n\n\n\nIn the derivation of the two kinds of waves, the crucial part! is the realization that these two expressions: \\[\n(\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1})\n\\tag{2}\\] and \\[\n\\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x+\\Delta x} - \\left. \\frac{\\partial \\xi}{\\partial x} \\right|_{x}\n\\] are essentially the second order spatial derivative \\(\\xi_{xx}\\). This is obvious if you recall the definition of second derivative: just the derivative of derivative. Or in discrete version, the difference of difference. In particular, Equation 2 can be also interpreted as the comparison between current value and the average of its two neighbors4: \\[\n(\\xi_{n+1} - \\xi_n) - (\\xi_n - \\xi_{n-1}) \\propto \\frac{T_{n-1} + T_{n+1}}{2} - T_n.\n\\tag{3}\\]\nWe will encounter this in the derivation of heat equation as well, so keep this in mind!\n\n\n\n4 This incredible idea is inspired by this video by 3Blue1Brown.\n\n\n2.2 2D Wave Equation\nYou may think the 2D analog is something like Figure 4. However, this kind of wave is vector-valued just like electromagnetic wave, where the “amplitude” \\(\\mathbf{\\xi}: \\mathbb{R}^n \\times \\mathbb{R} \\to \\mathbb{R}^n\\) is a time-dependent vector field. We will focus on scalar-valued waves in this article, leaving vector-valued waves in the future discussion5. In fact, the transverse wave (shown in Figure 5) is quite accurate for 2D scalar-valued waves.\n5 For those of the curious reader, the wave equation \\(\\mathbf{\\xi}_{tt} = c^2 \\nabla^2 \\mathbf{\\xi}\\) is also true component-wise for vector-valued waves, i.e., \\((\\mathbf{\\xi}_i)_{tt} = c^2 \\nabla^2 \\mathbf{\\xi}_i\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Vector-valued 2D longitudinal wave\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Scalar-valued 2D transverse wave\n\n\n\n\n\n\nYou can imagine Figure 5 as a curved rubber band with similar assumptions in Section 2.1.2. Suppose:\n\n\\(T\\): Tension per unit length6.\n\\(\\sigma\\): Surface mass density of the rubber band (mass per unit area).\n\n6 Note that it doesn’t make sense to say “tension at a point” but “tension at both sides of a line of a given length” (unit: N/m).Along \\(z\\) axies, \\[\n\\begin{align*}\n&\\Sigma F = \\Delta m \\cdot \\ddot{\\xi} \\\\\n\\implies\\quad &(T \\Delta y) \\frac{\\partial^2 \\xi}{\\partial x^2} \\Delta x + (T \\Delta x) \\frac{\\partial^2 \\xi}{\\partial y^2} \\Delta y = \\sigma (\\Delta x \\Delta y) \\frac{\\partial^2 \\xi}{\\partial t^2} \\\\\n\\implies\\quad &\\frac{T}{\\sigma} (\\xi_{xx} + \\xi_{yy}) = \\ddot{\\xi}.\n\\end{align*}\n\\]\nThis can also be generalized to wave equation in \\(n\\) dimension: \\[\n\\boxed{\n\\ddot{\\xi} = \\frac{T}{\\sigma} \\nabla^2 \\xi. }\n\\]"
  },
  {
    "objectID": "posts/pde-derivation/index.html#heat-equation",
    "href": "posts/pde-derivation/index.html#heat-equation",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "3 Heat Equation",
    "text": "3 Heat Equation\n\n3.1 1D Heat Equation\n\n3.1.1 Modelling\nThe following physical model of heat conduction on a rod is based on the following assumptions and facts:\n\nNo thermal source on the thin rod.\n(Prop Relation 1) \\(T\\) (in Kelvin) is defined to be proportional7 to thermal energy \\(Q\\) (in \\(\\text{J}\\)), i.e., \\[\nT : \\propto Q.\n\\]\nThis is reflected in the following well-known fact: \\[\n\\Delta Q = cm \\Delta T,\n\\] where \\(c\\) is the specific heat capacity of the rod, \\(m\\) is the mass we analyze on.\n(Prop Relation 2) Conduction of heat energy is proportional to temperature difference8: \\[\n\\boxed{\nq = -k \\nabla T,}\n\\] where \\(q\\) is called heat flux density9, the amount of thermal energy passing through a point (1D case) or a unit area (3D case)\nThermal energy in conserved!\n\n\n7 Loosely speaking, this is the definition of temperature. If there is no temperature (\\(T = 0\\text{ K}\\)), then there is no particle motion, \\(Q = 0\\).8 This famous equation is called Fourier’s law for heat conduction.9 Since in 1D case, \\(q\\) is just the amount of thermal energy pssing through a point. Thus in 1D, we often drop the word “density” by just saying “heat flux”.\n\n\n\n\n\nImportance of proportion relations\n\n\n\nYou may have noticed there are two proportional relations: \\[\n    T : \\propto Q.\n\\] This allows us to use temperature to reflect energy! Temperature is more tangible than energy. \\[\n    q = -k \\nabla T.\n\\] This connects time derivative and space derivative. So Heat equation is actully easier to derive since this Fourier’s law directly gives us nearly the final result. I say nearly because the only thing left is the conservation of energy!\n\n\n\n\n\n3.1.2 Derivation\nThe notations used are defined below:\n\n\\(\\lambda\\): Line density of the rod (mass per unit length).\n\\(x, x + \\Delta x\\): The coordinates of the start and end points of the rod segment we analyze (Right is the positive direction).\n\\(T: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\): Temperature field (time dependent) on the rod.\n\nThe quantity we care is obviously how the temperature changes over time and space. We chooce a tiny segment of the rod of length \\(\\Delta x\\) and build equation on it. The temperature on that extremely small segment can be viewed as a constant.\n\n\n\n\n\n\nFigure 6: Energy conservation on a small element of the rod\n\n\n\nAs shown in Figure 6, suppose the left side of the rod is hotter than its right side10. We know that the thermal energy increased in \\(\\mathrm{d}t\\) should equal to thermal energy entering the segment on the left minus thermal energy leaving the segment on the right: \\[\n\\frac{\\mathrm{d} Q}{\\mathrm{d} t} = q_{\\text{in, left}} - q_{\\text{out, right}}.\n\\tag{4}\\]\n10 This is an assumption without loss of generality in terms of writing out the heat equation. The signs can be a lot tricker to determine if you don’t have a certain physical picture.Now we expect \\(q_{\\text{in, left}}\\) and \\(q_{\\text{out, right}}\\) to be both positive. The gradient of temperature \\(\\frac{\\partial T}{\\partial x}\\) on position \\(x\\) and \\(x + \\Delta x\\) are both negative (since temperature decreases along the positive direction), therefore, \\[\n\\begin{aligned}\nq_{\\text{in, left}} &= \\left. -k \\frac{\\partial T}{\\partial x} \\right|_{x} &gt; 0 \\\\\nq_{\\text{out, right}} &= \\left. -k \\frac{\\partial T}{\\partial x} \\right|_{x + \\Delta x} &gt; 0.\n\\end{aligned}\n\\]\nTherefore, Equation 4 becomes: \\[\n\\begin{align*}\n& \\frac{\\mathrm{d} Q}{\\mathrm{d} t} = k (T_x|_{x+\\Delta x} - T_x|_{x}) \\\\\n\\implies\\quad & c (\\lambda \\Delta x) \\frac{\\partial T}{\\partial t} = k T_{xx} \\Delta x \\\\\n\\implies\\quad & T_t = \\frac{k}{c \\lambda} T_{xx}.\n\\end{align*}\n\\]\nJust this simple, we derived the heat equation! \\[\n\\boxed{\nT_t = \\frac{k}{c \\lambda} T_{xx}.}\n\\]\n\n\n\n\n\n\n\nWhy do wave equation and heat equation so similar and different?\n\n\n\n\nWhy do they both have \\(\\xi_{xx}\\) term?\nThe “forcing” on a small element is proportional to the change of changes. This is very common in nature.\n\nIn wave case, the force on each particle not depend on the change of displacement of adjacent particles, but the change of the change (because first order determines force on either side, second order determines the net force on the particle we analyze)!\nIn heat case, the heat increase in a small segment does not depend on first order derivative of temperature field, that’s just one side. There’s yet another side that may compensate for this change in temperature. So it’s the difference of difference that matters.\n\nWhy temperature only cares first order time derivative?\nThere’s a fundamental difference between wave equation and heat equation: Heat will never overshoot but wave will. In other words, a spring will overshoot its equilibrium with maximal speed, resulting a movement of back and forth. But heat will evolve as smooth and efficient as possible, never oscillating.\n\n\n\n\n\n\n\n\n\n\nHeat diffusion\n\n\n\n\n\n\n\nWave propagation11\n\n\n\n\n\n\nA second order derivative in time means overshooting.\n\nSecond order time derivative appears in wave equation is because Newton’s second law involves acceleration. Whereas in heat equation the first order time derivative actually comes from conservation laws. There is a wide class of PDEs (called continuity equation) that can be derived from conservation law. We will discuss this later.\nJust to give a intuitive understanding of the difference behavior of wave and heat, recall that in Equation 3 we introduced a new way of looking at second order spatial derivative (Difference to the average of its neighbors). As shown in Figure 7, the second order spatial derivative is proportional to the blue \\(d\\).\n\n\n\n\n\n\nFigure 7: Difference between \\(d \\propto a\\) and \\(d\\ propto v\\)\n\n\n\nThe only difference in wave and heat equation is that if \\(d\\) is proportional to the acceleration \\(a\\) (of the middle particle), or its velocity \\(v\\), i.e., \\[\n\\dot{\\xi} = \\xi\n\\tag{5}\\] or \\[\n\\ddot{\\xi} = \\xi.\n\\tag{6}\\]\nThe solution to Equation 5 is the famous exponential decay, whereas the solution to Equation 6 is obviously the ocsillation!\n\n\n\n11 Made with this website where you can explore different PDEs visually!\n\n\n3.2 3D Heat Equation\nNow substitude the rod with a 3D material like metal. Suppose\n\n\\(\\rho\\): Volume density of the rod (mass per unit volume).\n\\(\\Delta V\\): Volume of the small segment we analyze, \\(\\Delta V = \\Delta x \\Delta y \\Delta z\\).\n\\(T: \\mathbb{R}^3 \\times \\mathbb{R} \\to \\mathbb{R}\\): Temperature field (time dependent) on the rod.\n\n\n\n\nIsolate a small sube inside the metal\n\n\nFrom conservation of thermal energy, the increase of \\(Q\\) is unit time equal to net energy entering the volume from \\(x, y, z\\) directions. We already know that in one dimension, say \\(x\\), the net increase in thermal energy is proportional to the second spatial derivative times the nudge length: \\[\nq = k T_{xx} \\overbrace{\\Delta x}^{\\text{nudge}}.\n\\]\nHowever, \\(q\\) is three-dimension is the heat flux density, so we also need to multiply the cross section area: \\[\n\\frac{\\mathrm{d} Q}{\\mathrm{d} t} = q \\cdot \\Delta \\text{Area} = \\overbrace{k T_{xx} \\Delta x}^{\\text{density}} \\cdot \\overbrace{(\\Delta y \\Delta z)}^{\\text{area size}}.\n\\]\nSum up all heat contributions from three directions: \\[\n\\begin{align*}\n& \\Sigma \\frac{\\mathrm{d} Q}{\\mathrm{d} t} = q_x (\\Delta y \\Delta z) + q_y (\\Delta z \\Delta x) + q_z (\\Delta x \\Delta y) \\\\\n\\implies\\quad & c (\\rho \\Delta V) \\frac{\\partial T}{\\partial t} = (k T_{xx} \\Delta x)(\\Delta y \\Delta z) + (k T_{yy} \\Delta y)(\\Delta z \\Delta x) + (k T_{zz} \\Delta z)(\\Delta x \\Delta y) \\\\\n\\implies\\quad & T_t = \\frac{k}{c \\rho} (T_{xx} + T_{yy} + T_{zz}),\n\\end{align*}\n\\] i.e., \\[\n\\boxed{\nT_t = \\frac{k}{c \\rho} \\nabla^2 T.}\n\\]\nThis could be natually generalized to \\(n\\) dimensions."
  },
  {
    "objectID": "posts/pde-derivation/index.html#references",
    "href": "posts/pde-derivation/index.html#references",
    "title": "PDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程",
    "section": "References",
    "text": "References\n\n\nLamoureux, Michael. 2006. “The Mathematics of PDEs and the Wave Equation.” https://mathtube.org/sites/default/files/lecture-notes/Lamoureux_Michael.pdf."
  },
  {
    "objectID": "posts/jk-ff/index.html",
    "href": "posts/jk-ff/index.html",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "",
    "text": "Most online VHDL descriptions of JK flip-flops (FF) are based on “processes” or circuit functionality (behavioral). Is it possible to simulate them only by constructing the circuit structure of the JK flip-flop?"
  },
  {
    "objectID": "posts/jk-ff/index.html#intro",
    "href": "posts/jk-ff/index.html#intro",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "",
    "text": "Most online VHDL descriptions of JK flip-flops (FF) are based on “processes” or circuit functionality (behavioral). Is it possible to simulate them only by constructing the circuit structure of the JK flip-flop?"
  },
  {
    "objectID": "posts/jk-ff/index.html#jk-ff-review",
    "href": "posts/jk-ff/index.html#jk-ff-review",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "2 JK FF Review",
    "text": "2 JK FF Review\nThe circuit structure is JK FF is very familiar to everybody, which is: \nThe corresponding truth table is shown below:\n\n\n\nC\nJ\nK\nQ\nQ̅\n\n\n\n\n↑\n0\n0\nlatch\nlatch\n\n\n↑\n0\n1\n0\n1\n\n\n↑\n1\n0\n1\n0\n\n\n↑\n1\n1\ntoggle\ntoggle\n\n\nx\n0\n0\nlatch\nlatch\n\n\nx\n0\n1\nlatch\nlatch\n\n\nx\n1\n0\nlatch\nlatch\n\n\nx\n1\n1\nlatch\nlatch\n\n\n\nwhere “latch” represent the Q output remembers whatever the last stored value was. “Toggle” means to flip Q, i.e. 0 -&gt; 1, 1 -&gt; 0. “↑” means the clock signal in a leading edge."
  },
  {
    "objectID": "posts/jk-ff/index.html#problem",
    "href": "posts/jk-ff/index.html#problem",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "3 Problem",
    "text": "3 Problem\n\n3.1 First try\nWrite the following content in JKFF.vhdl:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity JKFF_nodelay is\n    Port (\n        J : in STD_LOGIC;     -- J input\n        K : in STD_LOGIC;     -- K input\n        clk : in STD_LOGIC;   -- Clock input\n        Q : out STD_LOGIC;    -- Output Q\n        QN : out STD_LOGIC    -- Output QN (complement of Q)\n    );\nend JKFF_nodelay;\n\narchitecture Structural of JKFF_nodelay is\n    -- Internal signals for latch and clock gating\n    signal S, R : STD_LOGIC;    -- Set and Reset inputs for the latch\n    signal Q_int, QN_int : STD_LOGIC := '0'; -- Internal Q and QN for feedback\n\nbegin\n    S &lt;= (J and clk) and QN_int;\n    R &lt;= (K and clk) and Q_int;\n\n    -- NOR gate-based latch\n    Q_int &lt;= not (R or QN_int);\n    QN_int &lt;= not (S or Q_int);\n\n    -- Outputs\n    Q &lt;= Q_int;    -- Main output\n    QN &lt;= QN_int;  -- Complementary output\nend Structural;\nThen write a testbench file to test the behavior of this circuit, you will find it will not work well.\n\n\n3.2 Reason\nThe reason (probably, only personal view) is that JK FF have two layers of feedback (instead of SR FF, which just have one), since we loop our output Q not only to the NOR gates, but also AND to our initial inputs J and K. This confuse the compiler because the resultant signal changes so fast and maybe not have a stable consequence, so the compiler do not know how to respond to this kind of feedback.\nSo we introduce some delay in the gates to simulate the reality closer."
  },
  {
    "objectID": "posts/jk-ff/index.html#solution",
    "href": "posts/jk-ff/index.html#solution",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "4 Solution",
    "text": "4 Solution\n\n4.1 Adding delay in the gates\nWe write the following content in another file JKFF.vhdl:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity JKFF is\n    Port (\n        J : in STD_LOGIC;     -- J input\n        K : in STD_LOGIC;     -- K input\n        clk : in STD_LOGIC;   -- Clock input\n        Q : out STD_LOGIC;    -- Output Q\n        QN : out STD_LOGIC    -- Output QN (complement of Q)\n    );\nend JKFF;\n\narchitecture Structural of JKFF is\n    -- Internal signals for latch and clock gating\n    signal S, R : STD_LOGIC;    -- Set and Reset inputs for the latch\n    signal Q_int, QN_int : STD_LOGIC := '0'; -- Internal Q and QN for feedback\n\nbegin\n    S &lt;= (J and clk) and QN_int after 0.1 ns;\n    R &lt;= (K and clk) and Q_int after 0.1 ns;\n\n    -- NOR gate-based latch\n    Q_int &lt;= not (R or QN_int) after 0.2 ns;\n    QN_int &lt;= not (S or Q_int) after 0.2 ns;\n\n    -- Outputs\n    Q &lt;= Q_int;    -- Main output\n    QN &lt;= QN_int;  -- Complementary output\nend Structural;\nAnd run the following JKFF_tb.vhdl:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity JKFF_tb is\n-- No ports for testbench\nend JKFF_tb;\n\narchitecture Behavioral of JKFF_tb is\n    -- Component declaration\n    component JKFF\n        Port (\n            J : in STD_LOGIC;\n            K : in STD_LOGIC;\n            clk : in STD_LOGIC;\n            Q : out STD_LOGIC;\n            QN : out STD_LOGIC\n        );\n    end component;\n\n    -- Signals to connect to the JKFF\n    signal J, K, clk : STD_LOGIC := '0';\n    signal Q, QN : STD_LOGIC;\n\nbegin\n    -- Instantiate the JK Flip-Flop\n    uut: JKFF\n        Port Map (\n            J =&gt; J,\n            K =&gt; K,\n            clk =&gt; clk,\n            Q =&gt; Q,\n            QN =&gt; QN\n        );\n\n    -- Clock generation process\n    clk_gen: process\n    begin\n        for i in 0 to 9 loop -- Generate 10 clock cycles\n            clk &lt;= '0';\n            wait for 9 ns; -- Low for 5 ns\n            clk &lt;= '1';\n            wait for 1 ns; -- High for 5 ns\n        end loop;\n        wait; -- End simulation after clock finishes\n    end process;\n\n    -- Stimulus process to apply test cases\n    stimulus: process\n    begin\n        -- Test Case 1: Hold state (J = 0, K = 0)\n        J &lt;= '0'; K &lt;= '0';\n        wait for 20 ns;\n\n        -- Test Case 2: Set state (J = 1, K = 0)\n        J &lt;= '1'; K &lt;= '0';\n        wait for 20 ns;\n\n        -- Test Case 3: Reset state (J = 0, K = 1)\n        J &lt;= '0'; K &lt;= '1';\n        wait for 20 ns;\n\n        -- Test Case 4: Toggle state (J = 1, K = 1)\n        J &lt;= '1'; K &lt;= '1';\n        wait for 40 ns;\n\n        -- Return to Hold state\n        J &lt;= '0'; K &lt;= '0';\n        wait for 20 ns;\n\n        wait; -- End simulation\n    end process;\nend Behavioral;\n\n\n4.2 Results\nWe will get the following waveform:\n\nThere are several strange things happen here:\n\n4.2.1 What happen before around 30 ns?\nThe Q and QN oscillates at the same pace. Why? It’s because both J and K are zero. For a JK FF, this means to remember the last value. But the last value of Q and QN are both zero (we initialize them in the JKFF.vhdl file):\nsignal Q_int, QN_int : STD_LOGIC := '0'; -- Internal Q and QN for feedback\nThis is invalid and unstable! So they oscillates with a period of 0.2 ns, which is exactly the delay time of the NOR gates.\n\n\n\nLocally zoomed in\n\n\nOK, if instead we initialize the Q and QN a valid value, say Q_int=0 and QN_int=1 like this (in JKFF.vhdl file):\nsignal Q_int : STD_LOGIC := '0'; -- Internal Q for feedback\nsignal QN_int : STD_LOGIC := '1'; -- Internal QN for feedback\nSince they are valid, hence stable, Q and QN will not oscillates as expected:\n\n\n\n4.2.2 Why not toggle successfully?\nAt around \\(t = 70\\) ns, both J and K are 1. This means at the leading edge of the clock signal clk, Q and QN should both flipped! But according to the waveform, they tried but failed, with a tiny pulse around that time.\nI tried several clk pulse width and analysed the JK circuit in “slow-motion” carefully (Try this! Very surprising!). Finally I figured it out:\nIt’s because JK FF don’t want the clk signal be high for too long. This is because if the clk line hold high for a sufficient long period, the signal at Q and QN will “backpropagate” (Haha just borrow the term) to the inputs, continue to control whether or not the J and K signal should come in. If we increase the so-called “duty-ratio” of the clk signal, we will see these:\n\n\n\nDuty ratio = 0.20\n\n\n\n\n\nDuty ratio = 0.30\n\n\n\n\n\nDuty ratio = 0.40\n\n\n\n\n\nDuty ratio = 0.65\n\n\nSome value of duty ratio (e.g. 0.65) happen to toggle the Q successfully, while others do not.\nYou can think of what value could the duty ratio be? (Given the clock cycle and the propagation delay of all gates) This is a very intereting yet tedious problem to consider. But as long as you understand why Q oscillates, you understand this.\nTherefore people say that there are no “JK latches”."
  },
  {
    "objectID": "posts/jk-ff/index.html#conclusion",
    "href": "posts/jk-ff/index.html#conclusion",
    "title": "JK Flip-flop not Behavioral (VHDL version)",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThe VHDL realization of a JK FF can be achieved by introducing propagation delay to the gates."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html",
    "href": "posts/isomorphism-theorem/index.html",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "",
    "text": "Prerequisites: Quotient group, kernel of a homomorphism, normal subgroup, basic category theory.\nApologize for the handwriting, since it’s hard for quarto to support tikz-cd package."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#takeaways",
    "href": "posts/isomorphism-theorem/index.html#takeaways",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "1 Takeaways",
    "text": "1 Takeaways\n\nYou gotta think of a homomorphism whenever there is a quotient.\nHomomorphisms are WAAAAY more important than groups themselves!\nFirst isomorphism theorem: Universal property of quotients, most important.\nSecond isomorphism theorem: How subgroups behave under projections.\nThird isomorphism theorem: Just a diagram commutes."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#normal-subgroup-iff-kernel",
    "href": "posts/isomorphism-theorem/index.html#normal-subgroup-iff-kernel",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "2 Normal Subgroup \\(\\iff\\) Kernel",
    "text": "2 Normal Subgroup \\(\\iff\\) Kernel\nIn this section, I will introduce to you a very important mathematical habit: always think of a homomorphism whenever there is a quotient. This helps me a lot in not only understanding the three isomorphism theorems, but also in various topics such as projective geometry, cohomology, etc. First, let’s look at some concepts, if you are familiar with them, you can skip this section.\n\n2.1 Center and Centralizer\nThere are two very similar concepts: center of a group and conjugation operation. Center can be generalized to the concept of a centralizer. Conjugation is related to the concept of a normal subgroup and normalizer.\nWe know that not every group is abelian, but there are some elements in a group that commute with every other element in the group. We collect them to form a set. In fact, not only a set, but also a subgroup, called the center of the group.\n\n\n\n\n\n\n\nCenter\n\n\n\n\nDefinition 1 The center \\(Z(G)\\) of a group \\(G\\) is: \\[\n\\begin{aligned}\nZ(G) :=& \\{ z \\in G \\mid \\forall g \\in G, zg = gz \\} \\\\\n=& \\{ z \\in G \\mid \\forall g \\in G, z = gzg^{-1} \\}.\n\\end{aligned}\n\\]\n\n\n\n\nHere are some properties of the center, you can think about them yourself:\n\n\\(Z(G) \\trianglelefteq G\\). (See Definition 4 for the definition of normal subgroup.)\n\\(Z(G) = G\\) iff \\(G\\) is abelian.\n\\(Z(G)\\) is itself abelian.\n\nIf we do not want to make it commutes with every element in the group, but only with some elements in a set \\(S\\) (not necessarily a subgroup1 of \\(G\\)), this generalize to the definition of the centralizer of \\(S\\) in \\(G\\):\n\n1 \\(S \\le G\\) means \\(S\\) is a subgroup of \\(G\\). \\(S \\subseteq G\\) means \\(S\\) is a subset of \\(G\\).\n\n\n\n\n\nCentralizer\n\n\n\n\nDefinition 2 The centralizer \\(C_G(S)\\) of a set \\(S\\) \\((S \\subseteq G)\\) of group \\(G\\) is: \\[\n\\begin{aligned}\nC_G(S) :=& \\{ c \\in G \\mid \\forall s \\in S, cs = sc \\} \\\\\n=& \\{ c \\in G \\mid \\forall s \\in S, c = scs^{-1} \\}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof: \\(C_G(S) \\le G\\)\n\n\n\n\n\n\nClosure:\nLet \\(a, b \\in C_G(S)\\), i.e., \\(\\forall s \\in S, as = sa, bs = sb\\). We want to show that \\(ab \\in C_G(S)\\): \\[\n(ab)s = a(bs) = a(sb) = (as)b = (sa)b = s(ab) \\implies ab \\in C_G(S), \\forall s \\in S.\n\\]\nAssociativity: Inherited from \\(G\\).\nIdentity: \\(e_G \\in C_G(S)\\) because \\(\\forall s \\in S, es = se = s\\).\nInverse: Let \\(a \\in C_G(S)\\), i.e., \\(\\forall s \\in S, as = sa\\). We want to show that \\(a^{-1} \\in C_G(S)\\): \\[\n\\begin{aligned}\nas = sa & \\implies a^{-1}as = a^{-1}sa \\\\\n& \\implies s = a^{-1}sa \\\\\n& \\implies sa^{-1} = a^{-1}saa^{-1} \\\\\n& \\implies sa^{-1} = a^{-1}s \\\\\n& \\implies a^{-1} \\in C_G(S).\n\\end{aligned}\n\\]\n\n\n\n\n\nClearly, the center is a special case of the centralizer: \\[\nZ(G) = C_G(G).\n\\]\n\n\n2.2 Conjugation\nThe sandwich operation \\(gzg^{-1}\\) in Definition 1 pops out frequenctly in math2. We give it a name: conjugation of \\(z\\) by \\(g\\).\n\n2 Similar matrices, quarternion representation of 3D rotation, etc.\n\n\n\n\n\nConjugation\n\n\n\n\nDefinition 3 \\(a, b \\in G\\) conjugate \\(:\\iff\\) \\(\\exists g \\in G\\) such that \\(a = gbg^{-1}\\).\n\n\n\n\nIt’s easy to show that conjugation defines an equivalence relation on \\(G\\):\n\n\n\n\n\n\n\nProof: Conjugation is an Equivalence Relation\n\n\n\n\n\nTo show conjugation defined in Definition 3 is an equivalence relation \\(\\sim\\), we need to show it is reflexive, symmetric, and transitive.\n\nReflexive: \\(\\exists e \\in G, a = eae^{-1}\\), hence \\(a \\sim a\\).\nSymmetric: \\[\n\\begin{aligned}\na \\sim b & \\iff \\exists g \\in G, a = gbg^{-1} \\\\\n& \\iff \\exists g^{-1} \\in G, b = g^{-1}ag \\\\\n& \\iff b \\sim a.\n\\end{aligned}\n\\]\nTransitive: \\[\n\\begin{aligned}\na \\sim b, b \\sim c & \\iff \\exists g, h \\in G, a = gbg^{-1}, b = hch^{-1} \\\\\n& \\iff \\exists g, h \\in G, a = ghch^{-1}g^{-1} \\\\\n& \\iff \\exists gh \\in G, a = (gh)c(gh)^{-1} \\\\\n& \\iff a \\sim c.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n2.3 A Natural Homomorphism\n\n\n\n\n\n\n\nConjugation Homomorphism\n\n\n\n\nProposition 1 Let \\(G\\) be a group and \\(f \\in \\operatorname{Hom}(G, \\operatorname{Aut}(G))\\), where \\(f(g)\\) is defined by conjugation by \\(g\\). Then \\[\n\\ker f = Z(G).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof of Proposition 1\n\n\n\n\n\n\nConjugation by \\(g\\) (denoted \\(\\operatorname{conj}_g\\)) is in \\(\\operatorname{Aut}(G)\\).\nWe need to check that \\(\\operatorname{conj}_g\\) gives an isomorphism from \\(G\\) to itself.\n\nClaim: \\(\\forall g \\in G, f(g) \\in \\operatorname{End}(G)\\).\n\\[\n  f(g)(h h') = g(h h')g^{-1} = ghg^{-1}gh'g^{-1} = f(g)(h) f(g)(h').\n  \\]\nClaim: \\(f(g)\\) is bijective.\n\\[\n  f^{-1}(g) = f(g^{-1}).\n  \\]\n\n\\(f\\) is indeed a homomorphism.\nTake \\(\\forall g, g' \\in G\\), then: \\[\n\\begin{aligned}\nf(g g')(h) &= gg'h(gg')^{-1} \\\\\n&= gg'hg'^{-1}g^{-1} \\\\\n&= f(g) \\circ f(g')(h).\n\\end{aligned}\n\\]\n\\(\\ker f = Z(G)\\).\n\\[\n\\begin{aligned}\n\\ker f &= \\{g \\in G : \\forall h \\in G, ghg^{-1} = h\\} \\\\\n&= \\{g \\in G : gh = hg \\text{ for all } h \\in G\\} \\\\\n&= Z(G).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nInner and Outer Automorphisms\n\n\n\n\nProposition 2 Under the condition in Proposition 1, \\[\n\\operatorname{im} f \\triangleleft \\operatorname{Aut}(G).\n\\]\nDefine the inner automorphism group to be: \\[\n\\operatorname{Inn}(G) := \\operatorname{im} f,\n\\] and naturally the Outer automorphism group to be: \\[\n\\operatorname{Out}(G) := \\frac{\\operatorname{Aut}(G)}{\\operatorname{Inn}(G)}.\n\\] Then we have the following exact sequence: \\[\n1 \\xrightarrow[]{} Z(G) \\hookrightarrow G \\xrightarrow[]{f} \\operatorname{Aut}(G) \\twoheadrightarrow \\operatorname{Out}(G) \\xrightarrow[]{} 1.\n\\]\n\n\n\n\nWe omitted the proof, but visually,\n\n\n\n\n\n\nFigure 1: Relation between \\(Z(G)\\), \\(\\operatorname{Inn}(G)\\), \\(\\operatorname{Out}(G)\\)\n\n\n\n\n\n2.4 Normal Subgroup and Normalizer\nIn fact, we could weaken the condition of \\(z = gzg^{-1}\\) in Definition 1 and \\(c = scs^{-1}\\) in Definition 2 to \\(N = gNg^{-1}\\) and \\(S = gSg^{-1}\\) to get the definition of a normal subgroup \\(N\\) and a normalizer \\(N_G(S)\\) respectively. This is different, because we no longer need pointwise commutativity but only commutativity as a set. In other words, the element after conjugation does not need to be exactly itself necessarily as long as it remains in some set.\n\n\n\n\n\n\n\nNormal Subgroup\n\n\n\n\nDefinition 4 A subgroup \\(N\\) of a group \\(G\\) is normal (denoted \\(N \\trianglelefteq G\\)), iff \\(\\forall g \\in G, N = gNg^{-1}\\), i.e., \\[\n\\forall g \\in G, \\exists n \\in N, \\forall m \\in N, gmg^{-1} = n,\n\\] i.e., normal subgroups are invariant under any conjugation.\n\n\n\n\n\n\n\n\n\n\n\nNormalizer\n\n\n\n\nDefinition 5 The normalizer \\(N_G(S)\\) of a set \\(S\\) \\((S \\subseteq G)\\) of group \\(G\\) is: \\[\n\\begin{aligned}\nN_G(S) :=& \\{ g \\in G \\mid gS = Sg \\} \\\\\n=& \\{ g \\in G \\mid S = gSg^{-1} \\}.\n\\end{aligned}\n\\]\n\n\n\n\nTheir relationship is: If \\(S \\trianglelefteq G\\), then \\(N_G(S) = G\\).\nThe following two results are important:\n\n\n2.5 Kernels are normal\n\n\n\n\n\n\n\nKernel of a homomorphism is normal\n\n\n\n\nLemma 1 Let \\(G, H\\) be groups and \\(\\varphi \\in \\operatorname{Hom}(G, H)\\). Then \\(\\ker(\\varphi) \\trianglelefteq G\\), \\(\\operatorname{im}(\\varphi) \\le H\\).\n\n\n\n\n\n\n\n\n\n\n\nProof: Kernel of a homomorphism is normal\n\n\n\n\n\nWe just need to show that \\(\\forall k \\in \\ker(\\varphi)\\), the conjugate of \\(k (gkg^{-1})\\) is also in \\(\\ker(\\varphi)\\), i.e., \\[\n\\varphi(gkg^{-1}) = e.\n\\] This is trivial.\n\n\n\n\n\n\n2.6 Normal subgroups are kernels\n\n\n\n\n\n\n\nEvery normal subgroup is the kernel of a homomorphism\n\n\n\n\nLemma 2 Let \\(N \\trianglelefteq G\\). Then \\(\\exists \\varphi \\in \\operatorname{Hom}(G, H)\\) s.t. \\(N = \\ker(\\varphi)\\).\n\n\n\n\nHINT: Take \\(\\varphi: G \\to G/N\\), the natural projection from \\(G\\) to the group of cosets (called the quotient group \\(G/N\\)).\n\n\n\n\n\n\nFigure 2: All homomorphisms factors uniquely through the quotient\n\n\n\nAs shown in Figure 2, any group homomorphism \\(\\varphi: G \\to H\\) factors uniquely through the quotient group \\(G/N\\), where \\(N = \\ker(\\varphi)\\). The unique embedding \\(\\bar{\\varphi}\\) implies the universal property of the quotient group (the pair \\((G/N, \\pi)\\) could be viewed as a initial object of a coslice category3).\n3 A coslice category is the dual of a slice category, which is a special case of a comma category.So we have establish the fact that quotients and group homomorphisms are equivalent!"
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#isomorphism-theorem-i",
    "href": "posts/isomorphism-theorem/index.html#isomorphism-theorem-i",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "3 Isomorphism Theorem I",
    "text": "3 Isomorphism Theorem I\nFigure 2 could be viewed as part of the canonical decomposition of a group homomorphism:\n\n\n\n\n\n\nFigure 3: The red box in the canonical decomposition is exactly the first isomorphism theorem\n\n\n\nThe red box in Figure 3 is exactly the first isomorphism theorem:\n\n\n\n\n\n\n\nFirst Isomorphism Theorem\n\n\n\n\nTheorem 1 Let \\(G\\) and \\(H\\) be groups and let \\(\\varphi: G \\to H\\) be a homomorphism. Then: \\[\nG/\\ker(\\varphi) \\simeq \\operatorname{im}(\\varphi)\n\\]\n\n\n\n\nThe rest of the isomorphism theorems largely relies on this first theorem."
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#isomorphism-theorem-ii",
    "href": "posts/isomorphism-theorem/index.html#isomorphism-theorem-ii",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "4 Isomorphism Theorem II",
    "text": "4 Isomorphism Theorem II\n\n\n\n\n\n\n\nSome lemmas\n\n\n\n\nLemma 3 Let \\(G\\) be a group and \\(N \\trianglelefteq G\\), \\(S \\le G\\). Then: \\[\nN \\trianglelefteq SN \\le G \\ge S \\trianglerighteq S \\cap N.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\(SN \\le G\\):\n\nNonempty: Trivial.\nClosure: Let \\(s_1n_1, s_2n_2 \\in SN\\), we want to show that \\((s_1n_1)(s_2n_2) \\in SN\\): \\[\n  (s_1n_1)(s_2n_2) = s_1(n_1s_2)n_2.\n  \\] How to deal with the middle term? Since \\(N \\trianglelefteq G\\), we have \\(n_1s_2 = sn_1\\) for some \\(s \\in N\\). Then: \\[\n  s_1(n_1s_2)n_2 = s_1(sn_1)n_2 = (s_1s)(n_1n_2) \\in SN.\n  \\]\nInverses: Let \\(sn \\in SN\\), we want to show that \\((sn)^{-1} = n^{-1}s^{-1} \\in SN\\). Again use the fact that \\(N \\trianglelefteq G\\): \\[\n  \\exists s' \\in N, n^{-1}s^{-1} = s'n^{-1} \\in SN.\n  \\]\n\nThe rest can be proved by this diagram:\n\n\n\n\n\n\nFigure 4: Proof of the isomorphism theorem 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Isomorphism Theorem\n\n\n\n\nTheorem 2 Let \\(G\\) be a group and \\(N \\trianglelefteq G\\), \\(S \\le G\\). Then: \\[\n\\frac{SN}{N} \\simeq \\frac{S}{S \\cap N}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs shown in Figure 4, we have:\n\n\\(N \\trianglelefteq SN\\), since \\(N = \\ker(\\varphi|_{SN})\\). By the first isomorphism theorem, we have \\(\\frac{SN}{N} \\simeq \\varphi (SN)\\).\n\\(S \\cap N \\trianglelefteq S\\), since \\(S \\cap N = \\ker(\\varphi|_{S})\\). By the first isomorphism theorem, we have \\(\\frac{S}{S \\cap N} \\simeq \\varphi (S)\\).\n\nPlus, \\(\\varphi (SN) = \\varphi(S)\\), therefore: \\[\n\\frac{SN}{N} \\simeq \\varphi (SN) = \\varphi(S) \\simeq \\frac{S}{S \\cap N},\n\\] i.e., \\[\n\\frac{SN}{N} \\simeq \\frac{S}{S \\cap N}.\n\\]"
  },
  {
    "objectID": "posts/isomorphism-theorem/index.html#isomorphism-theorem-iii",
    "href": "posts/isomorphism-theorem/index.html#isomorphism-theorem-iii",
    "title": "Understanding Isomorphism Theorems for Groups 群同构定理的理解",
    "section": "5 Isomorphism Theorem III",
    "text": "5 Isomorphism Theorem III\n\n\n\n\n\n\n\nThird Isomorphism Theorem\n\n\n\n\nTheorem 3 Let \\(G\\) be a group and \\(N \\trianglelefteq G\\), \\(K \\trianglelefteq G\\), \\(N \\subset K\\). Then \\(N \\trianglelefteq K\\) and \\[\n\\frac{G/N}{K/N} \\simeq \\frac{G}{K}\n\\]\n\n\n\n\nTo prove this theorem, we need to prove a lemma:\n\n\n\n\n\n\n\nThe image of normal subgroup is normal\n\n\n\n\nLemma 4 Let \\(G\\) be a group, \\(K \\trianglelefteq G\\), \\(\\varphi \\in \\operatorname{Hom}(G, H)\\). The image of \\(K\\) under \\(\\varphi\\) is normal in \\(H\\), i.e., \\[\n\\varphi(K) \\trianglelefteq \\operatorname{im} \\varphi\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe need to show \\(\\forall h \\in \\operatorname{im} \\varphi\\) and \\(\\forall k \\in K\\), \\(h\\varphi(k)h^{-1} \\in \\varphi(K)\\).\nLet:\n\n\\(h \\in \\operatorname{im}(\\varphi)\\), so there exists \\(g \\in G\\) such that \\(h = \\varphi(g)\\),\n\\(k \\in K\\), so \\(\\varphi(k) \\in \\varphi(K)\\)\n\nNow compute: \\[\nh \\varphi(k) h^{-1} = \\varphi(g) \\varphi(k) \\varphi(g)^{-1}\n= \\varphi(g k g^{-1}) \\quad \\text{(since \\( \\varphi \\) is a homomorphism)}\n\\]\nBut since $ K G $, we know \\(g k g^{-1} \\in K\\), so: \\[\n\\varphi(g k g^{-1}) \\in \\varphi(K).\n\\] Therefore: \\[\nh \\varphi(k) h^{-1} \\in \\varphi(K).\n\\]\nSo \\(\\varphi(K)\\) is closed under conjugation by elements of \\(\\operatorname{im}(\\varphi)\\), i.e.,\n\\[\n\\varphi(K) \\trianglelefteq \\operatorname{im}(\\varphi).\n\\]\n\n\n\n\nNow the proof of the third isomorphism theorem can be proved by this diagram:\n\n\n\n\n\n\nFigure 5: Commutative diagram of groups\n\n\n\n\n\n\n\n\n\n\nProof of the third isomorphism theorem\n\n\n\n\n\nFigure 5 can be visually represented by this cartoon:\n\n\n\n\n\n\nFigure 6: Visual demo of the proof of the third isomorphism theorem\n\n\n\nby the first isomorphism theorem, we have: \\[\nG/N \\simeq \\sigma_1 (G) \\triangleright \\sigma_1 (K) \\simeq K/N.\n\\] Therefore, \\[\n\\frac{G/N}{K/N} = \\frac{\\sigma_1 (G)}{\\sigma_1 (K)} \\simeq \\sigma_2 \\circ \\sigma_1 (G) =: \\sigma (G) \\simeq \\frac{G}{K}.\n\\]"
  },
  {
    "objectID": "posts/electrostatics/index.html",
    "href": "posts/electrostatics/index.html",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "",
    "text": "Changing the mindset from forces to fields.\nMaxwell’s First Equation = Inverse Square + Superposition\nCurl-free (conservative) property of static electric field has nothing to do with the inverse square property! (It’s because the radial direction of the field.)"
  },
  {
    "objectID": "posts/electrostatics/index.html#takeaway",
    "href": "posts/electrostatics/index.html#takeaway",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "",
    "text": "Changing the mindset from forces to fields.\nMaxwell’s First Equation = Inverse Square + Superposition\nCurl-free (conservative) property of static electric field has nothing to do with the inverse square property! (It’s because the radial direction of the field.)"
  },
  {
    "objectID": "posts/electrostatics/index.html#sec-welcome",
    "href": "posts/electrostatics/index.html#sec-welcome",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "1 Welcome!",
    "text": "1 Welcome!\nThis is the first chapter of my electromagnetics series. I will introduce to you the theory of electromagnetism through this series, not in the order of history, but in the order of a self-contained logic that satisfies people. The reason I did this is that history is too complicated, confusing and sometimes even wrong. One would not fully understand the trajactory of any subject (geometry, Newton’s law, General relativity, etc) without experiencing it. So I would NOT recommend you go through the tedious history of any subject but develop your own logic-complete explanation of it after reading tons of wikipedia and stackexchange, in other words, that explanation should make sense in the history in another parallel universe!\nYou will understand the first one of the famous Maxwell’s Equations in this blog:\n\\[\n\\begin{aligned}\n    \\mathbf{\\nabla} \\cdot \\mathbf{E} &= \\frac{\\rho}{\\varepsilon_0} \\quad &\\text{(Gauss's law)} \\\\\n    \\mathbf{\\nabla} \\cdot \\mathbf{B} &= 0 \\quad &\\text{(Gauss's law for magnetism)} \\\\\n    \\mathbf{\\nabla} \\times \\mathbf{E} &= -\\frac{\\partial \\mathbf{B}}{\\partial t} \\quad &\\text{(Faraday's law)} \\\\\n    \\mathbf{\\nabla} \\times \\mathbf{B} &= \\mu_0 \\mathbf{J} + \\mu_0 \\varepsilon_0 \\frac{\\partial \\mathbf{E}}{\\partial t} \\quad &\\text{(Ampère's law)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/electrostatics/index.html#physics-world-to-math-world",
    "href": "posts/electrostatics/index.html#physics-world-to-math-world",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "2 Physics World to Math World",
    "text": "2 Physics World to Math World\nPhysics originates from observations in the real world. If you keep an eye on it, you could discover the following phenomena yourself:\n\nThe force between two charges is proportional to their charge, and inversely proportional to their distance, squared.\nNo isolated “magnetic charges” found in nature.\nChanging magnetic flux leads to current in a circuit!\nTwo parallel wires attract or repel each other when current is applied!\n\nThese four observations leads to the four equations by Maxwell."
  },
  {
    "objectID": "posts/electrostatics/index.html#integral-form-of-maxwells-first-equation",
    "href": "posts/electrostatics/index.html#integral-form-of-maxwells-first-equation",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "3 Integral Form of Maxwell’s First Equation",
    "text": "3 Integral Form of Maxwell’s First Equation\n\n3.1 Inverse Square Law\nAccording to observation 1, we could write1: \\[\nF \\propto \\frac{Qq}{r^2},\n\\] or in an equation: \\[\n\\boxed{\nF = k \\frac{Qq}{r^2},\n}\n\\]\n1 Non-bold-face letters (\\(F\\)) are scalars, bold-face letters (\\(\\mathbf{F}\\)) are vectors.2 The unit of charge (Coulomb) is defined in the following way (The reason Ampere are defined first is because Ampere happens to be one of the base unit of SI): - Current: Ampere (I: A): First we define Ampere (surprise!) to be the intensity of current on two ideal wires that are 1 metre apart and produces a force of \\(2 \\times 10^{-7}\\) N between them (1N is defined to be the force that makes an object of mass 1kg moves at the acceleration of \\(1\\text{m}/\\text{s}^2\\)) - Charge: Coulomb (Q: C): Then we define Coulomb to be the amount of charge the that passes through a point with a current of 1A over 1 second.where \\(k\\) is a constant, \\(Q\\) and \\(q\\) are the amount of charge2 on two small objects, \\(r\\) is their distance.\n\n\n3.2 Conservative Property\nLet’s consider the work done when we fix \\(Q\\) and gradually move \\(q\\) in a certain path. Since the force from \\(Q\\) to \\(q\\) is always radially from \\(Q\\), it turns out that the work done only depends on the initial and final position of \\(q\\) regardless of the moving process in between! We say a force like this Conservative3. We would like to think that there is a number \\(U\\) attached at every spatial point around \\(Q\\) such that the the work done from point \\(A\\) and \\(B\\) is just that \\(U(A)-U(B)\\)4, i.e., \\[\nW_{AB} =: U(A) - U(B).\n\\]\n3 It’s worth noting that conservativity has nothing to do with the inverse square property. The potential energy of hypothetical “inverse force” and “inverse cubic force” are: \\[\nU_{\\frac{1}{r}} = -kQq \\ln r,\n\\] and \\[\nU_{\\frac{1}{r^3}} = \\frac{kQq}{2} \\frac{1}{r^2}.\n\\]4 Why not \\(U(B)-U(A)\\)? Because we want the quantity \\(U\\) also indicates the tendancy that \\(q\\) would move. More likely to move, \\(U\\) should be larger.\n\n\n\n\\(U(A)\\) is expected to be larger than \\(U(B)\\)\n\n\n5 Actually, should be \\(U(r) = -k \\frac{Q}{r} + \\operatorname{const}\\). But when \\(r \\to \\infty\\), we expect \\(U\\) to be \\(0\\) because \\(q\\) do not have the ability to do work.\n\\(U(r)\\) is called the potential energy at \\(r\\). We claim5 that \\[\n\\boxed{\nU(r) = -k \\frac{Qq}{r},\n}\n\\] because \\[\nW_{AB}\n= \\int_{A \\to B} \\mathbf{F} \\cdot \\mathbf{\\mathrm{d}r}\n= \\int_{r_0}^{r_1} k \\frac{Qq}{r^2} \\mathrm{d}r\n= -k \\frac{Qq}{r_1} - k \\frac{Qq}{r_0}\n=: U(A) - U(B).\n\\]\nTherefore, the work done is just the difference of \\(U\\) with a negative sign: \\[\nW_{AB} = - \\Delta U.\n\\]\nWe also know that\n\\[\n\\mathrm{d} W_{AB} = \\mathbf{F} \\cdot \\mathrm{d} \\mathbf{l} = F \\cdot \\mathrm{d} l_\\parallel,\n\\] where \\(\\mathrm{d} \\mathbf{l}\\) is a small displacement and \\(\\mathrm{d} l_\\parallel\\) is the length of the projection of that small displacement onto the direction of \\(\\mathbf{F}\\), i.e., the direction of \\(\\mathbf{\\nabla} U\\), reversed. Hence, \\[\nF = \\frac{\\mathrm{d} W_{AB}}{\\mathrm{d} l_\\parallel} = - \\frac{\\mathrm{d} U}{\\mathrm{d} l_\\parallel}.\n\\]\nIn vector notation, \\[\n\\boxed{\n\\mathbf{F} = - \\mathbf{\\nabla} U.\n}\n\\tag{1}\\]\n\n\n3.3 Get Rid of Test-charge\nWhen \\(q\\) is far smaller than \\(Q\\), it is called a test charge, which is used to “test” the effect of \\(Q\\) to its surroundings and minimize other interference. It is naturally to get rid of \\(q\\) and define a quantity \\(E\\) that only depends on \\(Q\\), we expect that \\(E\\) satisfies: \\[\n\\mathbf{F} =: \\mathbf{E} q,\n\\] where \\(E\\) obviously equal to: \\[\n\\boxed{\nE = k \\frac{Q}{r^2}.\n}\n\\]\nWe call \\(E\\) the electric field generated by \\(Q\\)6.\n6 Since \\(\\mathbf{E}\\) is equivalent to force (just up to a constant), all properties of \\(\\mathbf{E}\\) is inherited from \\(\\mathbf{F}\\), such as vector property, superposition, conservativity, etc.Therefore, Equation 1 could be written as: \\[\n\\mathbf{E}q = - \\mathbf{\\nabla} U.\n\\tag{2}\\]\nWe could also get rid of \\(q\\) in Equation 2 by defining a quantity \\(V\\) called the (Electric) potential generated by \\(Q\\): \\[\n\\boxed{\nU := Vq,\n}\n\\] so we have \\[\n\\mathbf{E}q = -\\mathbf{\\nabla} V q\n\\] \\[\n\\implies\n\\boxed{\n\\mathbf{E} = - \\mathbf{\\nabla} V.\n}\n\\tag{3}\\]\nYou can compare Equation 1 and Equation 3, the latter is test-charge-free version of the former!\n\n\n\n\n\n\nTip 1: The spirit of fields\n\n\n\nThis mindset from force to field is extremely important! “Field” originates from “force” but later evolves independently from it, as you will see. Thinking in terms of “fields” rather than “forces” is a key factor that distinguishes beginners from experts. Now you have evolved to the second level – make “fields” be your second nature!\n\n\n\n\n3.4 An Interesting Question\nHow to know how many charges inside some closed region?\n\n\n\n\n\nThe question\n\n\n\nProposition 1 The charge inside some closed surface \\(S\\) can be calculated by only looking at the field sitting on its surface7: \\[\n\\text{Flux} \\propto Q_{\\text{in}},\n\\] where the flux \\(\\Phi\\) is defined: \\[\n\\Phi := \\oiint_S \\mathbf{E} \\cdot \\mathrm{d} \\boldsymbol{A}.\n\\]\n7 This claim directly comes from the inverse square law and superposition principle of fields.\n\nSolution 1. \n\nWe will consider the case then there is only one point charge \\(q\\) inside \\(S\\).\nConsider a sphere \\(R\\) of radius \\(r\\) around \\(q\\) in Figure 1, it’s obviously that the flux through \\(R\\) does not related to \\(r\\), because the surface area increases at the rate of \\(r^2\\) and the field decays at the rate of \\(1/r^2\\). Just to be intimidating, \\[\n\\Phi_R = \\oiint_R k \\frac{q}{r^2} dA = k \\frac{q}{r^2} \\cdot 4 \\pi r^2 = 4 \\pi k q \\propto q.\n\\]\nWe commonly let \\[\n\\boxed{\n     k = \\frac{1}{4 \\pi \\epsilon_0}\n}\n  \\tag{4}\\] to simpify8 Equation 4 to be \\[\n\\Phi_R = \\frac{q}{\\epsilon_0}.\n\\]\n\n8 We introduce the symbol \\(\\epsilon_0\\) by the motivation to eliminate the “\\(4 \\pi\\)” Equation 4. But the meaning of \\(\\epsilon_0\\) would be clear later until we introduce the electric fields in matter. Don’t worry.\nNow we claim that \\[\n\\Phi_R = \\Phi_S,\n\\] where \\(S\\) is an arbitrary closed surface outside \\(R\\).\nAgain we use the inverse square property, the flux through \\(\\mathrm{d}R\\) should be same as the flux through the blue circle in Figure 1. Plus, the flux through the blue circle is exactly the same as the flux through \\(\\mathrm{d}S\\) as shown in Figure 2 (since their “perpendicular” surface area are the same)\nWe then use superposition property of fields to obtain the equation of multiple charges enclosed.\nSuppose there are \\(N=3\\) point charges inside \\(S\\) as shown in Figure 3, the total flux is\n\\[\n\\Phi\n= \\oiint_S \\mathbf{E} \\cdot  \\mathrm{d} \\mathbf{A}\n= \\sum_{i = 1}^3 \\left(\\oiint_S \\mathbf{E}_i \\cdot  \\mathrm{d} \\mathbf{A}\\right)\n= \\sum_{i = 1}^3 \\frac{q_i}{\\epsilon_0}\n= \\frac{Q_{\\text{in}}}{\\epsilon_0}.\n  \\tag{5}\\]\nOf course Equation 5 can be generalized when \\(N\\) is arbitrary. And ura! We have just proof the integral version of Maxwell’s first equation! \\[\n\\boxed{\n\\oiint_S \\mathbf{E} \\cdot  \\mathrm{d} \\mathbf{A} = \\frac{Q_{\\text{in}}}{\\epsilon_0}.\n}\n  \\tag{6}\\]\nThis is also known as Gauss’s Law. Equation 6 holds for any closed surface \\(S\\).\n\n\n\n\n\n\n\n\n\n\nFigure 1: Single point charge case\n\n\n\n\n\n\n\n\n\nFigure 2: The flux through the blue circle is exactly the same as the flux through \\(\\mathrm{d}S\\)\n\n\n\n\n\n\n\n\n\nFigure 3: Superposition of the fields of every point charge inside \\(S\\)"
  },
  {
    "objectID": "posts/electrostatics/index.html#differential-form-of-maxwells-first-equation",
    "href": "posts/electrostatics/index.html#differential-form-of-maxwells-first-equation",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "4 Differential Form of Maxwell’s First Equation",
    "text": "4 Differential Form of Maxwell’s First Equation\nThe charges in the real world are not commonly appears like an infinitesimal point. They distributed evenly through a body instead of concentrate on a point of no size. Therefore, \\(Q_{\\text{in}}\\) on the RHS9 of Equation 6 could be write as an integral: \\[\n\\frac{Q_{\\text{in}}}{\\epsilon_0} = \\iiint_V \\frac{\\rho}{\\epsilon_0} \\mathrm{d}V,\n\\] where \\(V\\) is the region enclosed by \\(S\\), \\(\\rho\\) is the density10 of charges at some place inside \\(S\\).\n9 Right hand side.10 In the case of point charges, this density if infinite. Mathematicians use so-called \\(\\delta\\)-function to describe the behaviour of this kind of “degenerated” density. It is not a function, just a symbol following some rules that deals with the Mathematical OCD that forces the form of a certain integral unchanged when some mass is concentrated on a small point.\n\n\n\nReal world charges are not point charge\n\n\n\nWhat about the LHS of Equation 6? We expect it to be also written in a kind of volume integral to cancel it out with the RHS: \\[\n\\oiint_S \\mathbf{E} \\cdot  \\mathrm{d} \\mathbf{A} = \\iiint_V \\boxed{???} \\mathrm{d}V.\n\\]\nAnd then \\[\n\\iiint_V \\boxed{???} \\mathrm{d}V = \\iiint_V \\frac{\\rho}{\\epsilon_0} \\mathrm{d}V\n\\tag{7}\\] holds for any11 volume \\(V\\), so we can claim that \\[\n\\boxed{???} = \\frac{\\rho}{\\epsilon_0}.\n\\tag{8}\\]\n11 Any is very important! Without any, we cannot derive Equation 7 from Equation 8.12 Also denoted \\(\\mathbf{\\nabla} \\cdot \\mathbf{E}\\).Luckily! Stoke’s Theorem tells us \\[\n\\boxed{???} = \\operatorname{div} \\mathbf{E}.\n\\] For those of you not familiar with multivariable calculus, \\(\\operatorname{div} \\mathbf{E}\\) is called the divergence12 of \\(\\mathbf{E}\\), which is a scalar-valued function purely derived from \\(\\mathbf{E}\\).\nFinally! We got the differential form: \\[\n\\boxed{\n\\operatorname{div} \\mathbf{E} = \\frac{\\rho}{\\epsilon_0}.\n}\n\\tag{9}\\]"
  },
  {
    "objectID": "posts/electrostatics/index.html#thinking-problem",
    "href": "posts/electrostatics/index.html#thinking-problem",
    "title": "EM Chapter I: Maxwell’s First Equation",
    "section": "5 Thinking Problem",
    "text": "5 Thinking Problem\n\nExercise 1 Think about what properties of \\(\\mathbf{E}\\) ensures that Equation 9 holds true?\n\n\nSolution 2. There are only two properties of electric fields that are used to obtain Equation 9:\n\nInverse square\nSuperposition\n\nStoke’s Theorem don’t count because it holds for any vector fields.\nSuperposition is trivial. The non-trivial part is inverse square. This property ensures that we can extend the sphere to an arbitrary surface in Figure 1. What is the nature of inverse square?\nWell, we live in 3-dimensional space. Everything that spreads should somehow decay at the rate of \\(1/r^2\\), like light, gravity, sound, etc. Otherwise it will against the conservation of energy. In general, if we live in a \\(N\\)-dim world, fields should naturally decay at the rate of \\(1/r^{N-1}\\). In other words, everything in \\(N\\)-dim world should decay at the rate of “area” decay. This paper by myself delved a little deeper inside this inspiration."
  },
  {
    "objectID": "posts/create-first-web/index.html",
    "href": "posts/create-first-web/index.html",
    "title": "Create your own website using Hugo on MacOS",
    "section": "",
    "text": "Building a generic website from scratch is a tough work. However, personal websites for blogs, a special type of website, is actually programmatic. Hugo provides a convenient building templates for that. To make a new blog in Hugo, one could only just create a new folder, write a markdown file (in a specific format) and that’s it. You don’t need ANY knowledge about HTML or CSS. Well do you need to buy a domain name for everyone to see your posts? Well, GitHub Pages is a free service where you just push some contents in a repository of your own with some extremely easy command line, you created your sites of names like yourname.github.io."
  },
  {
    "objectID": "posts/create-first-web/index.html#introduction-and-overview",
    "href": "posts/create-first-web/index.html#introduction-and-overview",
    "title": "Create your own website using Hugo on MacOS",
    "section": "",
    "text": "Building a generic website from scratch is a tough work. However, personal websites for blogs, a special type of website, is actually programmatic. Hugo provides a convenient building templates for that. To make a new blog in Hugo, one could only just create a new folder, write a markdown file (in a specific format) and that’s it. You don’t need ANY knowledge about HTML or CSS. Well do you need to buy a domain name for everyone to see your posts? Well, GitHub Pages is a free service where you just push some contents in a repository of your own with some extremely easy command line, you created your sites of names like yourname.github.io."
  },
  {
    "objectID": "posts/create-first-web/index.html#tools-needed",
    "href": "posts/create-first-web/index.html#tools-needed",
    "title": "Create your own website using Hugo on MacOS",
    "section": "2 Tools needed",
    "text": "2 Tools needed\n\n2.1 Homebrew (Packet Manager)\nWe will use a packet manager called Homebrew to install Hugo. Follow the commands here to download Homebrew first, or you can execute:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nOnce downloaded, if you don’t get any error by entering this in the terminal in any folder, you installed it properly (Install testing):\nbrew --version\n\n\n2.2 Git (Version Control System)\nYou can use Homebrew to install Git:\nbrew install git\ngit --version # install testing"
  },
  {
    "objectID": "posts/create-first-web/index.html#install-hugo",
    "href": "posts/create-first-web/index.html#install-hugo",
    "title": "Create your own website using Hugo on MacOS",
    "section": "3 Install Hugo",
    "text": "3 Install Hugo\nRun this in any folder:\nbrew install hugo\nInstall testing:\nhugo version"
  },
  {
    "objectID": "posts/create-first-web/index.html#run-example-theme",
    "href": "posts/create-first-web/index.html#run-example-theme",
    "title": "Create your own website using Hugo on MacOS",
    "section": "4 Run Example Theme",
    "text": "4 Run Example Theme\n\n4.1 Create Framework\nEnter the terminal in any folder, you will be creating another main folder called mysite in it. Folder mysite will contain all the contents that are relevant to your website:\nhugo new site mysite\ncd mysite\nFolder mysite should look like this:\n\n\n\nInside mysite folder\n\n\nwith most folders in it empty. This is the framework.\n\n\n4.2 Choose Theme\nThe themes folder is empty, now we will add some code representing a theme inside it. Now choose a template here and downlaod its source code folder inside the themes folder. I use Stack\ncd themes/\ngit clone https://github.com/CaiJimmy/hugo-theme-stack.git # Replace as needed\n\n\n4.3 Run Theme\nNow there should be a folder inside themes. Now copy all the things inside a folder like exampleSite into the main mysite folder (‘’replace’’).\n\n\n\nCopy the contents in exampleSite into mysite\n\n\nGo to the main mysite folder and remove the original hugo.tomal file, or you can do:\npwd # should be in \"mysite\"\nrm hugo.tomal\nThen (in mysite folder), run:\nhugo server -D\nit will prompt something like:\nBuilt in 865 ms\nEnvironment: \"development\"\nServing pages from disk\nRunning in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender\nWeb Server is available at http://localhost:53844/ (bind address 127.0.0.1) \nPress Ctrl+C to stop\nEnter the link provided (http://localhost:53844/). You are done!"
  },
  {
    "objectID": "posts/create-first-web/index.html#play-around",
    "href": "posts/create-first-web/index.html#play-around",
    "title": "Create your own website using Hugo on MacOS",
    "section": "5 Play Around",
    "text": "5 Play Around\nThis is easy, just compare the contents in each folder and the website and modify things a little."
  },
  {
    "objectID": "posts/create-first-web/index.html#publish-your-website",
    "href": "posts/create-first-web/index.html#publish-your-website",
    "title": "Create your own website using Hugo on MacOS",
    "section": "6 Publish your website",
    "text": "6 Publish your website\nNote the link (http://localhost:53844/) is private and cannot be visited on other devices. So follow these steps to publish it:\n\n6.1 Create GitHub Repository\nGo to your GitHub page and click ‘+’ on the right upper corner, choose New repository and name it like this:\n\n\n\nName of your repository\n\n\ni.e., yourname.github.io, which will be your own domain name.\n\n\n6.2 Push Contents\nRun this:\nhugo --theme=hugo-theme-stack --baseURL=\"https://yourname.github.io/\" --buildDrafts\nThen push the contents in your folder public on it by:\ncd public\ngit init\ngit remote add origin https://github.com/yourname/yourname.github.io.git # change as needed\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\nYou can access your website on https://yourname.github.io/ within several minutes.\n\n\n6.3 Notes\nAfter you change contents locally, the contents on https://yourname.github.io/ will not change automatically, you will have to push it on GitHub again:\ncd public\ngit add .\ngit commit -m \"Add something new\"\ngit push origin main\nAlso the public website will not update instantly, you will have to wait several minutes."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Marcobisky",
    "section": "",
    "text": "University of Electronic Science and Technology of China (UESTC) (Sept 2022 — Present)\n\nStudent, School of Communication Engineering.\n\nUniversity of Glasgow, Scotland, UK (Sept 2022 — present)\n\nStudent, School of Electronic and Computer Engineering."
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Marcobisky",
    "section": "",
    "text": "University of Electronic Science and Technology of China (UESTC) (Sept 2022 — Present)\n\nStudent, School of Communication Engineering.\n\nUniversity of Glasgow, Scotland, UK (Sept 2022 — present)\n\nStudent, School of Electronic and Computer Engineering."
  },
  {
    "objectID": "cv/index.html#engaged-projects",
    "href": "cv/index.html#engaged-projects",
    "title": "Marcobisky",
    "section": "ENGAGED PROJECTS",
    "text": "ENGAGED PROJECTS\nMovable Antenna (MA) for Anti-jamming (Just start)\n\nMain tools: matlab.\nA heuristic investigation into Anti-jamming through stochastic antenna movement.\nConducted under the supervision of Prof. Weidong Mei at UESTC.\n\nComputer Vision (CV) for Quadrotor Aircraft (Feb 2025 — Jun 2025)\n\nMain tools: matlab, python, ROS2.\nAutomatic quadrotor aircraft for objection detection, route planning, and closed-loop flight control.\n6-people group.\n\nRV32I CPU Core for Education  (Jan 2025 — Mar 2025)\n\n\n\n\nGPIO simulation in Digital\n\n\nMain tools: verilog, VHDL, Digital, Kicad, iCESuger FPGA.\nSimulate an entire RISC-V 32 bit CPU in verilog and Digital Software.\nSupport basic peripherals such as GPIOs, IIC, UART, VGA, etc.\nSimple boot ROM in assembly, minimal interrupt service for running a Linux kernel.\n\nAME Source Coding (Oct 2024 — Nov 2024)\n\n\n\n\nProposed AME coding scheme\n\n\nMain tools: python, matlab.\nFinal project of Information Theory Course.\nSecond-order Markov Adapative Approximation (AME) to source-coding the Game of Thrones.\nPerformance evaluation of Huffman and Fano coding.\n\nCNN for Mbed (Feb 2024 — May 2024)\n\n\n\n\nProposed CNN in L432KC MCU\n\n\nMain tools: python, C++.\nConvolutional Neural Network (CNN) integration into an MCU.\nSmart fall detection, body temperature monitoring and real-time data visualization for patients.\n\nA Study of Generalized Fields and Extension to Higher Dimensions1 (Oct 2023 — Feb 2024)\n1 I submitted this paper to the American Journal of Physics, but it was declined for publication.\n\n\nFields in high dimension can be reduced\n\n\n\nA theoretical study of generalized natural fields and behaviours in higher dimensions.\nLargely motivated by my tutor Mr. Yidong Liu and my friends and completed by myself.\n\nHuman Voice Recognition Smart Car (Sept 2023 — Dec 2023)\n\n\n\n\nVoice-controlled car\n\n\nMain tools: C++, STM32F103C8T6 MCU, etc.\nLeader of a 4-people team.\nEnglish words recognition for car movement controlling.\nBasic operations: Moving forwards and backwards, turning or sliding left and right, etc.\n\nAuto Door Opener for Dormitory (Sept 2023 — Oct 2023)\n\n\n\n\nDoor opener tested on breadboard\n\n\nMain tools: C++, Nucleo L432KC MCU, Mbed library, OLED screen, etc.\nThe final project of the Microelectronic System course.\nOpening the dormitory door by password input.\nBasic functions: Setting up password manually, automatically lock for repeated wrong passwords, OLED message displaying, etc.\n\n“XinTong Cup” Electronic Design Competition: Electronic Keyboard Music Player (Sept 2022 — Oct 2022)\n\nMain tools: Keil C51, STC89C52RC MCU, etc.\nLeader of 3-people team.\nA simplified 8-key music player using register-based development on a 8-bit MCU by ST company.\nFunctionality: Single note playing, chord playing, recording ability, replay and rewind capability, etc."
  },
  {
    "objectID": "cv/index.html#academic-recordfullscore",
    "href": "cv/index.html#academic-recordfullscore",
    "title": "Marcobisky",
    "section": "ACADEMIC RECORD2",
    "text": "ACADEMIC RECORD2\n\nDetailed scores of core courses (GPA: 3.88 out of 4.00)\n\n\n\n\n\n\n\nYear\nSubject\nScore (Full mark: 100)\n\n\n\n\nYear 1\nCalculus I/II   Linear Algebra   C Programming   Physics I  \n91/92   84   95   88  \n\n\nYear 2\nPhysics II   Signal and Systems   Probability and Statistics   Microelectronic Systems   Embedded Processors   Circuit Analysis and Design   Computer Network   Academic English  \n96   91   92   92   95   95   94   89  \n\n\nYear 3\nInformation Theory   Principles of Communication   Digital Circuit Design   Machine Learning   Stochastic Signal Analysis  \n91   95   86   86   82"
  },
  {
    "objectID": "cv/index.html#relevant-skills",
    "href": "cv/index.html#relevant-skills",
    "title": "Marcobisky",
    "section": "RELEVANT SKILLS",
    "text": "RELEVANT SKILLS\n\nIT Skills: Latex, (Quarto) Markdown, Typst, Manim3, Github4, Microsoft Office.\nComputer Programming: C/C++, Matlab, Python.\nEmbedded System Programming: RISCV assembly, STM89C5x (Standard lib), Keil C51.\nMath: Self learned (Abstract Algebra (Harvard E-222)), Point-set Topology, Measure Theory, Complex Analysis (MIT 18.04), Functional Analysis, Elementary Differential Geometry, Lie Groups and Lie Algebras (still learning). I didn’t focus on all epsilons and deltas, but their motivations and application potentials.\nTeam Work: Zoom meeting, Notion team, Microsoft team.\nLanguage: No problem in understanding English lectures, GRE score 317, native Chinese.\n\n3 See this video in which I use manim to explain the relation between the adjoint and dual operator.4 My Github."
  },
  {
    "objectID": "cv/index.html#others",
    "href": "cv/index.html#others",
    "title": "Marcobisky",
    "section": "OTHERS",
    "text": "OTHERS\n\nAwards\n\nFirst Prize in the 7th National College Student Art Exhibition and Performance: Symphony No. 4 in D minor, Op. 120, 4th movement, by Robert Schumann. (In violin section)\nTop Academic Scholarship of UESTC: First-class Scholarship for the past two years.\nChina National Scholarship, 2024: Prestigious national award granted for academic excellence, leadership, and overall achievement.\n\n\n\nInterests\n\nClassical Music Enthusiast🎻: Violin player in UESTC symphony orchestra, votary of legendary composer Gustav Mahler and Johann Sebastian Bach.\nBadminton Lover🏸: Sports always refreshes me at any time.\nLearning Everything🔍: I believe everything is learnable by First Principle Thinking and curiosity.\nVolunteer Work🤝: Enjoy helping others. Over 15 hours of volunteering."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My posts",
    "section": "",
    "text": "CCD 通信电路速成笔记\n\n\n\n\n\n\nCircuit\n\n\nCrash-Course\n\n\n\n内容正确性仅限于格院 CCD 这一门课 (更新中)\n\n\n\n\n\nJun 16, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nSolving Laplace’s Equation using Separation of Variables\n\n\n\n\n\n\nDifferential-Equation\n\n\n\nLearn how to solve stationary heat distribution problem under specific boundary conditions\n\n\n\n\n\nMay 1, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Projective Space and Projective Linear Group 射影空间与射影线性群\n\n\n\n\n\n\nAlgebra\n\n\nDifferential-Geometry\n\n\n\nProjective spaces, unit quaternions, \\(SU(2), SO(3)\\), 3-sphere, Möbius transformations, double cover, etc.\n\n\n\n\n\nApr 21, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nUAV Control 无人机控制原理\n\n\n\n\n\n\nControl-Theory\n\n\n\n基于四元数的无人机姿态控制原理 (待完成)\n\n\n\n\n\nApr 7, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nPDE: Wave and Heat Equations Made Obvious 推导波动方程与热传导方程\n\n\n\n\n\n\nDifferential-Equation\n\n\n\nFirst chapter of PDE series!\n\n\n\n\n\nApr 5, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Isomorphism Theorems for Groups 群同构定理的理解\n\n\n\n\n\n\nAlgebra\n\n\n\nIf you struggle to have a clear mental picture of the isomorphism theorems, I hope this helps.\n\n\n\n\n\nMar 31, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nControl Case Study: LQR for Inverted Pendulum!\n\n\n\n\n\n\nControl-Theory\n\n\n\n一个完整的 MATLAB 控制系统设计指南，平衡倒立摆的线性反馈控制系统。\n\n\n\n\n\nMar 25, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Matrices 特殊矩阵与算子\n\n\n\n\n\n\nAlgebra\n\n\n\n正规矩阵、厄米矩阵、酉矩阵、反厄米矩阵等性质\n\n\n\n\n\nMar 15, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nThree ways to Understand the Mixed Product of Vectors! 向量混合积的三种理解\n\n\n\n\n\n\nDifferential-Geometry\n\n\n\nWhen it comes to \\(a \\cdot (b \\times c)\\), what’s in your head?\n\n\n\n\n\nMar 9, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nEM Chapter I: Maxwell’s First Equation\n\n\n\n\n\n\nPhysics\n\n\n\nUnderstand \\(\\mathbf{\\nabla} \\cdot \\mathbf{E} = \\frac{\\rho}{\\varepsilon_0}\\) in one article!\n\n\n\n\n\nMar 7, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?\n\n\n\n\n\n\nDifferential-Geometry\n\n\n\nWhy \\(e_i\\) is exactly the same as \\(\\frac{\\partial}{\\partial x^i}\\)? How to define tangent space at some point of a manifold?\n\n\n\n\n\nMar 4, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is symmetry? 什么是对称性?\n\n\n\n\n\n\nAlgebra\n\n\n\nSymmetry is nothing but a group acts on an object!\n\n\n\n\n\nMar 3, 2025\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nJK Flip-flop not Behavioral (VHDL version)\n\n\n\n\n\n\nHardware\n\n\n\nEasy to write a behavioral description of JK-FF in VHDL, what about writing it in terms of gates?\n\n\n\n\n\nDec 8, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\n在 ios 下载第三方应用\n\n\n\n\n\n\nAwesome-Mac\n\n\n\n会闪退, 散了吧(´･_･`)\n\n\n\n\n\nOct 29, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares as Projection 最小二乘法的投影解释\n\n\n\n\n\n\nAlgebra\n\n\n\nThinking least squares in this way really helps!\n\n\n\n\n\nSep 21, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\njava项目导入Launchpad方案 MacOS\n\n\n\n\n\n\nAwesome-Mac\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nCreate your own website using Hugo on MacOS\n\n\n\n\n\n\nCS\n\n\n\nThis is how I built my first personal website without any prior knowledge of HTML.\n\n\n\n\n\nAug 31, 2024\n\n\nMarcobisky\n\n\n\n\n\n\n\n\n\n\n\n\nRTL Analysis on MacOS under 300MB\n\n\n\n\n\n\nAwesome-Mac\n\n\n\nWanna compile verilog on MacOS but without programing an FPGA? Check this out!\n\n\n\n\n\nAug 31, 2024\n\n\nMarcobisky\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jinming Ren",
    "section": "",
    "text": "Hi! I’m an third-year undergraduate student at University of Electronic Science and Technology of China (UESTC) and University of Glasgow (UofG). I majored in electronic and computer engineering (ECE). I’m a math enthusiast and classical music lover.\nI’m looking for a PhD position in machine learning, computer science or wireless communication in 2026, I love innovation and challenges! See my CV here.\nOn this site I also keep a partial list of my blogs, which are actively updating.\n\n\n\n\n📧 Email: 3191293752@qq.com | marcobisky@outlook.com\n\n📞 Phone: +86 17882004164"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jinming Ren",
    "section": "",
    "text": "Hi! I’m an third-year undergraduate student at University of Electronic Science and Technology of China (UESTC) and University of Glasgow (UofG). I majored in electronic and computer engineering (ECE). I’m a math enthusiast and classical music lover.\nI’m looking for a PhD position in machine learning, computer science or wireless communication in 2026, I love innovation and challenges! See my CV here.\nOn this site I also keep a partial list of my blogs, which are actively updating."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jinming Ren",
    "section": "",
    "text": "📧 Email: 3191293752@qq.com | marcobisky@outlook.com\n\n📞 Phone: +86 17882004164"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html",
    "href": "posts/digital-analysis-mac/index.html",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "",
    "text": "CLAIM: This article synthesizes conclusions and methods from multiple websites and is not pure original. 本文综合了多个网站的结论和方法，并非原创。\nYou may find these websites useful:\n\nWorkflow for FPGA development\nSome open-source tools\nIceStudio for M2 Mac\nFPGA Toolchain\nVHDL compile chain on MacOS\n\n\n\n\n\nYou need these:\n\ngtkwave: main RTL wave creator\nIcarus Verilog, and this manual\nYoSYS: .v to .json\nGHDL: compile, link and simulation tool for VHDL\nnetlistsvg: .json to .svg\n\nNot nessasarily required:\n\nPulseview: Logic Analyzer (Not required though)\nDigital: Visual simulation\n\nUseful vscode plugins:\n\nVerilog Support: vscode verilog language highlighter\nWavetrace: A nice vscode plugin to replace gtkwave"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html#intro-and-overview",
    "href": "posts/digital-analysis-mac/index.html#intro-and-overview",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "",
    "text": "CLAIM: This article synthesizes conclusions and methods from multiple websites and is not pure original. 本文综合了多个网站的结论和方法，并非原创。\nYou may find these websites useful:\n\nWorkflow for FPGA development\nSome open-source tools\nIceStudio for M2 Mac\nFPGA Toolchain\nVHDL compile chain on MacOS\n\n\n\n\n\nYou need these:\n\ngtkwave: main RTL wave creator\nIcarus Verilog, and this manual\nYoSYS: .v to .json\nGHDL: compile, link and simulation tool for VHDL\nnetlistsvg: .json to .svg\n\nNot nessasarily required:\n\nPulseview: Logic Analyzer (Not required though)\nDigital: Visual simulation\n\nUseful vscode plugins:\n\nVerilog Support: vscode verilog language highlighter\nWavetrace: A nice vscode plugin to replace gtkwave"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html#first-project-in-verilog",
    "href": "posts/digital-analysis-mac/index.html#first-project-in-verilog",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "2 First Project in Verilog",
    "text": "2 First Project in Verilog\n\n2.1 Install Icarus Verilog Compiler\nbrew install icarus-verilog\n\n\n2.2 Compilation and Simulation\n\nCreate new folder called Verilog, then create two test files named GatedDLatch.v and GatedDLatch_tb.v. The former is the description file of the circuit, the latter is for testbench. And write the following contents respectively:\n// in GatedDLatch.v\nmodule GatedDLatch (Data, WE, Out, OutBar);\n\n    input Data;\n    input WE;\n\n    output Out;\n    output OutBar;\n\n    // component name(output, input1, input2)\n    wire S;\n    wire R;\n    wire Dbar;\n    nand g1(S, Data, WE);\n    not g2(Dbar, Data);\n    nand g3(R, WE, Dbar);\n    nand g4(Out, S, OutBar);\n    nand g5(OutBar, R, Out);\n    \n\nendmodule\nand\n// in GatedDLatch_tb.v\n`timescale 1ns / 1ns // simulation time, time precision = 1ns\n//Import the main code into the testbench\n`include \"GatedDLatch.v\"\n\nmodule GatedDLatch_tb;\n//Inputs as registers\nreg Data;\nreg WE;\n\n//Outputs as wires\nwire Out;\nwire OutBar;\n\n//Initialisation\nGatedDLatch uut(Data, WE, Out, OutBar);\n\ninitial begin\n    //Name of the graph file that gets generated after we run\n    $dumpfile(\"GatedDLatch_tb.vcd\");\n    $dumpvars(0,GatedDLatch_tb);\n\n    Data = 0;\n    WE = 0;\n    #10;\n\n    Data = 1;\n    #4;\n    WE = 1;\n    #2;\n    WE = 0;\n    #4;\n\n    Data = 0;\n    #4;\n    WE = 1;\n    #2;\n    WE = 0;\n    #4;\n\n    $display(\"Test complete\");\nend\nendmodule\n\nRun this in the terminal:\niverilog -o GatedDLatch_tb.vvp GatedDLatch_tb.v\n\nUse vvp command to convert the binary temperary file GatedDLatch_tb.vvp to GatedDLatch_tb.vcd waveform file:\nvvp GatedDLatch_tb.vvp\n\nInstall Wavetrace in vscode to view the waveform:\n\n\n\nDisplay the waveform in vscode\n\n\n\nYou can also install gtkwave to view the waveform.\nbrew install gtkwave\ngtkwave GatedDLatch_tb.vcd\n\n\n\nBefore imported\n\n\n\nExpand GatedDLatch_tb list to display the waveform:\n\n\n\nDisplaying waveform\n\n\n\n\n2.3 Synthesis and Visualization\n\nWe can also visualize the circuit topology (called generating schematics). First, use YoSYS to convert the verilog code into gate-level netlist. Of course you should install the command line tool YoSYS:\nbrew install yosys\nyosys -V # Verify Yosys installation\n\nYoSYS will first convert the circuit structure description file GatedDLatch.v into a json file:\nyosys -p \"prep -top GatedDLatch; write_json GatedDLatch.json\" GatedDLatch.v\n\nThen we install another tool called netlistsvg:\n# Install Node.js (if not already installed)\nbrew install node\n\n# Install netlistsvg globally using npm\nnpm install -g netlistsvg\n\n# Verify netlistsvg installation\nnetlistsvg --version\n\nUsing the netlistsvg tool to convert GatedDLatch.json to GatedDLatch.svg:\nnetlistsvg GatedDLatch.json -o GatedDLatch.svg\n\nPreviewing GatedDLatch.svg will give you the circuit schematic:\n\n\n\nGatedDLatch.svg\n\n\n\n\n2.4 Makefile Work Flow\n\nThe entire workflow can be divided into two major independent parts:\n\n\nCompilation and Simulation:\n\niverilog (Compilation)\nvvp (Simulation)\n\nSynthesis and Circuit Structure Visualization:\n\nyosys (Synthesis)\nnetlistsvg (Visualization)\n\n\n\nWe use a Makefile to automate this process (ensure that Make and related components are installed): Create a Makefile file in the previously created Verilog folder and add the following content:\n# Description: Makefile for GatedDLatch\nCIRCUIT_STRUCT = GatedDLatch\n\n# Directories\nBUILD_DIR = build\n\n# Ensure the build directory exists\n$(BUILD_DIR):\n    mkdir -p $(BUILD_DIR)\n\n# Compilation: iverilog compilation\niverilog: $(CIRCUIT_STRUCT).v $(CIRCUIT_STRUCT)_tb.v | $(BUILD_DIR)\n    iverilog -o $(BUILD_DIR)/$(CIRCUIT_STRUCT)_tb.vvp $(CIRCUIT_STRUCT)_tb.v\n\n# Simulation: generate waveform (.vcd)\nvvp: $(BUILD_DIR)/$(CIRCUIT_STRUCT)_tb.vvp\n    vvp $(BUILD_DIR)/$(CIRCUIT_STRUCT)_tb.vvp\n\n# Synthesis: generate circuit structure configuration file (.json)\nYOSYS: $(CIRCUIT_STRUCT).v | $(BUILD_DIR)\n    yosys -p \"prep -top $(CIRCUIT_STRUCT); write_json $(BUILD_DIR)/$(CIRCUIT_STRUCT).json\" $(CIRCUIT_STRUCT).v\n\n# Visualization: generate human readable (.svg) from .json\nNETLISTSVG: $(BUILD_DIR)/$(CIRCUIT_STRUCT).json | $(BUILD_DIR)\n    netlistsvg $(BUILD_DIR)/$(CIRCUIT_STRUCT).json -o $(BUILD_DIR)/$(CIRCUIT_STRUCT).svg\n\n# Schematic diagram only: Synthesis then Visualization\nschematic: YOSYS NETLISTSVG\n\n# Run all steps\nrun_all: iverilog vvp schematic\n\n# Clean build directory\nclean:\n    rm -rf $(BUILD_DIR)\n    rm -f $(CIRCUIT_STRUCT)_tb.vcd\n\nAfter modifying the files, simply execute:\nmake clean\nmake run_all\n\nThis will generate all the relevant files:\n\n\n\nProject File Structure"
  },
  {
    "objectID": "posts/digital-analysis-mac/index.html#first-project-in-vhdl",
    "href": "posts/digital-analysis-mac/index.html#first-project-in-vhdl",
    "title": "RTL Analysis on MacOS under 300MB",
    "section": "3 First Project in VHDL",
    "text": "3 First Project in VHDL\n\n3.1 Install GHDL Compiler\n\nSimilar to Verilog, VHDL is also a hardware description language. Compiling it requires another tool: GHDL. Installing it on macOS can be tricky. The following steps have been tested on an M2 Mac (as of 2024-09-01):\n\nInstall vhdl using brew:\nbrew install vhdl\n\nvhdl has two versions: LLVM and mcode. The LLVM version has some issues on macOS, and the brew-installed version uses LLVM, so we manually download the mcode version from here. I downloaded ghdl-macos-11-mcode.tgz.\nExtract it by double-clicking, and you will get three files:\n\n\n\nGHDL-mcode package\n\n\n\nCopy and paste these three files to the following path: /opt/homebrew/Caskroom/ghdl/4.1.0:\n\n\n\nCopy and Paste\n\n\nIn the terminal, type:\nghdl --version\n\nIf you encounter security prompts, go to System Settings &gt; Privacy & Security to allow access:\n\n\n\nAllow Access in Privacy & Security\n\n\n\n\n3.2 Compilation, Linking and Simulation\n\nUnlike Verilog, VHDL requires an additional Linking step, which connects the component declarations with their implementation files (testbench). Why doesn’t Verilog require this? Because the testbench file in Verilog includes the declaration contents (include \"GatedDLatch.v\"), so it links automatically.\n\nIn Verilog, we used two separate tools (iverilog and vvp) for compilation and simulation. However, for VHDL, we only need one tool: GHDL.\n\nCreate a new folder VHDLDemo, and within it, create two files: demo.vhdl and demo_tb.vhdl. The former describes the circuit structure, and the latter serves as the testbench file (you can also use .vhd as the suffix). Add the following content to the respective files:\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity demo is\n    port (\n        A : in  STD_LOGIC;\n        B : in  STD_LOGIC;\n        O : out STD_LOGIC\n    );\nend demo;\n\narchitecture Behavioral of demo is\nbegin\n    O &lt;= not (A and B); -- NAND gate\nend Behavioral;\nand\nlibrary IEEE;\nuse IEEE.STD_LOGIC_1164.ALL;\n\nentity demo_tb is\nend demo_tb;\n\narchitecture Behavioral of demo_tb is\n    signal A : STD_LOGIC := '0';\n    signal B : STD_LOGIC := '0';\n    signal O : STD_LOGIC;\n\n    -- Instantiate the unit under test (UUT)\n    component demo\n        port (\n            A : in  STD_LOGIC;\n            B : in  STD_LOGIC;\n            O : out STD_LOGIC\n        );\n    end component;\nbegin\n    UUT: demo port map (\n        A =&gt; A,\n        B =&gt; B,\n        O =&gt; O\n    );\n\n    -- Test process\n    process\n    begin\n        -- Test case 1: A = 0, B = 0\n        A &lt;= '0';\n        B &lt;= '0';\n        wait for 10 ns;\n        \n        -- Test case 2: A = 0, B = 1\n        A &lt;= '0';\n        B &lt;= '1';\n        wait for 10 ns;\n\n        -- Test case 3: A = 1, B = 0\n        A &lt;= '1';\n        B &lt;= '0';\n        wait for 10 ns;\n\n        -- Test case 4: A = 1, B = 1\n        A &lt;= '1';\n        B &lt;= '1';\n        wait for 10 ns;\n\n        -- End of simulation\n        wait;\n    end process;\nend Behavioral;\n\nCreate a Makefile to automate the process:\n# Description: Makefile for VHDLDemo\nCIRCUIT = demo\nTB = demo_tb\nBUILD_DIR = build\n\n# Ensure the build directory exists\n$(BUILD_DIR):\n    mkdir -p $(BUILD_DIR)\n\n# Compilation: compile the design and testbench\nghdl_compile: $(BUILD_DIR)\n    ghdl -a --workdir=$(BUILD_DIR) $(CIRCUIT).vhdl\n    ghdl -a --workdir=$(BUILD_DIR) $(TB).vhdl\n\n# Linking: Elaborate the design and testbench\nghdl_elab: ghdl_compile\n    ghdl -e --workdir=$(BUILD_DIR) $(TB)\n\n# Simulation: simulate the testbench\nghdl_simulate: ghdl_elab\n    ghdl -r --workdir=$(BUILD_DIR) $(TB) --vcd=$(BUILD_DIR)/$(TB).vcd\n\n### These cannot work for now##############################################\n# # Synthesis: generate circuit structure configuration file (.json), you should have ghdl plugin installed for yosys, but I have error: \"ERROR: No such command: ghdl\" or \"dyld[5264]: missing symbol called\", possible solution could be to install yosys from source, but not sure\n# YOSYS: $(CIRCUIT).vhdl | $(BUILD_DIR)\n#   yosys -p \"ghdl $(CIRCUIT); prep -top $(CIRCUIT); write_json -compat-int $(BUILD_DIR)/$(CIRCUIT).json\" $(CIRCUIT).vhdl\n\n# # Visualization: generate human readable (.svg) from .json\n# NETLISTSVG: $(BUILD_DIR)/$(CIRCUIT).json  | $(BUILD_DIR)\n#   netlistsvg $(BUILD_DIR)/$(CIRCUIT).json -o $(BUILD_DIR)/$(CIRCUIT).svg\n\n# # Schematic diagram only: Synthesis then Visualization\n# schematic: YOSYS NETLISTSVG\n\n# # Run all steps\n# run_all: ghdl_compile ghdl_elab ghdl_simulate YOSYS NETLISTSVG\n### These cannot work for now##############################################\n\n# Run compilation, linking and simulation\nrun_cls: ghdl_compile ghdl_elab ghdl_simulate\n\n# Clean build directory\nclean:\n    rm -rf $(BUILD_DIR)\n    rm -f $(TB)\n\n# Experiment: Run testbench without the design file\nrun_tb_only: $(BUILD_DIR)\n    ghdl -a --workdir=$(BUILD_DIR) $(TB).vhdl\n    ghdl -r --workdir=$(BUILD_DIR) $(TB) --vcd=$(BUILD_DIR)/$(TB)_no_design.vcd\n\nThe running process is very clear. In the command line, execute:\nmake clean\nmake run_cls\n\nThis will complete the process.\n\nIf you want to see what happens if you skip compiling and linking the design file (demo.vhdl), run:\nmake clean # Cannot omitted!\nmake run_tb_only\n\nYou can compare the two .vcd files generated in VSCode or using GTKWave; they are different as expected.\n\n\n3.3 No Synthesis and Visualization Plan\n\nNote that this Makefile does not include steps to generate a schematic diagram because yosys requires a ghdl plugin. Currently, the integration is not very stable. You can refer to ghdl-yosys-plugin and building-ghdl for more details. However, the suggested methods have been tested on M2 Mac and result in errors:\n /----------------------------------------------------------------------------\\\n |  yosys -- Yosys Open SYnthesis Suite                                       |\n |  Copyright (C) 2012 - 2024  Claire Xenia Wolf &lt;claire@yosyshq.com&gt;         |\n |  Distributed under an ISC-like license, type \"license\" to see terms        |\n \\----------------------------------------------------------------------------/\n Yosys 0.44 (git sha1 80ba43d26, clang++ 15.0.0 -fPIC -O3)\n\n-- Running command `ghdl demo_tb.vhdl -e demo_tb; prep -top demo_tb; write_json demo.json' --\n\n1. Executing GHDL.\ndyld[5264]: missing symbol called\nzsh: abort      yosys -m ghdl -p\n\nThis issue has also been mentioned in the Issues section of ghdl-yosys-plugin, but there is no solution yet.\n\nPossible solutions might include compiling and installing yosys from source, ensuring the correct versions of yosys and ghdl, or checking if any component of the FPGA Toolchain is missing. Alternatively, you could try converting VHDL to Verilog using some tool (like GPT) and then synthesizing the schematics."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html",
    "href": "posts/inverted-pendulum/index.html",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "",
    "text": "Control block diagram\n\n\nThe objective of this article is to provide a self-contained guide to building a linear feedback control system for the classical inverted pendulum problem in MATLAB, i.e., to design a controller by applying a force to the cart \\(M\\) to balance the pendulum at the upright position (shown in Figure 1). We will use the following standard equations of a control system: \\[\n\\begin{aligned}\n    \\dot{\\mathbf{x}} &= A \\mathbf{x} + B \\mathbf{u} \\\\\n    \\mathbf{y} &= C \\mathbf{x} + D \\mathbf{u} \\\\\n    \\mathbf{u} &= -K \\mathbf{y}.\n\\end{aligned}\n\\]\nA brief description of the symbols and their physical interpretation is given in the following table. If you have difficulty understanding their meanings, don’t worry, feel free to scan the next section.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nThis Article\n\n\n\n\n\\(\\mathbf{x} \\in TM\\)\nSystem state vector in the tangent bundle of dimension \\(n\\).\n\\(\\mathbf{x} = [x, \\dot{x}, \\theta, \\dot{\\theta}]^t \\in T(\\mathbb{R}^1 \\times \\mathbb{S}^2)\\)\n\n\n\\(\\mathbf{u} \\in TM_c\\)\nThe values control knobs (“actuators”) in a space of dimension \\(q\\) (\\(q &lt; n\\)).\nForce on the cart \\(\\mathbf{u} = \\mathbf{F} \\in \\mathbb{R}\\).\n\n\n\\(\\mathbf{y} \\in TM_o\\)\nThe observable output vector of the system (the values that we can measure) in a space of dimension \\(p\\) (\\(p &lt; n\\)).\nAll dimension of \\(\\mathbf{x}\\) is observable \\(\\mathbf{y} = \\mathbf{x}\\).\n\n\n\\(A^{n \\times n}\\)\nThe linear infinitesimal generator of the system.\nModelling and linearization needed, see Equation 5.\n\n\n\\(B^{n \\times q}\\)\nHow the control knobs affect the state vector.\nForce analyzing needed, see Equation 6.\n\n\n\\(C^{p \\times n}\\)\nConvert the state vector to what we can actually measure.\n\\(C = I\\).\n\n\n\\(D^{p \\times q}\\)\nSometimes the control knobs affect the system observable output directly.\nApplying the force has no direct effect on the observable \\(D = 0\\).\n\n\n\\(K^{q \\times p}\\)\nThe linear feedback matrix. The observable \\(\\mathbf{y}\\) is mapped linearly to the control knobs.\nWe use LQR to optimize this matrix."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#intro",
    "href": "posts/inverted-pendulum/index.html#intro",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "",
    "text": "Control block diagram\n\n\nThe objective of this article is to provide a self-contained guide to building a linear feedback control system for the classical inverted pendulum problem in MATLAB, i.e., to design a controller by applying a force to the cart \\(M\\) to balance the pendulum at the upright position (shown in Figure 1). We will use the following standard equations of a control system: \\[\n\\begin{aligned}\n    \\dot{\\mathbf{x}} &= A \\mathbf{x} + B \\mathbf{u} \\\\\n    \\mathbf{y} &= C \\mathbf{x} + D \\mathbf{u} \\\\\n    \\mathbf{u} &= -K \\mathbf{y}.\n\\end{aligned}\n\\]\nA brief description of the symbols and their physical interpretation is given in the following table. If you have difficulty understanding their meanings, don’t worry, feel free to scan the next section.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nThis Article\n\n\n\n\n\\(\\mathbf{x} \\in TM\\)\nSystem state vector in the tangent bundle of dimension \\(n\\).\n\\(\\mathbf{x} = [x, \\dot{x}, \\theta, \\dot{\\theta}]^t \\in T(\\mathbb{R}^1 \\times \\mathbb{S}^2)\\)\n\n\n\\(\\mathbf{u} \\in TM_c\\)\nThe values control knobs (“actuators”) in a space of dimension \\(q\\) (\\(q &lt; n\\)).\nForce on the cart \\(\\mathbf{u} = \\mathbf{F} \\in \\mathbb{R}\\).\n\n\n\\(\\mathbf{y} \\in TM_o\\)\nThe observable output vector of the system (the values that we can measure) in a space of dimension \\(p\\) (\\(p &lt; n\\)).\nAll dimension of \\(\\mathbf{x}\\) is observable \\(\\mathbf{y} = \\mathbf{x}\\).\n\n\n\\(A^{n \\times n}\\)\nThe linear infinitesimal generator of the system.\nModelling and linearization needed, see Equation 5.\n\n\n\\(B^{n \\times q}\\)\nHow the control knobs affect the state vector.\nForce analyzing needed, see Equation 6.\n\n\n\\(C^{p \\times n}\\)\nConvert the state vector to what we can actually measure.\n\\(C = I\\).\n\n\n\\(D^{p \\times q}\\)\nSometimes the control knobs affect the system observable output directly.\nApplying the force has no direct effect on the observable \\(D = 0\\).\n\n\n\\(K^{q \\times p}\\)\nThe linear feedback matrix. The observable \\(\\mathbf{y}\\) is mapped linearly to the control knobs.\nWe use LQR to optimize this matrix."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#physics-model",
    "href": "posts/inverted-pendulum/index.html#physics-model",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "2 Physics Model",
    "text": "2 Physics Model\nIn this section, we will derive the equations of motion for the cart-pendulum system using Lagrangian Mechanics. I hope Figure 1 will be enough to explain the notations used in this article.\n\n\n\n\n\n\nFigure 1: Cart-pendulum Model\n\n\n\nThe dynamics of the system can be computed using Lagrangian Mechanics:\n\\[\n\\boxed{\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\partial L}{\\partial q_i} - \\frac{\\partial L}{\\partial q_i} = Q_i^{\\text{non-conservative}},\n}\n\\tag{1}\\] where \\(q_i\\) is either \\(x\\) or \\(\\theta\\) in our case. The lagrangian \\(L\\) is system kinetic energy minus potential energy: \\[\nL = T - V.\n\\]\n\n2.1 Kinetic Energy\nThe total kinetic energy of the system is the sum of the kinetic energy of the cart and the kinetic energy of the pendulum: \\[\nT = T_M + T_m.\n\\]\nFirst we compute the position of \\(m\\): \\[\n\\mathbf{r}_m = \\begin{bmatrix} x + \\ell \\sin \\theta \\\\ -\\ell \\cos \\theta \\end{bmatrix}.\n\\]\nTherefore, \\[\n\\begin{aligned}\n    T_m &= \\frac{1}{2} m \\dot{\\mathbf{r}}_m^T \\dot{\\mathbf{r}}_m \\\\\n    &= \\frac{1}{2} m \\left[(\\dot{x} + \\ell \\dot{\\theta} \\cos \\theta)^2 + (\\ell \\dot{\\theta} \\sin \\theta)^2\\right] \\\\\n    &= \\frac{1}{2} m \\left[ \\dot{x}^2 + \\ell^2 \\dot{\\theta}^2 + 2 \\ell \\dot{x} \\dot{\\theta} \\cos \\theta \\right].\n\\end{aligned}\n\\]\nThe kinetic energy of \\(M\\) is easy: \\[\nT_M = \\frac{1}{2} M \\dot{x}^2.\n\\]\nThe total kinetic energy is: \\[\nT = \\frac{1}{2} (M + m) \\dot{x}^2 + \\frac{1}{2} m \\ell^2 \\dot{\\theta}^2 + m \\ell \\dot{x} \\dot{\\theta} \\cos \\theta.\n\\]\n\n\n2.2 Potential Energy\nOnly the pendulum has potential energy: \\[\nV = - m g \\ell \\cos \\theta.\n\\]\n\n\n2.3 Euler-Lagrange Equation\nThe lagrangian of the system is: \\[\n\\begin{aligned}\n    L &= T - V \\\\\n    &= \\frac{1}{2} (M + m) \\dot{x}^2 + \\frac{1}{2} m \\ell^2 \\dot{\\theta}^2 + m \\ell \\dot{x} \\dot{\\theta} \\cos \\theta + m g \\ell \\cos \\theta.\n\\end{aligned}\n\\]\n\n2.3.1 Euler-Lagrange in \\(x\\)\nFor \\(q_1 = x\\): \\[\n\\begin{aligned}\n    \\frac{\\partial L}{\\partial \\dot{x}} &= (M + m) \\dot{x} + m \\ell \\dot{\\theta} \\cos \\theta \\\\\n    \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\frac{\\partial L}{\\partial \\dot{x}} \\right) &= (M + m) \\ddot{x} + m \\ell (-\\sin \\theta \\dot{\\theta}^2 + \\cos \\theta \\ddot{\\theta}) \\\\\n    \\frac{\\partial L}{\\partial x} &= 0 \\\\\n    Q_x^{\\text{non-conservative}} &= F - b \\dot{x}.\n\\end{aligned}\n\\]\nBy Equation 1, we have: \\[\n(M + m) \\ddot{x} + m \\ell (-\\sin \\theta \\dot{\\theta}^2 + \\cos \\theta \\ddot{\\theta}) = F - b \\dot{x}.\n\\tag{2}\\]\n\n\n2.3.2 Euler-Lagrange in \\(\\theta\\)\nFor \\(q_2 = \\theta\\): \\[\n\\begin{aligned}\n    \\frac{\\partial L}{\\partial \\dot{\\theta}} &= m \\ell^2 \\dot{\\theta} + m \\ell \\dot{x} \\cos \\theta \\\\\n    \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\frac{\\partial L}{\\partial \\dot{\\theta}} \\right) &= m \\ell^2 \\ddot{\\theta} + m \\ell (\\ddot{x} \\cos \\theta - \\dot{x} \\sin \\theta \\dot{\\theta}) \\\\\n    \\frac{\\partial L}{\\partial \\theta} &= - m g \\ell \\sin \\theta - m \\ell \\dot{x} \\sin \\theta \\dot{\\theta} \\\\\n    Q_{\\theta}^{\\text{non-conservative}} &= 0.\n\\end{aligned}\n\\]\nBy Equation 1, we have: \\[\nm \\ell \\ddot{x} \\cos \\theta + m \\ell^2 \\ddot{\\theta} + m g \\ell \\sin \\theta = 0.\n\\tag{3}\\]\n\n\n2.3.3 Matrix Form\nWrite Equation 2 and Equation 3 in matrix form: \\[\n\\begin{bmatrix}\n    M + m & m \\ell \\cos \\theta \\\\\n    m \\ell \\cos \\theta & m \\ell^2\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\ddot{x} \\\\ \\ddot{\\theta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    F - b \\dot{x} + m \\ell \\sin \\theta \\dot{\\theta}^2 \\\\\n    - m g \\ell \\sin \\theta\n\\end{bmatrix}.\n\\tag{4}\\]\n\n\n\n2.4 Matlab Code\nThe following function computes the state-space model of the inverted pendulum:\n\n\ninvpend.m\n\n% x -&gt; state vector (x, xdot, theta, thetadot)\n% m -&gt; mass of pendulum\n% M -&gt; mass of cart\n% L -&gt; length of pendulum\n% g -&gt; gravity\n% b -&gt; friction = -b*xdot\n% F -&gt; force applied to cart\n% dx -&gt; derivative of state vector\n\nfunction dx = invpend(x, m, M, L, g, b, F)\n    % Unpack state variables\n    x1 = x(1);     % cart position\n    x2 = x(2);     % cart velocity (xdot)\n    x3 = x(3);     % pendulum angle (theta)\n    x4 = x(4);     % angular velocity (thetadot)\n    \n    % Precompute useful terms\n    sin_theta = sin(x3);\n    cos_theta = cos(x3);\n    theta_dot = x4;\n    \n    % Mass matrix\n    D = [M + m,         m * L * cos_theta;\n         m * L * cos_theta,   m * L^2];\n\n    % Right-hand side (forces/accelerations)\n    RHS = [F - b * x2 + m * L * sin_theta * theta_dot^2;\n           -m * g * L * sin_theta];\n\n    % Solve for accelerations\n    accel = D \\ RHS;  % Equivalent to inv(D) * RHS, but more stable\n    \n    % Return time derivative of state vector\n    dx = zeros(4, 1);\n    dx(1) = x2;         % xdot\n    dx(2) = accel(1);   % xddot\n    dx(3) = x4;         % thetadot\n    dx(4) = accel(2);   % thetaddot\nend\n\nThe following function simulates the motion of the system without damping and external force:\n\n\nsimulation_invpend.m\n\nclear all; close all; clc;\n\n% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 4;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 30;          % damping coefficient\ntime = 0:.1:10; % Time samples\n\n% Initial conditions\nx0 = [0; 0; .2; 0]; % x, xdot, theta, thetadot\n\n% Solve ODE\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, 0), time, x0);\n\n% Animation\nfor k = 1:length(t)\n    invpend_plot(x(k, :), L, true, 'simulation_invpend.gif');\nend\n\n\n\n\n\n\n\n\ninvpend_plot() function\n\n\n\n\n\nfunction invpend_plot(x, L, saveGif, gifFileName)\n    % Extract state variables\n    cart_x = x(1);    % Cart position\n    theta = x(3);     % Pendulum angle\n    \n    % Compute pendulum position\n    pend_x = cart_x + L * sin(theta);\n    pend_y = L * cos(theta);\n    \n    % Figure setup\n    clf; hold on; axis equal; grid on;\n    xlim([-2 2]); ylim([-1.5 1.5]);\n    \n    % Draw cart\n    cart_w = 0.4; cart_h = 0.2;\n    rectangle('Position', [cart_x - cart_w/2, -cart_h/2, cart_w, cart_h], ...\n              'Curvature', 0.1, 'FaceColor', [0.5 0.5 0.5]);\n    \n    % Draw pendulum\n    plot([cart_x, pend_x], [0, pend_y], 'k-', 'LineWidth', 2); % Rod\n    plot(pend_x, pend_y, 'ro', 'MarkerSize', 10, 'MarkerFaceColor', 'r'); % Mass\n    \n    % Draw ground\n    plot([-2 2], [0, 0], 'k', 'LineWidth', 2);\n    \n    % If saveGif is true, save the current frame to a GIF file\n    persistent gifFile frameCount currentGifName\n    if nargin &lt; 3\n        saveGif = false;\n    end\n    \n    if saveGif\n        % Set default filename if not provided\n        if nargin &lt; 4 || isempty(gifFileName)\n            gifFileName = 'invpend_animation.gif';\n        end\n        \n        % Reset frame count if this is a new gif file\n        if isempty(currentGifName) || ~strcmp(currentGifName, gifFileName)\n            frameCount = 1;\n            currentGifName = gifFileName;\n        else\n            frameCount = frameCount + 1;\n        end\n        \n        % Capture the current figure as an image\n        frame = getframe(gcf);\n        im = frame2im(frame);\n        [imind, cm] = rgb2ind(im, 256);\n        \n        % Write to the GIF file\n        if frameCount == 1\n            imwrite(imind, cm, gifFileName, 'gif', 'Loopcount', inf, 'DelayTime', 0.1);\n        else\n            imwrite(imind, cm, gifFileName, 'gif', 'WriteMode', 'append', 'DelayTime', 0.1);\n        end\n    end\n    \n    drawnow;\nend\n\n\n\n\n\n\n\nSimulation result of simulation_invpend.m"
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#linearized-phase-space-model",
    "href": "posts/inverted-pendulum/index.html#linearized-phase-space-model",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "3 Linearized Phase Space Model",
    "text": "3 Linearized Phase Space Model\n\n3.1 Linearization of the original dynamics\nIn this section, we build the linear version of the original system (without control) in the phase space.\nFirst of all, the state of the system can be described in a vector: \\[\n\\mathbf{x} = \\begin{bmatrix}\nx \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\n\\end{bmatrix}\n\\in T (\\mathbb{R}^1 \\times \\mathbb{S}^1)\n\\]\nNow we want to use a linear model \\(\\dot{\\mathbf{x}} = \\mathbf{Ax}\\) to analyze the system at \\(\\theta = 0\\), i.e., we want to find matrix \\(A \\in \\mathbb{C}^{4 \\times 4}\\) s.t., \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\n\\begin{bmatrix}\n    x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\n\\end{bmatrix}\n=\n\\overbrace{\n\\begin{bmatrix}\n    \\dot{x} \\\\ \\ddot{x} \\\\ \\dot{\\theta} \\\\ \\ddot{\\theta}\n\\end{bmatrix}\n}^{\\dot{\\mathbf{x}}}\n=\n\\overbrace{\n\\begin{bmatrix}\n    0 & 1 & 0 & 0 \\\\\n    ? & ? & ? & ? \\\\\n    0 & 0 & 0 & 1 \\\\\n    ? & ? & ? & ?\n\\end{bmatrix}\n}^{A}\n\\overbrace{\n\\begin{bmatrix}\n    x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\n\\end{bmatrix}\n}^{\\mathbf{x}}\n\\]\nNote the the first and third row are trivial identities, we only need to derive the second and fourth row of \\(A\\). We already solved Equation 4 in invpend.m. But now we have to solve it explicitly to derive the partial derivatives for the jacobian: \\[\n\\begin{bmatrix}\n    \\ddot{x} \\\\ \\ddot{\\theta}\n\\end{bmatrix}\n=\n\\phi (x, \\dot{x}, \\theta, \\dot{\\theta})\n=\n\\begin{bmatrix}\n    \\frac{F - b \\dot{x} + m \\ell \\sin \\theta \\dot{\\theta}^2 - mg \\cos \\theta \\sin \\theta}{M + m \\sin^2 \\theta} \\\\\n    \\frac{- \\cos \\theta \\left(F - b \\dot{x} + m \\ell \\sin \\theta \\dot{\\theta}^2\\right) + (M+m) g \\ell \\sin \\theta}{M \\ell + m \\ell \\sin^2 \\theta}\n\\end{bmatrix}.\n\\]\nNow obviously the function \\(\\phi\\) is non-linear. Its jacobian1 at phase point \\(\\mathbf{x}_{\\text{up}} = [x, 0, 0, 0]^t\\) is: \\[\n\\begin{aligned}\n    [J]\n    &=\n    \\begin{bmatrix}\n    \\frac{\\partial \\phi_1}{\\partial x} & \\frac{\\partial \\phi_1}{\\partial \\dot{x}} & \\frac{\\partial \\phi_1}{\\partial \\theta} & \\frac{\\partial \\phi_1}{\\partial \\dot{\\theta}} \\\\\n    \\frac{\\partial \\phi_2}{\\partial x} & \\frac{\\partial \\phi_2}{\\partial \\dot{x}} & \\frac{\\partial \\phi_2}{\\partial \\theta} & \\frac{\\partial \\phi_2}{\\partial \\dot{\\theta}}\n    \\end{bmatrix}_{\\mathbf{x} = \\mathbf{x}_{\\text{up}}} \\\\\n    &=\n    \\begin{bmatrix}\n    0 & -\\frac{b}{M} & -\\frac{mg}{M} & 0 \\\\\n    0 & -\\frac{b}{M \\ell} & -\\frac{(m + M)g}{M \\ell} & 0\n    \\end{bmatrix}.\n\\end{aligned}\n\\]\n1 Just to refresh, the Jacobian \\(J\\) of a (non-linear) function \\(\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at point \\(p \\in \\mathbb{R}^n\\) can be viewed as the local transformation from the neighborhood of \\(p\\) to the neighborhood of \\(\\phi(p)\\). \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\) are not viewed as vector spaces but manifolds. In our case it’s a little weird to compute the jacobian of \\(A\\) since \\(A\\) defines a vector field, not mapping between manifolds. Or it is? \\(A\\) is indeed a mapping between manifolds! Because vector field themselves are a section of the tangent bundle. By the way, this jacobian actually takes me a lot of effort to compute.Therefore, \\[\nA =\n\\begin{bmatrix}\n    0 & 1 & 0 & 0 \\\\\n    0 & -\\frac{b}{M} & -\\frac{mg}{M} & 0 \\\\\n    0 & 0 & 0 & 1 \\\\\n    0 & -\\frac{b}{M \\ell} & -\\frac{(m + M)g}{M \\ell} & 0\n\\end{bmatrix}.\n\\tag{5}\\]\n\n\n3.2 Deriving matrix \\(B\\) for control\nThe only control knob is \\(\\mathbf{u} = \\mathbf{F}\\), the force on the cart.\n\n\n\nThe control knob \\(\\mathbf{F}\\) affects \\(\\dot{x}\\) and \\(\\dot{\\theta}\\)\n\n\nBy Newton’s second law, it’s obvious that \\[\nB \\mathbf{u} =\n\\begin{bmatrix}\n    0 \\\\ \\frac{1}{M} \\\\ 0 \\\\ -\\frac{1}{M \\ell}\n\\end{bmatrix} F.\n\\tag{6}\\]\nEach entry of \\(B\\) in Equation 6 is explained in the following table:\n\n\n\n\n\n\n\nEntry\nExplanation\n\n\n\n\nB(1,1) = 0\nThe velocity \\(\\dot{x}\\) of the cart \\(M\\) will not suddenly change due to the force \\(F\\).\n\n\nB(2,1) = \\(\\frac{1}{M}\\)\nThe acceleration \\(\\ddot{x}\\) flash to \\(\\frac{F}{M}\\) due to \\(F\\).\n\n\nB(3,1) = 0\nThe angular velocity \\(\\dot{\\theta}\\) of the pendulum \\(m\\) will not suddenly change due to the force \\(F\\).\n\n\nB(4,1) = \\(-\\frac{1}{M \\ell}\\)\nThe angular acceleration \\(\\ddot{\\theta}\\) of the pendulum \\(m\\) flash to \\(-\\frac{F}{M \\ell}\\) due to \\(F\\). The pendulum is moving backward relative to the cart, hence the negative sign."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#controllability-of-the-system",
    "href": "posts/inverted-pendulum/index.html#controllability-of-the-system",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "4 Controllability of the system",
    "text": "4 Controllability of the system\n\n4.1 Original system stability\nFrom intuition, the system where \\(\\theta = 0\\) is unstable. How to know that from Equation 5? We know that the system \\(\\dot{\\mathbf{x}} = A \\mathbf{x}\\) is stable iff all the eigenvalues of \\(A\\) are rigorously negative. The instability of the system can be verified by the following Matlab code:\n\n\noriginal_system_stability.m\n\nclear all; close all; clc;\n\n% Simulation parameters\nm = 3;          % Mass of pendulum\nM = 5;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 5;          % damping coefficient\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\neig(A)\n\n\n\n\n\n\n\n\nResult of original_system_stability.m\n\n\n\n\n\nans =\n         0\n   -4.1883\n   -0.6157\n    3.8040\nSince there are two non-negative eigenvalues, the system is unstable.\n\n\n\n\n\n\n4.2 Controllability\nOne great thing about feedback is that we can change the fundamental dynamics of the system, changing its eigenvalues2 to make it stable (as shown in Figure 2).\n2 Also called poles for historical reasons.\n\n\n\n\n\nFigure 2: Feedback control change the underlying dynamics of the system\n\n\n\nThe system with linear feedback is: \\[\n\\begin{aligned}\n    \\dot{\\mathbf{x}} &= A \\mathbf{x} + B \\mathbf{u} \\\\\n    \\mathbf{u} &= -K \\mathbf{x},\n\\end{aligned}\n\\] or \\[\n\\dot{\\mathbf{x}} = (A - B K) \\mathbf{x},\n\\] whose dynamics could be very different from the original system \\(\\dot{\\mathbf{x}} = A \\mathbf{x}\\).\nWe want to take full control of the system, i.e., does there exist some \\(\\mathbf{F} = \\mathbf{u}(t)\\) to drive the system state point to anywhere in the phase space? In other words, is the system controllable?\n\n\n\n\n\n\n\nControllability\n\n\n\n\nTheorem 1 The following statements are equivalent:\n\nThe system is controllable.\nIts controllability matrix \\(\\mathcal{C}\\) is full rank, \\[\n\\mathcal{C} :=\n\\begin{bmatrix}\nB & AB & A^2 B & \\cdots & A^{n-1} B\n\\end{bmatrix}.\n\\]\nThe poles of the system can be placed arbitrarily3, i.e., the matrix \\(A - B K\\) could have arbitrary eigenvalues.\n\n\nThe reachability space \\(\\mathcal{R}\\) is the full phase space, \\[\n\\mathcal{R} := \\{\\mathbf{\\xi} \\in TM: \\exists \\text{ input } \\mathbf{u}(t) \\text{ s.t. } \\mathbf{x}(t) = \\mathbf{\\xi}\\}\n\\]\n\n\n\n\n\n3 In fact, there is a built-in Matlab function K = place(A, B, desired_eigs_vec) to help you place the poles of the system to any desired locations.By Theorem 1, the system is controllable iff ctrb(A, B) has rank \\(4\\). In fact, the following Matlab code can verify this.\n\n\nsys_controllability.m\n\nclear all; close all; clc;\n\n% Simulation parameters\nm = 3;          % Mass of pendulum\nM = 5;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 5;          % damping coefficient\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\nrank(ctrb(A, B))\n\n\n\n\n\n\n\n\nResult of sys_controllability.m\n\n\n\n\n\nans =\n     4\nSince \\(\\mathcal{C}=\\) ctrb(A, B) has full rank \\(4\\), the system \\(\\dot{\\mathbf{x}} = (A - B K) \\mathbf{x}\\) is controllable."
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#finding-the-feedback-matrix-k",
    "href": "posts/inverted-pendulum/index.html#finding-the-feedback-matrix-k",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "5 Finding the feedback matrix \\(K\\)",
    "text": "5 Finding the feedback matrix \\(K\\)\n\n5.1 Random pole placement\nFor controllable systems, Theorem 1 also guarantees that we can place the poles of the system to any desired locations, say desired_eigs_vec = [-3; -4; -5; -6], just randomly some numbers in the left half of the complex plane, to ensure the stability of the system. We use place() in Matlab to find such \\(K\\) and use that \\(K\\) to simulate the control effect of the system.\n\n\nsimulation_linear_control.m\n\nclear all; close all; clc;\n\n%% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 10;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 2;          % damping coefficient\ntime = 0:.1:6; % Time samples\n\n%% Initial conditions\nx0 = [0; 0; -.4; 0]; % x, xdot, theta, thetadot\n\n%% pole placement\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\ndesired_eigs_vec = [-3; -4; -5; -6];\nK = place(A, B, desired_eigs_vec);\n\n% Just to verify that the poles are where we want them\neig(A - B*K)\n\n%% Solve ODE\ndesired_state_vec = [1; 0; 0; 0];\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, -K * (x - desired_state_vec)), time, x0);\n\n%% Animation\nfor k = 1:length(t)\n    invpend_plot(x(k, :), L, true, 'simulation_linear_control.gif');\nend\n\n\n\n\n\n\n\nFigure 3: Simulation result of simulation_linear_control.m\n\n\n\nWe can see in Figure 3, the inverted pendulum is able to be stabilized at the status \\(\\mathbf{x} = [1, 0, 0, 0]^t\\).\nBy changing the desired eigenvalues, we can adjust the convergent speed of the system.\n\n\n\n\n\n\n\nMatlab code for changing the desired eigenvalues\n\n\n\n\n\nclear all; close all; clc;\n\n%% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 10;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 2;          % damping coefficient\ntime = 0:.1:12; % Time samples\n\n%% Initial conditions\nx0 = [0; 0; -.4; 0]; % x, xdot, theta, thetadot\n\n%% pole placement\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\ndesired_eigs_vec_1 = [-1; -2; -3; -4];\ndesired_eigs_vec_2 = [-2; -3; -4; -5];\ndesired_eigs_vec_3 = [-3; -4; -5; -6];\ndesired_eigs_vec_4 = [-4; -5; -6; -7];\n\nK = place(A, B, desired_eigs_vec_4);\n\n% Just to verify that the poles are where we want them\neig(A - B*K)\n\n%% Solve ODE\ndesired_state_vec = [1; 0; 0; 0];\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, -K * (x - desired_state_vec)), time, x0);\n\n%% Animation\nfor k = 1:length(t)\n    % invpend_plot(x(k, :), L, true, 'convergence_speed_4.gif');\n    invpend_plot(x(k, :), L, false);\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\ndesired_eigs_vec_1 = [-1; -2; -3; -4]\n\n\n\n\n\n\n\ndesired_eigs_vec_2 = [-2; -3; -4; -5]\n\n\n\n\n\n\n\n\n\ndesired_eigs_vec_3 = [-3; -4; -5; -6]\n\n\n\n\n\n\n\ndesired_eigs_vec_4 = [-4; -5; -6; -7]\n\n\n\n\n\nWe can see that more negative eigenvalues lead to faster convergence, at the risk of breaking the linearity limit and making the system less robust. So we need to make a trade-off between the convergence speed and the robustness (convergence speed is usually limited by the power of the control). Also, we may want to change the convergence style: whether to make the cart move as quickly as possible, or save as much energy as much as possible as long as the pendulum is inverted? Thankfully there is a way to find the required and “optimal” (in a sense) eigenvalues – the Linear Quadratic Regulator (LQR).\n\n\n5.2 Linear Quadratic Regulator (LQR)\nThe idea is to define a metric (“loss function” in optimization jargon) to measure the badness of the eigenvalues (either too less robust or cost a lot to control), and then minimize that metric. Someone came up with this weird Equation 7:\n\\[\n\\tilde{J} := \\int_0^\\infty (\\mathbf{x}^t Q \\mathbf{x} + \\mathbf{u}^t R \\mathbf{u}) \\ \\mathrm{d}t,\n\\tag{7}\\] where \\(Q^{n \\times n}\\) and \\(R^{q \\times q}\\) are symmetric, positive-definite matrices, which typically are diagonal. In our example, we could choose \\[\nQ =\n\\begin{bmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 5 & 0 \\\\\n    0 & 0 & 0 & 70\n\\end{bmatrix},\n\\quad\nR = 0.5,\n\\] which we put bigger penalties on \\(\\theta\\) and \\(\\dot{\\theta}\\) because we want them to converge to their expected value quickly. For the position \\(x\\) and velocity \\(\\dot{x}\\), we don’t care much, so we put a small penalty on them. Also we put a penalty on the control input \\(F\\) since our control device is able to provide large enough instantaneous force and we don’t care the expenditure of energy.\nTherefore, \\(\\tilde{J}\\) will be: \\[\n\\tilde{J} = \\int_0^\\infty (x^2 + \\dot{x}^2 + 5 \\theta^2 + 70 \\dot{\\theta}^2 + 0.5 F^2) \\  \\mathrm{d}t.\n\\]\nNow we are using Linear feedback to Regulate a system to minimize a Quadratic loss function, hence the name Linear Quadratic Regulator (LQR).\nThe solution to this LQR problem deserves a separate lecture, the solution is given by the Algebraic Riccati Equation. But in Matlab, there is a one-line command we can use to solve the corresponding \\(K\\) matrix:\nK = lqr(A, B, Q, R)\n\n\n5.3 Simulation by changing Q matrix\nWe simulate the dynamics result with different \\(Q\\) matrices and fixed \\(R = 0.01\\):\n\n\n\n\n\n\n\nMatlab code for comparing different Q matrices\n\n\n\n\n\nclear all; close all; clc;\n\n%% Simulation parameters\nm = 2;          % Mass of pendulum\nM = 10;         % Mass of cart\nL = 1;          % Length of pendulum\ng = -9.81;       % Gravity\nb = 2;          % damping coefficient\ntime = 0:.4:100; % Time samples\n\n%% Initial conditions\nx0 = [0; 0; -.4; 0]; % x, xdot, theta, thetadot\n\n%% LQR pole placement\n\n% Define matrix, xdot = Ax + Bu\nA = [0, 1,          0,              0;\n     0, -b/M,       -m*g/M,         0;\n     0, 0,          0,              1;\n     0, -b/(M*L),   -(M+m)*g/(M*L), 0];\n\nB = [0; 1/M; 0; -1/(M*L)];\n\n% Define penalty Q and R matrix\n% Desired: Move the cart to the desired position as fast as possible\n% Q = [400, 0,  0,  0;\n%      0, 50,  0,  0;\n%      0, 0,  1,  0;\n%      0, 0,  0,  1];\n\n% Desired: Move the cart as smooth as possible\nQ = [20, 0,  0,  0;\n     0, 400,  0,  0;\n     0, 0,  1,  0;\n     0, 0,  0,  1];\n\n% Desired: Don't change the angle so much\n% Q = [4, 0,  0,  0;\n%      0, 4,  0,  0;\n%      0, 0,  500,  0;\n%      0, 0,  0,  50];\n\n% Desired: Save energy\nR = .01;\n\nK = lqr(A, B, Q, R)\n\n% Just to verify the stability of the closed loop system\neig(A - B*K)\n\n%% Solve ODE\ndesired_state_vec = [1; 0; 0; 0];\n[t, x] = ode45(@(t, x) invpend(x, m, M, L, g, b, -K * (x - desired_state_vec)), time, x0);\n\n%% Animation\nfor k = 1:length(t)\n    invpend_plot(x(k, :), L, true, 'simulation_lqr_4.gif');\n    % invpend_plot(x(k, :), L, false);\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Q = \\operatorname{diag}(400, 50, 1, 1)\\): Fast lock in\n\n\n\n\n\n\n\n\\(Q = \\operatorname{diag}(20, 400, 1, 1)\\): Smooth cart movement\n\n\n\n\n\n\n\n\\(Q = \\operatorname{diag}(4, 4, 500, 50)\\): Keep straight up\n\n\n\n\n\n\n\n5.4 Simulation by changing R values\nWe simulate the dynamics result with fixed \\(Q - \\operatorname{diag} (20, 400, 1, 1)\\) matrix and different \\(R\\) values:\n\n\n\n\n\n\n\n\n\n\\(R = 0.001\\): More control power\n\n\n\n\n\n\n\n\n\n\\(R = 1.5\\): Median control power\n\n\n\n\n\n\n\n\n\n\\(R = 5\\): Least control power"
  },
  {
    "objectID": "posts/inverted-pendulum/index.html#references",
    "href": "posts/inverted-pendulum/index.html#references",
    "title": "Control Case Study: LQR for Inverted Pendulum!",
    "section": "References",
    "text": "References\n\n\nÅström, Karl Johan, and Richard M Murray. 2021. Feedback Systems. Princeton University Press.\n\n\nBrunton, Steven L, and Jose Nathan Kutz. 2019. Data-Driven Science and Engineering : Machine Learning, Dynamical Systems, and Control. Cambridge University Press.\n\n\nPowell, J David. 2012. Feedback Control of Dynamic Systems : International Version/Matlab & Simulink. Pearson Education Limited."
  },
  {
    "objectID": "posts/java-to-launchpad/index.html",
    "href": "posts/java-to-launchpad/index.html",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "",
    "text": "我想在launchpad上面启动一个 java 项目，但是application只支持启动 .app 文件，怎么办？"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#intro",
    "href": "posts/java-to-launchpad/index.html#intro",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "",
    "text": "我想在launchpad上面启动一个 java 项目，但是application只支持启动 .app 文件，怎么办？"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#解决方案",
    "href": "posts/java-to-launchpad/index.html#解决方案",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "2 解决方案",
    "text": "2 解决方案\nspotlight 搜索 Automator，打开 Automator，按 command+W 关闭弹出的窗口：\n\n在导航栏中再次打开 Automator，选择 Application：\n\n搜索栏中搜索 Run AppleScript，拖拽到右侧的空白区域:\n\n加入以下内容：\non run {input, parameters}\n    set p to POSIX path of (path to me)\n    do shell script \"java -jar \" & p & \"/Contents/Java/YOURJARFILE.jar\"\n \nend run\n记得替换 YOURJARFILE.jar 为你的 jar 文件名。但是此时 \"/Contents/Java/ 这个路径是不存在的，而且 YOURJARFILE.jar 也没有在这个路径下，所以以后我们需要创建这个路径。但是首先我们先保存(Command+S)这个文件为一个 .app 文件，路径为 /Application，文件名为你期待这个app的名字，如：YOURJARFILE.app。\n进入 /Application/YOURJARFILE.app，右键点击 Show Package Contents，在 Contents 文件夹下创建 java 文件夹，将你的 YOURJARFILE.jar 拷贝放入这个文件夹。\n\n然后你就可以通过 launchpad .app 文件启动你的 java 项目了。"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#改图标",
    "href": "posts/java-to-launchpad/index.html#改图标",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "3 改图标",
    "text": "3 改图标\n还是在 Contents 文件夹下，把这个 .icns 文件换成你自己的图标文件，比如可以把 png 文件转换成 icns 文件，然后替换这个文件(文件名不变)。\n\n然后 Refresh the Icon Cache (if necessary):\ntouch /path/to/YOURJARFILE.app\nkillall Dock\nFinish!"
  },
  {
    "objectID": "posts/java-to-launchpad/index.html#references",
    "href": "posts/java-to-launchpad/index.html#references",
    "title": "java项目导入Launchpad方案 MacOS",
    "section": "4 References",
    "text": "4 References\n\nHow to convert .jar to .app on Mac - a Java tutorial\nLaunching a jar file as an app on Mac (from the dock)"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html",
    "href": "posts/least-squares-as-projection/index.html",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "",
    "text": "The goal is to find the linear model \\(y = \\beta_0 + \\beta_1 x\\) such that the sum of squared errors between the predicted values and the actual data is minimized."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#introduction",
    "href": "posts/least-squares-as-projection/index.html#introduction",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "",
    "text": "The goal is to find the linear model \\(y = \\beta_0 + \\beta_1 x\\) such that the sum of squared errors between the predicted values and the actual data is minimized."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#linear-model",
    "href": "posts/least-squares-as-projection/index.html#linear-model",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "2 Linear Model",
    "text": "2 Linear Model\nThe form of the linear model is:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nwhere \\(y_i\\) is the observed value, \\(x_i\\) is the independent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\epsilon_i\\) is the error term.\nWe wish to find \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted values \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\) minimize the sum of squared errors between \\(\\hat{y}_i\\) and the observed values \\(y_i\\)."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#design-matrix-and-observation-vector",
    "href": "posts/least-squares-as-projection/index.html#design-matrix-and-observation-vector",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "3 Design Matrix and Observation Vector",
    "text": "3 Design Matrix and Observation Vector\nTo make the problem more convenient, we represent it using vectors and matrices.\n\n3.1 Design Matrix\nDefine the design matrix \\(\\mathbf{X}\\) as:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n\\]\nThe first column contains only 1s, representing the constant term \\(\\beta_0\\), and the second column contains the values of the independent variable \\(x_i\\).\n\n\n3.2 Observation Vector\nDefine the observation vector \\(\\mathbf{y}\\) as:\n\\[\n\\mathbf{y} = \\begin{bmatrix}\n2 \\\\\n3 \\\\\n5 \\\\\n7\n\\end{bmatrix}\n\\]\nThis vector contains all the observed values \\(y_i\\).\n\n\n3.3 Parameter Vector\nDefine the parameter vector \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\)."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#sum-of-squared-errors-objective-function",
    "href": "posts/least-squares-as-projection/index.html#sum-of-squared-errors-objective-function",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "4 Sum of Squared Errors Objective Function",
    "text": "4 Sum of Squared Errors Objective Function\nIn regression, our goal is to find the parameters \\(\\boldsymbol{\\beta}\\) such that the predicted values \\(\\hat{\\mathbf{y}} = \\mathbf{X} \\boldsymbol{\\beta}\\) are as close as possible to the observed values \\(\\mathbf{y}\\), by minimizing the sum of squared errors (SSE):\n\\[\nS(\\beta_0, \\beta_1) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n\\]"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#deriving-the-normal-equation",
    "href": "posts/least-squares-as-projection/index.html#deriving-the-normal-equation",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "5 Deriving the Normal Equation",
    "text": "5 Deriving the Normal Equation\nThe key idea of least squares is to find \\(\\boldsymbol{\\beta}\\) such that the residual \\(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\) is minimized. Geometrically, this means that the residual should be orthogonal to the column space of the design matrix \\(\\mathbf{X}\\), which leads to the normal equation:\n\\[\n\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}) = 0\n\\]\nExpanding this:\n\\[\n\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\n\\]\nThis is the normal equation, which can be solved to find the least squares estimate \\(\\hat{\\boldsymbol{\\beta}}\\)."
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#solving-the-normal-equation",
    "href": "posts/least-squares-as-projection/index.html#solving-the-normal-equation",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "6 Solving the Normal Equation",
    "text": "6 Solving the Normal Equation\nNow, let’s compute the parts of the normal equation.\n\n6.1 Compute \\(\\mathbf{X}^T \\mathbf{X}\\)\n\\[\n\\mathbf{X}^T \\mathbf{X} = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n= \\begin{bmatrix}\n4 & 10 \\\\\n10 & 30\n\\end{bmatrix}\n\\]\n\n\n6.2 Compute \\(\\mathbf{X}^T \\mathbf{y}\\)\n\\[\n\\mathbf{X}^T \\mathbf{y} = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\\n3 \\\\\n5 \\\\\n7\n\\end{bmatrix}\n= \\begin{bmatrix}\n17 \\\\\n50\n\\end{bmatrix}\n\\]\n\n\n6.3 Solve the Normal Equation\nNow we solve the normal equation:\n\\[\n\\begin{bmatrix}\n4 & 10 \\\\\n10 & 30\n\\end{bmatrix} \\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n17 \\\\\n50\n\\end{bmatrix}\n\\]\nTo solve this, we first compute the inverse of \\(\\mathbf{X}^T \\mathbf{X}\\):\n\\[\n(\\mathbf{X}^T \\mathbf{X})^{-1} = \\frac{1}{(4)(30) - (10)(10)} \\begin{bmatrix}\n30 & -10 \\\\\n-10 & 4\n\\end{bmatrix} = \\frac{1}{20} \\begin{bmatrix}\n30 & -10 \\\\\n-10 & 4\n\\end{bmatrix}\n\\]\nNext, we compute \\(\\hat{\\boldsymbol{\\beta}}\\):\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n\\]\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\frac{1}{20} \\begin{bmatrix}\n30 & -10 \\\\\n-10 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n17 \\\\\n50\n\\end{bmatrix}\n= \\frac{1}{20} \\begin{bmatrix}\n(30)(17) + (-10)(50) \\\\\n(-10)(17) + (4)(50)\n\\end{bmatrix}\n\\]\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\frac{1}{20} \\begin{bmatrix}\n510 - 500 \\\\\n-170 + 200\n\\end{bmatrix}\n= \\frac{1}{20} \\begin{bmatrix}\n10 \\\\\n30\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.5 \\\\\n1.5\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#least-squares-estimate",
    "href": "posts/least-squares-as-projection/index.html#least-squares-estimate",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "7 Least Squares Estimate",
    "text": "7 Least Squares Estimate\nBy solving the normal equation, we find \\(\\hat{\\beta}_0 = 0.5\\) and \\(\\hat{\\beta}_1 = 1.5\\). Thus, the regression model is:\n\\[\n\\hat{y} = 0.5 + 1.5x\n\\]"
  },
  {
    "objectID": "posts/least-squares-as-projection/index.html#conclusion",
    "href": "posts/least-squares-as-projection/index.html#conclusion",
    "title": "Least Squares as Projection 最小二乘法的投影解释",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nUsing the projection approach, we see that the least squares estimate is the projection of the observation vector \\(\\mathbf{y}\\) onto the space spanned by the columns of the design matrix \\(\\mathbf{X}\\). By solving the normal equation, we found the parameters \\(\\hat{\\beta}_0 = 0.5\\) and \\(\\hat{\\beta}_1 = 1.5\\), which minimize the sum of squared errors."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#problem-formulation",
    "href": "posts/pde-solve-laplace-equation/index.html#problem-formulation",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "1 Problem Formulation",
    "text": "1 Problem Formulation\nSuppose we have a rectangular sheet of steel sized \\(p \\times q\\). We somehow force the temperature on the boundary of the sheet to be some deterministic function \\(f, g, h, w\\) shown in Figure 1. When the system settles stablely, what is the temperature distribution \\(v(x, y)\\) inside the sheet?\n\n\n\n\n\n\nFigure 1: How to solve the temperature distribution given boundary condition?\n\n\n\nSince the steady heat distribution is governed by the Laplace’s equation. The problem is basically to:\n\nSolve the Laplace’s equation given a (Dirichlet) boundary condition1 (temperature functions):\\[ \\nabla^2 v = 0. \\]\n1 See here for different types of boundary conditions."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#existence-and-uniqueness-of-solutions",
    "href": "posts/pde-solve-laplace-equation/index.html#existence-and-uniqueness-of-solutions",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "2 Existence and uniqueness of solutions",
    "text": "2 Existence and uniqueness of solutions\nIt’s good habit to always check the existence and uniqueness of a mathematical object.\n\nExistence: Does the solution exist?\nIn this case, one can guess by life experience that the solution exists since no matter how we force the temperature on the boundary, there must be a temperature distribution inside the sheet.\nUniqueness: Does the temperature distribution settles to a unique solution?\nThis is not immediately obvious2. Could there exist two different stable temperature distributions? The answer is no.\n\n2 Not every PDE has a unique solution. For example, the uniqueness of the famous Navier-Stokes equation is still unsolved."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#linearity-simplifies-the-problem",
    "href": "posts/pde-solve-laplace-equation/index.html#linearity-simplifies-the-problem",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "3 Linearity simplifies the problem",
    "text": "3 Linearity simplifies the problem\nStare Figure 2 for a while, you will find we just need to solve the steady heat distribution \\(F(x, y)\\) with an easier boundary condition: \\[\n\\begin{align}\nF(0, y) &= 0, & y \\in [0, q] \\tag{BC1} \\\\\nF(0, y) &= 0, & y \\in [0, q] \\tag{BC2} \\\\\nF(x, 0) &= 0, & x \\in [0, p] \\tag{BC3} \\\\\nF(x, q) &= f(x), & x \\in [0, p] \\tag{BC4}\n\\end{align}\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 2: Linearity of Laplace’s equation simplifies the boundary condition\n\n\n\n\n\n\nSimulate the subproblem"
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#separation-of-variables-sov",
    "href": "posts/pde-solve-laplace-equation/index.html#separation-of-variables-sov",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "4 Separation of variables (SoV)",
    "text": "4 Separation of variables (SoV)\nSuppose \\(F(x,y)\\) could be written as: \\[\nF(x,y) = A(x)B(y)\n\\tag{2}\\]\n\n\n\n\n\n\n\nUniqueness legitimize seperation of variables!\n\n\n\n\nDefinition 1 Equation 2 is such a bold and unreasonable assumption. Why is that??\n\nIt literally doesn’t make any sense, until it indeed gives a solution!\n\nThe seperation of variables is just a guess. You will later find that if we continue reasoning using this assumption, we will eventually arrive at a solution that satisfies both the Laplace’s equation and the boundary condition.\nHere is the point: The uniqueness theorem guarantees that the solution is unique!, which means the solution we just got is the only solution, though we have no fluent logic to derive it! This is sort of like:\nImagine your primary school teacher ask you to solve the equation: \\[\n2x + 3 = 1.\n\\tag{3}\\]\nYou have no idea how to solve it, but you guess that \\(x\\) satisfies: \\[\nx + 1 = 0,\n\\] and you get \\(x = -1\\), which indeed satisfies Equation 3. By Fundamental theorem of algebra, Equation 3 has and only has one solution. Therefore, its solution is \\(x = -1\\). This is a valid reasoning!!\n\n\n\n\nThen we plugin Equation 2 into the Laplace’s equation: \\[\n\\begin{align*}\n&\\nabla^2 (AB) = 0 \\\\\n\\implies\\quad &\\frac{\\partial^2 (AB)}{\\partial x^2} + \\frac{\\partial^2 (AB)}{\\partial y^2} = 0 \\\\\n\\implies\\quad &A_{xx}(x)B(y) + A(x)B_{yy}(y) = 0 \\\\\n\\implies\\quad &\\underbrace{\\frac{A_{xx}(x)}{A(x)}}_{\\text{function of }x} = \\underbrace{-\\frac{B_{yy}(y)}{B(y)}}_{\\text{function of }y}, \\quad \\forall (x, y) \\in [0, p] \\times [0, q].\n\\end{align*}\n\\]\nNow here is a very important step, for all point in the rectangle, a function of \\(x\\) equals a function of \\(y\\), always. This means they must be constant functions as shown in Figure 3.\n\\[\n\\boxed{\n\\frac{A_{xx}(x)}{A(x)} = -\\frac{B_{yy}(y)}{B(y)} = \\text{const} \\equiv \\lambda.\n}\n\\tag{4}\\]\nIf we denote the constant as \\(\\lambda\\), this is actually problematic because \\(\\lambda\\) could be different numbers as long as \\(A_{xx}(x)/A(x)\\) and \\(-B_{yy}(y)/B(y)\\) are equal. We will later see that there are countably \\(\\lambda\\) values that are available and we shall use series to represent the solution.\n\n\n\n\n\n\nFigure 3: A function of \\(x\\) equals a function of \\(y\\) pointwise forces them constant functions\n\n\n\nNow we get two ODEs from Equation 4: \\[\n\\begin{cases} A_{xx} = \\lambda A \\\\\nB_{yy} = -\\lambda B\n\\end{cases}\n\\tag{5}\\]\nRefer to Section 7 for the solution of this kind of ODE."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#boundary-conditions-applied",
    "href": "posts/pde-solve-laplace-equation/index.html#boundary-conditions-applied",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "5 Boundary conditions applied",
    "text": "5 Boundary conditions applied\n\n5.1 Determine the sign of \\(\\lambda\\)\n\n\n\n\n\n\n\nClaim\n\n\n\n\nProposition 1 We claim that: \\[\n\\lambda \\le 0.\n\\]\n\nThis is because if \\(\\lambda &gt; 0\\), \\[\nA(x) \\in \\operatorname{span}_\\mathbb{R}\\{\\cosh(\\sqrt{\\lambda} x), \\sinh(\\sqrt{\\lambda} x)\\}.\n\\] But hyperbolic functions look like the right graph of Figure 5, there is no element in \\(\\operatorname{span}_\\mathbb{R}\\{\\cosh(\\sqrt{\\lambda} x), \\sinh(\\sqrt{\\lambda} x)\\}\\) that equals \\(0\\) at both ends of the rectangle. Only periodic sinusoidal functions can do that.\n\n\n\nFrom Proposition 1, Equation 5 implies \\[\n\\begin{cases} A(x) \\in \\operatorname{span}_\\mathbb{R}\\{\\cos(\\omega x), \\sin(\\omega x)\\} \\cup \\{\\alpha_1 x + \\beta_1\\} \\\\\nB(y) \\in \\operatorname{span}_\\mathbb{R}\\{\\cosh(\\omega y), \\sinh(\\omega y)\\} \\cup \\{\\alpha_2 y + \\beta_2\\}\n\\end{cases}\n\\tag{6}\\] where \\(-\\lambda = \\omega^2\\)\n\n\n5.2 Determine valid \\(\\lambda\\) values\n\n5.2.1 Left and right boundary conditions\nLet’s start with the easier boundary conditions @BC1 and @BC2.\n\\(A(x)\\) in Equation 6 are just sinusoidal functions or linear functions. If we force it to be zero on the edges, all of a sudden \\(\\alpha_1\\) and \\(\\beta_1\\) should be both zero, and the coefficients before \\(\\cos(\\omega x)\\) also must be zero, otherwise the function will never be zero at the line \\(x=0\\). Also the horizontal length \\(p\\) must be integer multiple of half period of \\(\\sin(\\omega x)\\), i.e., \\[\nn \\cdot \\frac{\\pi}{\\omega} = p, \\quad n = 1, 2, 3, \\cdots.\n\\]\nTherefore, there are only countably many valid \\(\\omega\\), hence countably many valid \\(\\lambda\\): \\[\n\\lambda = -\\omega^2 \\in \\left\\{ -\\frac{n^2 \\pi^2}{p^2} \\right\\}_{n=1}^\\infty.\n\\]\nSo \\(A(x)\\) in Equation 6 is in a subspace: \\[\n\\boxed{\nA(x) \\in \\operatorname{span}_\\mathbb{R}\\left\\{\\sin\\left(\\frac{n \\pi}{p} x\\right)\\right\\}}\n\\]\nAs shown in Figure 4, \\(A(x)\\) is like the harmonics on a string.\n\n\n\n\n\n\nFigure 4: Mental picture for \\(A(x)\\) and \\(B(y)\\)\n\n\n\n\n\n5.2.2 Top and bottom boundary conditions\nOn top and bottom edges, we have the boundary condition (BC3) and (BC4) in Equation 1.\n\\(B(y)\\) in Equation 6 must obey these boundary conditions. (BC3) in Equation 1 forces \\(\\beta_2 = 0\\), and the coefficients before \\(\\cosh(\\omega y)\\) to be zero. So \\(B(y)\\) in Equation 6 is in a subspace: \\[\n\\boxed{\nB(y) \\in \\operatorname{span}_\\mathbb{R}\\left\\{\\sinh\\left(\\frac{n \\pi}{p} y\\right)\\right\\} \\cup \\{\\alpha_2 y\\}}\n\\]\n\n\n\n5.3 Fourier series for the undetermined coefficients\nWe have not used the boundary condition (BC4) in Equation 1 yet. Actually, it leads to the introduction to Fourier series! Let’s write what does \\(F(x,y)\\) looks like up to now: \\[\n\\begin{aligned}\nF(x, y) &= A(x)B(y) \\\\\n&= c \\sin\\left(\\frac{n \\pi}{p} x\\right) \\sinh\\left(\\frac{n \\pi}{p} y\\right) \\\\\n\\text{or } &= c' 0 \\cdot y\n\\end{aligned}\n\\tag{7}\\] where \\(c\\) depends on the choice of \\(n\\), \\(n\\) can be any positive integer. Now we have a very important claim:\n\n\n\n\n\n\n\nClaim\n\n\n\n\nProposition 2 If we add these valid \\(F\\) together, the sum is still valid: \\[\n\\boxed{\nF(x,y) = \\sum_{n=1}^\\infty c_n \\sin\\left(\\frac{n \\pi}{p} x\\right) \\sinh\\left(\\frac{n \\pi}{p} y\\right).}\n\\tag{8}\\]\n\nThink about why!?\nEach solution in Equation 7 is a solution to the Laplace’s equation, right? By the superposition principle, their sum is also a solution to the Laplace’s equation!\n\n\n\nHere is the most beautiful part: We plugin (BC4) in Equation 1 into Equation 8: \\[\nF(x, q) = \\sum_{n=1}^\\infty \\underbrace{c_n \\sinh\\left(\\frac{n \\pi}{p} q\\right)}_{\\mathclap{\\scriptsize \\text{Fourier coefficients of }f(x)}} \\sin\\left(\\frac{n \\pi}{p} x\\right) = f(x).\n\\tag{9}\\]\nThe \\(c_n \\sinh(n \\pi q/p)\\) are just constants, which is exactly the Fourier coefficients of the function \\(f(x)\\)!\n\n\n\n\n\n\n\nDeriving \\(c_n\\) through Fourier analysis\n\n\n\n\n\nWe know that a real-valued function \\(f(x)\\) of period \\(T\\) could be decomposed as: \\[\nf(x) = a_0 + \\sum_{n=1}^\\infty a_n \\cos(n \\omega_0 x) + \\sum_{n=1}^\\infty b_n \\sin(n \\omega_0 x),\n\\tag{10}\\] where \\[\n\\begin{cases}\na_n &= \\frac{2}{T} \\langle f, \\cos(n \\omega_0 x) \\rangle \\\\\nb_n &= \\frac{2}{T} \\langle f, \\sin(n \\omega_0 x) \\rangle \\\\\na_0 &= \\frac{1}{T} \\langle f, 1 \\rangle.\n\\end{cases}\n\\tag{11}\\]\nWe consider the function \\(f(x)\\) to be a periodic function with period \\(T=2p\\) (not \\(p\\)) since we need \\[\n\\omega_0 = \\frac{\\pi}{p}.\n\\]\nNow we know that all \\(a_n = 0\\). Compare Equation 11 with Equation 9, \\[\nb_n = \\frac{2}{2p} \\left\\langle f, \\sin\\left(\\frac{n \\pi}{p}x\\right) \\right\\rangle = c_n \\sinh\\left(\\frac{n \\pi}{p} q\\right).\n\\] Solve for \\(c_n\\): \\[\n\\begin{aligned}\nc_n &= \\frac{1}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\left\\langle f, \\sin\\left(\\frac{n \\pi}{p}x\\right) \\right\\rangle \\\\\n&= \\frac{1}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\int_0^{2p} f(x) \\sin\\left(\\frac{n \\pi}{p}x\\right) \\mathrm{d}x \\\\\n&= \\frac{2}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\int_0^{p} f(x) \\sin\\left(\\frac{n \\pi}{p}x\\right) \\mathrm{d}x.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#conclusion",
    "href": "posts/pde-solve-laplace-equation/index.html#conclusion",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThe solution to the simplified boundary condition subproblem is: \\[\nF(x,y) = \\sum_{n=1}^\\infty c_n \\sin\\left(\\frac{n \\pi}{p} x\\right) \\sinh\\left(\\frac{n \\pi}{p} y\\right),\n\\tag{12}\\]\nwhere \\(c_n\\) satisfies: \\[\nc_n = \\frac{2}{p \\sinh\\left(\\frac{n \\pi}{p} q\\right)} \\int_0^{p} f(x) \\sin\\left(\\frac{n \\pi}{p}x\\right) \\mathrm{d}x.\n\\]\nBy the superposition principle (shown in Figure 2), the solution to the original problem is the linear combination of the solutions in the form of Equation 12."
  },
  {
    "objectID": "posts/pde-solve-laplace-equation/index.html#sec-2nd-order-ode-refresh",
    "href": "posts/pde-solve-laplace-equation/index.html#sec-2nd-order-ode-refresh",
    "title": "Solving Laplace’s Equation using Separation of Variables",
    "section": "7 2nd-order ODE Refresh",
    "text": "7 2nd-order ODE Refresh\n\n\n7.1 Complex domain solution\nConsider the following 2nd-order ODE: \\[\n\\ddot{x} = \\lambda x,\n\\tag{13}\\] where \\(x(t) \\in \\mathbb{C}, \\lambda \\in \\mathbb{R}\\). What function satisfies this special property that if we differentiate it twice, we get itself up to a constant? Exponentials! So we guess: \\[\nx(t) = e^{rt}\n\\] with \\[\nr = \\pm \\sqrt{\\lambda}.\n\\]\nTherefore, the general solution to Equation 13 is: \\[\nx(t) = c_1 e^{\\sqrt{\\lambda} t} + c_2 e^{-\\sqrt{\\lambda} t},\n\\tag{14}\\] where \\(c_1, c_2 \\in \\mathbb{C}\\) are constants. Done!\nWe can use different notation according to the sign of \\(\\lambda\\):\n\n\\(\\lambda &lt; 0\\): Let \\(-\\lambda = \\omega^2\\), then Equation 14 can also be written as: \\[\nx(t) = c_1' \\cos(\\omega t) + c_2' \\sin(\\omega t) \\quad (c_1', c_2' \\in \\mathbb{C})\n  \\tag{15}\\] since \\[\n\\operatorname{span}_\\mathbb{C}\\{e^{i\\omega t}, e^{-i\\omega t}\\} = \\operatorname{span}_\\mathbb{C}\\{\\cos(\\omega t), \\sin(\\omega t)\\}\n\\]\n\\(\\lambda &gt; 0\\): Let \\(\\lambda = a^2\\), then Equation 14 can also be written as: \\[\nx(t) = c_1'' \\cosh(at) + c_2'' \\sinh(at) \\quad (c_1'', c_2'' \\in \\mathbb{C})\n  \\tag{16}\\] since \\[\n\\operatorname{span}_\\mathbb{C}\\{e^{at}, e^{-at}\\} = \\operatorname{span}_\\mathbb{C}\\{\\cosh(at), \\sinh(at)\\}.\n\\]\n\\(\\lambda = 0\\):\n\n\\[\n\\begin{align*}\n&\\ddot{x} = 0 \\\\\n\\implies\\quad &\\dot{x} = \\alpha \\\\\n\\implies\\quad &x(t) = \\alpha x + \\beta\n\\end{align*}\n\\]\n\n\n\n\n\n\nFigure 5: The graph of trigonometric and hyperbolic functions\n\n\n\n\n\n7.2 Real domain solution\nThe solution of Equation 13 when \\(x(t)\\) is confined to \\(\\mathbb{R}\\) is very simple. One just need to confine \\(c_1', c_2'\\) in Equation 15 and \\(c_1'', c_2''\\) in Equation 16 to \\(\\mathbb{R}\\): \\[\nx(t) = c_1 \\cos(\\omega t) + c_2 \\sin(\\omega t) \\quad (\\lambda &lt; 0, -\\lambda = \\omega^2, c_1, c_2 \\in \\mathbb{R})\n\\] and \\[\nx(t) = c_1 \\cosh(at) + c_2 \\sinh(at) \\quad (\\lambda &gt; 0, \\lambda = a^2, c_1, c_2 \\in \\mathbb{R}).\n\\]"
  },
  {
    "objectID": "posts/quadrocopter-control/index.html",
    "href": "posts/quadrocopter-control/index.html",
    "title": "UAV Control 无人机控制原理",
    "section": "",
    "text": "One attitude of a quadrocopter can be represented by either an \\(R \\in SO(3)\\) or two quaternions \\(\\pm q \\in U(\\mathbb{H})\\).\n\nEvery possible attitude of a quadrocopter corresponds an element in \\(SO(3)\\) uniquely. We also know that the space of unit quaternions \\(U(\\mathbb{H})\\) double covers \\(SO(3)\\): \\[\nU(\\mathbb{H}) \\overset{2:1} \\twoheadrightarrow SO(3).\n\\tag{1}\\]\nThis means one rotation can also be expressed by two different unit quaternions, \\(q\\) and \\(-q\\).\n\n\n\n\n\n\n\nHow do unit quaternions represent rotations?\n\n\n\n\n\nA rotation around an axies \\(\\mathbf{v} \\in \\mathbb{S}^2\\) by an angle \\(\\theta\\) can be represented by a quaternion \\(q\\): \\[\nq = \\cos\\left(\\frac{\\theta}{2}\\right) + \\sin\\left(\\frac{\\theta}{2}\\right) \\mathbf{v}.\n\\]\nTherefore, \\(\\forall q \\in U(\\mathbb{H})\\), \\[\n\\begin{aligned}\nq &= q_0 + q_1 \\mathbf{i} + q_2 \\mathbf{j} + q_3 \\mathbf{k} \\\\\n&\\equiv q_0 + \\mathbf{u} \\\\\n&= \\underbrace{q_0}_{\\cos \\frac{\\theta}{2}} + \\underbrace{\\lVert\\mathbf{u}\\rVert}_{\\sin \\frac{\\theta}{2}} \\cdot \\underbrace{\\frac{1}{\\lVert\\mathbf{u}\\rVert} (q_1 \\mathbf{i} + q_2 \\mathbf{j} + q_3 \\mathbf{k})}_{\\text{rotation axies}}.\n\\end{aligned}\n\\] i.e., the imaginary part of \\(q\\) encodes the rotation axis and the real part encodes the rotation angle.\nCompare this with how the angular velocity vector \\(\\boldsymbol{\\Omega}\\) encodes the rotation information: Figure 3.\n\n\n\n\n\n\n\n\n\n\n\nQuick proof on the double covering\n\n\n\n\n\nWe will approach Equation 1 from the following simple steps:\n\nMental picture for the manifold \\(SO(3)\\): Three dimensional solid ball modulo the antipodal points on its surface: \\[\nSO(3) \\simeq D^3/\\sim\n\\]\n\n\n\n\n\n\nFigure 1: Every point on \\(D^3/\\sim\\) represent a rotation\n\n\n\nUnit quaternions sits bijectively on the 3-sphere \\(S^3\\): \\[\nU(\\mathbb{H}) \\simeq \\mathbb{S}^3.\n\\]\nThis is easy to see from the definition of unit quaternions: \\[\n\\begin{aligned}\nU(\\mathbb{H}) &:= \\{ q \\in \\mathbb{H} : |q| = 1 \\} \\\\\n&\\simeq \\{ (a, b, c, d) \\in \\mathbb{R}^4 : a^2 + b^2 + c^2 + d^2 = 1 \\} \\\\\n&=: \\mathbb{S}^3\n\\end{aligned}\n\\] where \\(q = a + b\\mathbf{i} + c \\mathbf{j} + d \\mathbf{k}\\).\nNatural double cover projection from sphere to the equator plate: \\[\n\\mathbb{S}^3 \\overset{2:1} \\twoheadrightarrow \\frac{\\mathbb{S}^3}{\\sim} \\simeq \\frac{D^3}{\\sim}.\n\\]\nJust have a look at 2-dimensional case:\n\n\n\n\n\n\nFigure 2: 2-dimensional case of the natural double covering\n\n\n\nFrom the above, we derived Equation 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: How \\(\\boldsymbol{\\Omega}(t)\\) encodes the rotation axis and angular velocity\n\n\n\n\nThe time evolution of the attitude of a quadrocopter \\(q(t)\\) and the angular velocity vector1 \\(\\boldsymbol{\\Omega}(t)\\) satisfy2: \\[\\dot{q} = \\frac{1}{2} q \\cdot \\Omega. \\tag{2}\\]\n1 The angular velocity vector (shown in Figure 3) \\(\\boldsymbol{\\Omega}(t_1) \\in \\mathbb{R}^3\\) encodes the rotation axis (\\(\\boldsymbol{\\hat{\\Omega}}\\)) and the angular velocity around that axis (\\(\\lVert\\boldsymbol{\\Omega}\\rVert\\)).2 You may be confused by how a quaternion could multiplied with a vector. It’s just because every vector \\(v = v^1 \\mathbf{i} + v^2 \\mathbf{j} + v^3 \\mathbf{k} \\in \\mathbb{R}^3\\) naturally embedded into \\(\\mathbb{H}\\) by a map \\(p: \\mathbb{R}^3 \\to \\mathbb{H}\\), \\(p(v) := 0 + v^1 \\mathbf{i} + v^2 \\mathbf{j} + v^3 \\mathbf{k}\\). Equation 2 is actually \\(\\dot{q} = \\frac{1}{2} q \\cdot p(\\boldsymbol{\\Omega})\\).\n\n\n\n\n\n\nFigure 4: Attitude of the quadrocopter as a path in \\(D^3/\\sim\\)\n\n\n\nIn other words, the attitude \\(q(t)\\) of a quadrocopter can be represented as a path in \\(D^3 / \\sim\\) (shown in Figure 4), how to know the angular velocity vector \\(\\mathbf{\\Omega}(t)\\) at a time \\(t_0\\)? Equation 2 tells us just take the tangent vector (actually a quaternion \\(\\dot{q}\\)) of the path at time \\(t_0\\), multiplied by 2 then divide by \\(q\\).\n\n\n\n\n\n\n\nProof of Equation 2\n\n\n\n\n\nIf both the attitude of the quadrocopter and the angular velocity vector is given at time \\(t\\), then the attitude configuration at time \\(t + \\Delta t\\) can be determined and can be considered as the composition of two consecutive rotations:\n\nFirst, rotate the quadrocopter by \\(q(t)\\).\nSecond, rotate the quadrocopter by a quaternion \\(r(t)\\) defined by \\(\\boldsymbol{\\Omega}(t)\\) and \\(\\Delta t\\), which represents a small rotation (where \\(\\Delta \\theta\\) is a small angle around the axis \\(\\hat{\\Omega}(t)\\)): \\[\n\\begin{aligned}\nr(t) &= \\cos\\left(\\frac{\\Delta \\theta}{2}\\right) + \\sin\\left(\\frac{\\Delta \\theta}{2}\\right) \\hat{\\Omega}(t) \\\\\n&= \\cos\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) + \\sin\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) \\hat{\\Omega}(t).\n\\end{aligned}\n\\]\n\nTherefore, the attitude at time \\(t + \\Delta t\\) can be expressed as3: \\[\n\\begin{align*}\n   &\\begin{aligned}\n   q(t + \\Delta t) &= q(t) \\cdot r(t) \\\\\n   &= q(t) \\cdot \\left( \\cos\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) + \\sin\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) \\hat{\\Omega}(t) \\right) \\\\\n   &= q(t) \\cdot \\left( 1 + \\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2} \\frac{\\Omega(t)}{\\lVert\\boldsymbol{\\Omega}(t)\\rVert} \\right)  + o(\\Delta t) \\quad \\text{(First order approximation)}\n   \\end{aligned} \\\\\n   \\implies\\quad & \\frac{q(t + \\Delta t) - q(t)}{\\Delta t} \\approx \\frac{1}{2} q(t) \\cdot \\Omega(t) \\\\\n   \\implies\\quad & \\dot{q}(t) = \\frac{1}{2} q(t) \\cdot \\Omega(t).\n\\end{align*}\n\\]\nWe are done!\n\n\n\n\n\n3 In this post, \\(q \\cdot p\\) reads from left to right, i.e., \\(q\\) is rotated first and then \\(p\\).\n\n\n\n\n\nQ&A for Equation 2\n\n\n\n\n\n\nQ1: The attitude of the quadrocopter can be represented by two quaternions \\(\\pm q \\in U(\\mathbb{H})\\), but there’s only one possible \\(\\mathbf{\\Omega}(t)\\) at a given time. Why?\nA1: Both \\(q\\) and \\(-q\\) satisfy Equation 2 with the same \\(\\mathbf{\\Omega}(t)\\): \\[\n\\begin{aligned}\n\\frac{\\mathrm{d}}{\\mathrm{d} t} q &= \\frac{1}{2} q \\cdot \\Omega \\\\\n\\frac{\\mathrm{d}}{\\mathrm{d} t} (-q) &= \\frac{1}{2} (-q) \\cdot \\Omega\n\\end{aligned}\n\\]\nQ2: Given any path4 \\(q(t)\\) in \\(D^3/\\sim\\), the angular velocity vector can be computed by: \\[\n\\Omega = 2 \\frac{\\dot{q}}{q}.\n\\] Does this result \\(\\Omega\\) guaranteed to be purely imaginary? (Because angular velocity vector must be purely imaginary).\nA2: Yes. We give the following theorems first:\n\n\n\n\n\n\n\n\nQuaternion product rule\n\n\n\n\nTheorem 1 Let quaternions \\[\n\\begin{aligned}\np &\\equiv a + \\mathbf{u}, \\\\\nq &\\equiv b + \\mathbf{v}.\n\\end{aligned}\n\\]\nTheir product is given by: \\[\np \\cdot q = ab - \\mathbf{u} \\cdot \\mathbf{v} + a \\mathbf{v} + b \\mathbf{u} + \\mathbf{u} \\times \\mathbf{v}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nInner product in \\(\\mathbb{H}\\)\n\n\n\n\nTheorem 2 Since \\(\\mathbb{H} \\simeq \\mathbb{R}^4\\) as vector spaces, we can borrow the inner product structure on \\(\\mathbb{R}^4\\) to \\(\\mathbb{H}\\). Let \\(p, q \\in \\mathbb{H}\\), define: \\[\n\\langle p, q \\rangle = \\sum_{i=0}^3 p_i q_i.\n\\] Also we have: \\[\n\\Re(p \\cdot q) = \\langle p, \\bar{q} \\rangle = \\langle \\bar{p}, q \\rangle.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof of Theorem 2\n\n\n\n\n\nBy Theorem 1, \\[\n\\begin{aligned}\n\\Re(p \\cdot q) &= \\Re(\\overbrace{ab - \\mathbf{u} \\cdot \\mathbf{v}}^{\\text{scalar}} + \\overbrace{a \\mathbf{v} + b \\mathbf{u} + \\mathbf{u} \\times \\mathbf{v}}^{\\text{vector}}) \\\\\n&= ab - \\mathbf{u} \\cdot \\mathbf{v} \\\\\n&= p_0 q_0 - p_1 q_1 - p_2 q_2 - p_3 q_3 \\\\\n&= \\langle p, \\bar{q} \\rangle \\\\\n&= \\langle \\bar{p}, q \\rangle.\n\\end{aligned}\n\\]\n\n\n\n\nRemember \\(q(t)\\) also lives on the surface \\(\\mathbb{S}^3\\), so: \\[\n   \\frac{\\dot{q}}{q} = \\dot{q} \\bar{q}.\n   \\]\nSince \\(\\dot{q}(t)\\) is a tangent vector to the surface \\(\\mathbb{S}^3\\), \\(\\dot{q} \\perp q\\) as vectors in \\(\\mathbb{R}^4\\), i.e., \\[\n   \\langle \\dot{q}, q \\rangle = 0.\n   \\] By Theorem 2, \\[\n   \\langle \\dot{q}, q \\rangle = \\Re (\\dot{q} \\cdot \\bar{q}) = 0,\n   \\] i.e., \\[\n   \\Re (\\Omega) = 0.\n   \\]\nSo \\(\\Omega\\) is purely imaginary. Done!\n\n\n\n\n4 Any path in \\(D^3/\\sim\\) is a legitimate attitude change of the quadrocopter.\nThe rest content may contain some errors, please be careful when reading."
  },
  {
    "objectID": "posts/quadrocopter-control/index.html#basic-rigid-body-dynamics",
    "href": "posts/quadrocopter-control/index.html#basic-rigid-body-dynamics",
    "title": "UAV Control 无人机控制原理",
    "section": "",
    "text": "One attitude of a quadrocopter can be represented by either an \\(R \\in SO(3)\\) or two quaternions \\(\\pm q \\in U(\\mathbb{H})\\).\n\nEvery possible attitude of a quadrocopter corresponds an element in \\(SO(3)\\) uniquely. We also know that the space of unit quaternions \\(U(\\mathbb{H})\\) double covers \\(SO(3)\\): \\[\nU(\\mathbb{H}) \\overset{2:1} \\twoheadrightarrow SO(3).\n\\tag{1}\\]\nThis means one rotation can also be expressed by two different unit quaternions, \\(q\\) and \\(-q\\).\n\n\n\n\n\n\n\nHow do unit quaternions represent rotations?\n\n\n\n\n\nA rotation around an axies \\(\\mathbf{v} \\in \\mathbb{S}^2\\) by an angle \\(\\theta\\) can be represented by a quaternion \\(q\\): \\[\nq = \\cos\\left(\\frac{\\theta}{2}\\right) + \\sin\\left(\\frac{\\theta}{2}\\right) \\mathbf{v}.\n\\]\nTherefore, \\(\\forall q \\in U(\\mathbb{H})\\), \\[\n\\begin{aligned}\nq &= q_0 + q_1 \\mathbf{i} + q_2 \\mathbf{j} + q_3 \\mathbf{k} \\\\\n&\\equiv q_0 + \\mathbf{u} \\\\\n&= \\underbrace{q_0}_{\\cos \\frac{\\theta}{2}} + \\underbrace{\\lVert\\mathbf{u}\\rVert}_{\\sin \\frac{\\theta}{2}} \\cdot \\underbrace{\\frac{1}{\\lVert\\mathbf{u}\\rVert} (q_1 \\mathbf{i} + q_2 \\mathbf{j} + q_3 \\mathbf{k})}_{\\text{rotation axies}}.\n\\end{aligned}\n\\] i.e., the imaginary part of \\(q\\) encodes the rotation axis and the real part encodes the rotation angle.\nCompare this with how the angular velocity vector \\(\\boldsymbol{\\Omega}\\) encodes the rotation information: Figure 3.\n\n\n\n\n\n\n\n\n\n\n\nQuick proof on the double covering\n\n\n\n\n\nWe will approach Equation 1 from the following simple steps:\n\nMental picture for the manifold \\(SO(3)\\): Three dimensional solid ball modulo the antipodal points on its surface: \\[\nSO(3) \\simeq D^3/\\sim\n\\]\n\n\n\n\n\n\nFigure 1: Every point on \\(D^3/\\sim\\) represent a rotation\n\n\n\nUnit quaternions sits bijectively on the 3-sphere \\(S^3\\): \\[\nU(\\mathbb{H}) \\simeq \\mathbb{S}^3.\n\\]\nThis is easy to see from the definition of unit quaternions: \\[\n\\begin{aligned}\nU(\\mathbb{H}) &:= \\{ q \\in \\mathbb{H} : |q| = 1 \\} \\\\\n&\\simeq \\{ (a, b, c, d) \\in \\mathbb{R}^4 : a^2 + b^2 + c^2 + d^2 = 1 \\} \\\\\n&=: \\mathbb{S}^3\n\\end{aligned}\n\\] where \\(q = a + b\\mathbf{i} + c \\mathbf{j} + d \\mathbf{k}\\).\nNatural double cover projection from sphere to the equator plate: \\[\n\\mathbb{S}^3 \\overset{2:1} \\twoheadrightarrow \\frac{\\mathbb{S}^3}{\\sim} \\simeq \\frac{D^3}{\\sim}.\n\\]\nJust have a look at 2-dimensional case:\n\n\n\n\n\n\nFigure 2: 2-dimensional case of the natural double covering\n\n\n\nFrom the above, we derived Equation 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: How \\(\\boldsymbol{\\Omega}(t)\\) encodes the rotation axis and angular velocity\n\n\n\n\nThe time evolution of the attitude of a quadrocopter \\(q(t)\\) and the angular velocity vector1 \\(\\boldsymbol{\\Omega}(t)\\) satisfy2: \\[\\dot{q} = \\frac{1}{2} q \\cdot \\Omega. \\tag{2}\\]\n1 The angular velocity vector (shown in Figure 3) \\(\\boldsymbol{\\Omega}(t_1) \\in \\mathbb{R}^3\\) encodes the rotation axis (\\(\\boldsymbol{\\hat{\\Omega}}\\)) and the angular velocity around that axis (\\(\\lVert\\boldsymbol{\\Omega}\\rVert\\)).2 You may be confused by how a quaternion could multiplied with a vector. It’s just because every vector \\(v = v^1 \\mathbf{i} + v^2 \\mathbf{j} + v^3 \\mathbf{k} \\in \\mathbb{R}^3\\) naturally embedded into \\(\\mathbb{H}\\) by a map \\(p: \\mathbb{R}^3 \\to \\mathbb{H}\\), \\(p(v) := 0 + v^1 \\mathbf{i} + v^2 \\mathbf{j} + v^3 \\mathbf{k}\\). Equation 2 is actually \\(\\dot{q} = \\frac{1}{2} q \\cdot p(\\boldsymbol{\\Omega})\\).\n\n\n\n\n\n\nFigure 4: Attitude of the quadrocopter as a path in \\(D^3/\\sim\\)\n\n\n\nIn other words, the attitude \\(q(t)\\) of a quadrocopter can be represented as a path in \\(D^3 / \\sim\\) (shown in Figure 4), how to know the angular velocity vector \\(\\mathbf{\\Omega}(t)\\) at a time \\(t_0\\)? Equation 2 tells us just take the tangent vector (actually a quaternion \\(\\dot{q}\\)) of the path at time \\(t_0\\), multiplied by 2 then divide by \\(q\\).\n\n\n\n\n\n\n\nProof of Equation 2\n\n\n\n\n\nIf both the attitude of the quadrocopter and the angular velocity vector is given at time \\(t\\), then the attitude configuration at time \\(t + \\Delta t\\) can be determined and can be considered as the composition of two consecutive rotations:\n\nFirst, rotate the quadrocopter by \\(q(t)\\).\nSecond, rotate the quadrocopter by a quaternion \\(r(t)\\) defined by \\(\\boldsymbol{\\Omega}(t)\\) and \\(\\Delta t\\), which represents a small rotation (where \\(\\Delta \\theta\\) is a small angle around the axis \\(\\hat{\\Omega}(t)\\)): \\[\n\\begin{aligned}\nr(t) &= \\cos\\left(\\frac{\\Delta \\theta}{2}\\right) + \\sin\\left(\\frac{\\Delta \\theta}{2}\\right) \\hat{\\Omega}(t) \\\\\n&= \\cos\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) + \\sin\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) \\hat{\\Omega}(t).\n\\end{aligned}\n\\]\n\nTherefore, the attitude at time \\(t + \\Delta t\\) can be expressed as3: \\[\n\\begin{align*}\n   &\\begin{aligned}\n   q(t + \\Delta t) &= q(t) \\cdot r(t) \\\\\n   &= q(t) \\cdot \\left( \\cos\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) + \\sin\\left(\\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2}\\right) \\hat{\\Omega}(t) \\right) \\\\\n   &= q(t) \\cdot \\left( 1 + \\frac{\\lVert\\boldsymbol{\\Omega}(t)\\rVert \\Delta t}{2} \\frac{\\Omega(t)}{\\lVert\\boldsymbol{\\Omega}(t)\\rVert} \\right)  + o(\\Delta t) \\quad \\text{(First order approximation)}\n   \\end{aligned} \\\\\n   \\implies\\quad & \\frac{q(t + \\Delta t) - q(t)}{\\Delta t} \\approx \\frac{1}{2} q(t) \\cdot \\Omega(t) \\\\\n   \\implies\\quad & \\dot{q}(t) = \\frac{1}{2} q(t) \\cdot \\Omega(t).\n\\end{align*}\n\\]\nWe are done!\n\n\n\n\n\n3 In this post, \\(q \\cdot p\\) reads from left to right, i.e., \\(q\\) is rotated first and then \\(p\\).\n\n\n\n\n\nQ&A for Equation 2\n\n\n\n\n\n\nQ1: The attitude of the quadrocopter can be represented by two quaternions \\(\\pm q \\in U(\\mathbb{H})\\), but there’s only one possible \\(\\mathbf{\\Omega}(t)\\) at a given time. Why?\nA1: Both \\(q\\) and \\(-q\\) satisfy Equation 2 with the same \\(\\mathbf{\\Omega}(t)\\): \\[\n\\begin{aligned}\n\\frac{\\mathrm{d}}{\\mathrm{d} t} q &= \\frac{1}{2} q \\cdot \\Omega \\\\\n\\frac{\\mathrm{d}}{\\mathrm{d} t} (-q) &= \\frac{1}{2} (-q) \\cdot \\Omega\n\\end{aligned}\n\\]\nQ2: Given any path4 \\(q(t)\\) in \\(D^3/\\sim\\), the angular velocity vector can be computed by: \\[\n\\Omega = 2 \\frac{\\dot{q}}{q}.\n\\] Does this result \\(\\Omega\\) guaranteed to be purely imaginary? (Because angular velocity vector must be purely imaginary).\nA2: Yes. We give the following theorems first:\n\n\n\n\n\n\n\n\nQuaternion product rule\n\n\n\n\nTheorem 1 Let quaternions \\[\n\\begin{aligned}\np &\\equiv a + \\mathbf{u}, \\\\\nq &\\equiv b + \\mathbf{v}.\n\\end{aligned}\n\\]\nTheir product is given by: \\[\np \\cdot q = ab - \\mathbf{u} \\cdot \\mathbf{v} + a \\mathbf{v} + b \\mathbf{u} + \\mathbf{u} \\times \\mathbf{v}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nInner product in \\(\\mathbb{H}\\)\n\n\n\n\nTheorem 2 Since \\(\\mathbb{H} \\simeq \\mathbb{R}^4\\) as vector spaces, we can borrow the inner product structure on \\(\\mathbb{R}^4\\) to \\(\\mathbb{H}\\). Let \\(p, q \\in \\mathbb{H}\\), define: \\[\n\\langle p, q \\rangle = \\sum_{i=0}^3 p_i q_i.\n\\] Also we have: \\[\n\\Re(p \\cdot q) = \\langle p, \\bar{q} \\rangle = \\langle \\bar{p}, q \\rangle.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nProof of Theorem 2\n\n\n\n\n\nBy Theorem 1, \\[\n\\begin{aligned}\n\\Re(p \\cdot q) &= \\Re(\\overbrace{ab - \\mathbf{u} \\cdot \\mathbf{v}}^{\\text{scalar}} + \\overbrace{a \\mathbf{v} + b \\mathbf{u} + \\mathbf{u} \\times \\mathbf{v}}^{\\text{vector}}) \\\\\n&= ab - \\mathbf{u} \\cdot \\mathbf{v} \\\\\n&= p_0 q_0 - p_1 q_1 - p_2 q_2 - p_3 q_3 \\\\\n&= \\langle p, \\bar{q} \\rangle \\\\\n&= \\langle \\bar{p}, q \\rangle.\n\\end{aligned}\n\\]\n\n\n\n\nRemember \\(q(t)\\) also lives on the surface \\(\\mathbb{S}^3\\), so: \\[\n   \\frac{\\dot{q}}{q} = \\dot{q} \\bar{q}.\n   \\]\nSince \\(\\dot{q}(t)\\) is a tangent vector to the surface \\(\\mathbb{S}^3\\), \\(\\dot{q} \\perp q\\) as vectors in \\(\\mathbb{R}^4\\), i.e., \\[\n   \\langle \\dot{q}, q \\rangle = 0.\n   \\] By Theorem 2, \\[\n   \\langle \\dot{q}, q \\rangle = \\Re (\\dot{q} \\cdot \\bar{q}) = 0,\n   \\] i.e., \\[\n   \\Re (\\Omega) = 0.\n   \\]\nSo \\(\\Omega\\) is purely imaginary. Done!\n\n\n\n\n4 Any path in \\(D^3/\\sim\\) is a legitimate attitude change of the quadrocopter.\nThe rest content may contain some errors, please be careful when reading."
  },
  {
    "objectID": "posts/quadrocopter-control/index.html#model",
    "href": "posts/quadrocopter-control/index.html#model",
    "title": "UAV Control 无人机控制原理",
    "section": "2 Model",
    "text": "2 Model\n\n2.1 Dynamics Equations without thrust\nThe dynamics \\(\\dot{\\mathbf{x}} = \\phi(\\mathbf{x})\\) is given by:\nPosition dynamics:\n\\[\\dot{\\mathbf{r}} = \\mathbf{v}\\]\nVelocity dynamics (Newton’s second law with only gravity):\n\\[\\dot{\\mathbf{v}} = -g\\mathbf{e}_3\\]\nOrientation dynamics (quaternion kinematics):\n\\[\\dot{q} = \\frac{1}{2}q \\cdot p(\\boldsymbol{\\Omega})\\]\nAs shown in equation (20) of the paper:\n\\[\\dot{q} = \\begin{pmatrix} \\dot{q}_0 \\\\ \\dot{q}_{1:3} \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} -q_{1:3}^T\\boldsymbol{\\Omega} \\\\ (S(q_{1:3}) + q_0\\mathbf{I})\\boldsymbol{\\Omega} \\end{pmatrix}\\]\nWhere \\(S(q1:3)S(q_{1:3})\nS(q1:3​)\\) is the skew-symmetric matrix:\n\\[S(q_{1:3}) = \\begin{pmatrix} 0 & -q_3 & q_2 \\\\ q_3 & 0 & -q_1 \\\\ -q_2 & q_1 & 0 \\end{pmatrix}\\]\nAngular velocity dynamics (Euler’s equations without external torques):\n\\[\\dot{\\boldsymbol{\\Omega}} = -\\mathbf{J}^{-1}(\\boldsymbol{\\Omega} \\times (\\mathbf{J}\\boldsymbol{\\Omega}))\\]\n\\[\\dot{\\mathbf{x}} = \\phi(\\mathbf{x}) = \\begin{pmatrix}\n\\mathbf{v} \\\n-g\\mathbf{e}_3 \\\n\\frac{1}{2}q \\cdot p(\\boldsymbol{\\Omega}) \\\n-\\mathbf{J}^{-1}(\\boldsymbol{\\Omega} \\times (\\mathbf{J}\\boldsymbol{\\Omega}))\n\\end{pmatrix}\\]\nIt’s worth noting that this represents the dynamics of a free-falling quadrocopter with no thrust forces (as the human specified “without control”). In reality, even in a passive state, a quadrocopter would have some baseline thrust from spinning propellers. For a more realistic “passive” model, we could include a constant collective thrust ff f acting along the body’s z-axis, which would modify the velocity dynamics to:\n\\[\n\\dot{\\mathbf{v}} = -g \\mathbf{e}_3 + \\frac{f}{m} R(q) \\mathbf{e}_3\n\\]\nWhere \\(R(q)\\) is the rotation matrix corresponding to quaternion \\(q\\).\nThese equations capture the full nonlinear dynamics of the quadrocopter in phase space, representing how position, velocity, orientation, and angular velocity evolve over time in the absence of control inputs."
  },
  {
    "objectID": "posts/quadrocopter-control/index.html#quadrocopter-dynamics-with-motor-control-in-phase-space",
    "href": "posts/quadrocopter-control/index.html#quadrocopter-dynamics-with-motor-control-in-phase-space",
    "title": "UAV Control 无人机控制原理",
    "section": "3 Quadrocopter Dynamics with Motor Control in Phase Space",
    "text": "3 Quadrocopter Dynamics with Motor Control in Phase Space\n\n3.1 State Vector and Basic Parameters\nAs before, the state vector in phase space is: \\[\\mathbf{x} = (\\mathbf{r}, \\mathbf{v}, q, \\boldsymbol{\\Omega})\\]\nWhere: - \\(\\mathbf{r} \\in \\mathbb{R}^3\\) is the position in inertial frame - \\(\\mathbf{v} \\in \\mathbb{R}^3\\) is the linear velocity in inertial frame - \\(q \\in \\mathbb{S}^3\\) is the unit quaternion representing orientation - \\(\\boldsymbol{\\Omega} \\in \\mathbb{R}^3\\) is the angular velocity vector in body frame\n\n\n3.2 Motor and Physical Parameters\n\n\\(m\\): mass of the quadrocopter\n\\(\\mathbf{J} \\in \\mathbb{R}^{3 \\times 3}\\): inertia tensor (typically diagonal for a symmetric quadrocopter)\n\\(g\\): gravitational acceleration (9.81 m/s²)\n\\(L\\): distance from center of mass to each motor (arm length)\n\\(k_T\\): thrust coefficient (converts squared motor speed to thrust force)\n\\(k_M\\): moment coefficient (relates thrust to reactive torque)\n\\(\\omega_i\\): angular velocity of motor \\(i\\) for \\(i \\in \\{1,2,3,4\\}\\)\n\\(\\mathbf{e}_3 = (0,0,1)^T\\): unit vector in the z-direction\n\n\n\n3.3 Motor Forces and Torques\nEach motor produces: - Thrust force: \\(F_i = k_T\\omega_i^2\\) - Reactive torque: \\(M_i = k_M\\omega_i^2\\)\nFor a typical “+” configuration (1-front, 2-right, 3-back, 4-left):\n\nTotal thrust: \\(F = \\sum_{i=1}^4 F_i = k_T\\sum_{i=1}^4 \\omega_i^2\\)\nMoment vector in body frame: \\[\\boldsymbol{\\tau} = \\begin{pmatrix} \\tau_x \\\\ \\tau_y \\\\ \\tau_z \\end{pmatrix} = \\begin{pmatrix}\nL(F_4 - F_2) \\\\\nL(F_1 - F_3) \\\\\nM_1 - M_2 + M_3 - M_4\n\\end{pmatrix}\\]\nSubstituting motor forces: \\[\\boldsymbol{\\tau} = \\begin{pmatrix}\nLk_T(\\omega_4^2 - \\omega_2^2) \\\\\nLk_T(\\omega_1^2 - \\omega_3^2) \\\\\nk_M(\\omega_1^2 - \\omega_2^2 + \\omega_3^2 - \\omega_4^2)\n\\end{pmatrix}\\]\n\n\n\n3.4 Control Input Mapping\nWe can define a mapping from motor speeds to control inputs: \\[\\begin{pmatrix} F \\\\ \\tau_x \\\\ \\tau_y \\\\ \\tau_z \\end{pmatrix} =\n\\begin{pmatrix}\nk_T & k_T & k_T & k_T \\\\\n0 & -Lk_T & 0 & Lk_T \\\\\nLk_T & 0 & -Lk_T & 0 \\\\\nk_M & -k_M & k_M & -k_M\n\\end{pmatrix}\n\\begin{pmatrix} \\omega_1^2 \\\\ \\omega_2^2 \\\\ \\omega_3^2 \\\\ \\omega_4^2 \\end{pmatrix}\\]\n\n\n3.5 Dynamics Equations with Control\n\nPosition dynamics: \\[\\dot{\\mathbf{r}} = \\mathbf{v}\\]\nVelocity dynamics (with thrust force): \\[\\dot{\\mathbf{v}} = -g\\mathbf{e}_3 + \\frac{F}{m}R(q)\\mathbf{e}_3\\]\nWhere \\(R(q)\\) is the rotation matrix corresponding to quaternion \\(q\\), which transforms the body-fixed thrust direction to the inertial frame.\nOrientation dynamics (quaternion kinematics): \\[\\dot{q} = \\frac{1}{2}q \\cdot p(\\boldsymbol{\\Omega})\\]\nAs expressed in component form: \\[\\dot{q} = \\begin{pmatrix} \\dot{q}_0 \\\\ \\dot{q}_{1:3} \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} -q_{1:3}^T\\boldsymbol{\\Omega} \\\\ (S(q_{1:3}) + q_0\\mathbf{I})\\boldsymbol{\\Omega} \\end{pmatrix}\\]\nWhere \\(S(q_{1:3})\\) is the skew-symmetric matrix of the vector part of the quaternion.\nAngular velocity dynamics (Euler’s equations with motor torques): \\[\\dot{\\boldsymbol{\\Omega}} = \\mathbf{J}^{-1}(\\boldsymbol{\\tau} - \\boldsymbol{\\Omega} \\times (\\mathbf{J}\\boldsymbol{\\Omega}))\\]\n\n\n\n3.6 Complete System Dynamics\nThe full dynamics of the controlled quadrocopter in phase space are:\n\\[\\dot{\\mathbf{x}} = \\phi(\\mathbf{x}, \\mathbf{u}) = \\begin{pmatrix}\n\\mathbf{v} \\\\\n-g\\mathbf{e}_3 + \\frac{F}{m}R(q)\\mathbf{e}_3 \\\\\n\\frac{1}{2}q \\cdot p(\\boldsymbol{\\Omega}) \\\\\n\\mathbf{J}^{-1}(\\boldsymbol{\\tau} - \\boldsymbol{\\Omega} \\times (\\mathbf{J}\\boldsymbol{\\Omega}))\n\\end{pmatrix}\\]\nWhere the control input \\(\\mathbf{u} = (\\omega_1^2, \\omega_2^2, \\omega_3^2, \\omega_4^2)\\) represents the squared motor speeds.\nThese nonlinear differential equations fully describe the quadrocopter’s motion under motor control, capturing the complex interactions between thrust forces, aerodynamic torques, and rigid body dynamics.\nNote that in practice, there may be additional terms for aerodynamic drag, motor dynamics, and other effects, but this model captures the essential physics of quadrocopter flight with motor control."
  },
  {
    "objectID": "posts/special-matrix/index.html",
    "href": "posts/special-matrix/index.html",
    "title": "Special Matrices 特殊矩阵与算子",
    "section": "",
    "text": "Definition: Normal Operator\n\n\n\n\nDefinition 1 \\(A \\in \\mathbb{C}^{n \\times n}\\) is called normal iff \\(A^H A = AA^H\\)\n\n\n\n\n\n\n\n\\(\\iff\\) \\(A\\) is unitarily-diagonalizable, i.e., \\[\n\\exists U \\in U(n), \\text{diagonal } D, \\text{s.t. } A = UDU^H. \\quad (\\text{Spectral Thm 2})\n\\]\n\\(\\iff\\) \\(\\exists \\text{ orthogonal eigenvectors that span } \\mathbb{C}^n.\\)\n\\(\\iff\\) \\(A \\text{ orthogonally non-defective}\\)\n\\(\\iff\\) \\(\\text{row covariance matrix of }A = \\text{column covariance matrix of }A\\)\nNormal matrix \\(A\\) does not necessarily invertible. Its eigenvalues can be \\(0, \\mathbb{R}\\) or \\(\\mathbb{C}\\).\nMental picture for normal operator:\n\\[\n\\boxed{\\text{Normal operators} \\iff \\text{Squeezing complex rectangular box.}}\n\\]"
  },
  {
    "objectID": "posts/special-matrix/index.html#normal-operator",
    "href": "posts/special-matrix/index.html#normal-operator",
    "title": "Special Matrices 特殊矩阵与算子",
    "section": "",
    "text": "Definition: Normal Operator\n\n\n\n\nDefinition 1 \\(A \\in \\mathbb{C}^{n \\times n}\\) is called normal iff \\(A^H A = AA^H\\)\n\n\n\n\n\n\n\n\\(\\iff\\) \\(A\\) is unitarily-diagonalizable, i.e., \\[\n\\exists U \\in U(n), \\text{diagonal } D, \\text{s.t. } A = UDU^H. \\quad (\\text{Spectral Thm 2})\n\\]\n\\(\\iff\\) \\(\\exists \\text{ orthogonal eigenvectors that span } \\mathbb{C}^n.\\)\n\\(\\iff\\) \\(A \\text{ orthogonally non-defective}\\)\n\\(\\iff\\) \\(\\text{row covariance matrix of }A = \\text{column covariance matrix of }A\\)\nNormal matrix \\(A\\) does not necessarily invertible. Its eigenvalues can be \\(0, \\mathbb{R}\\) or \\(\\mathbb{C}\\).\nMental picture for normal operator:\n\\[\n\\boxed{\\text{Normal operators} \\iff \\text{Squeezing complex rectangular box.}}\n\\]"
  },
  {
    "objectID": "posts/special-matrix/index.html#hermitian-self-adjoint-operator",
    "href": "posts/special-matrix/index.html#hermitian-self-adjoint-operator",
    "title": "Special Matrices 特殊矩阵与算子",
    "section": "2 Hermitian (Self-adjoint) Operator",
    "text": "2 Hermitian (Self-adjoint) Operator\n\n\n\n\n\n\n\nDefinition: Hermitian Operator\n\n\n\n\nDefinition 2 \\(A \\in \\mathbb{C}^{n \\times n}\\) is called hermitian (self-adjoint) iff \\(A = A^H\\)\n\n\n\n\n\n2.1 Notes\n\n\\[\nA \\text{ hermitian}\n\\iff \\begin{equation*}\n  \\begin{cases} A \\text{ normal} &  \\\\ \\text{spectrum of }A \\subseteq \\mathbb{R}. &  \\end{cases}\n\\end{equation*}\n\\]\nAlso self-adjoint operators do not necessarily invertible.\nSelf-adjoint operators could be think of as normal operators with real spectrum.\nMental picture for hermitian operators:\n\\[\n\\boxed{\\text{Hermitian operators} \\iff \\text{Squeezing complex rectangular box in a particular way that creatures living under projection }\\pi: \\mathbb{C}^n \\to \\mathbb{R}^n \\text{ do NOT think it is a rotation.}}\n\\]\nIf \\(A \\in \\mathbb{R}^{n\\times n} &lt; \\mathbb{C}^{n\\times n}\\) is hermitian, it is also called symmetric, i.e., \\[\nA = A^t.\n\\]"
  },
  {
    "objectID": "posts/special-matrix/index.html#skew-hermitian-operator",
    "href": "posts/special-matrix/index.html#skew-hermitian-operator",
    "title": "Special Matrices 特殊矩阵与算子",
    "section": "3 Skew-Hermitian Operator",
    "text": "3 Skew-Hermitian Operator\n\n\n\n\n\n\n\nDefinition: Hermitian Operator\n\n\n\n\nDefinition 3 \\(A \\in \\mathbb{C}^{n \\times n}\\) is called skew-hermitian iff \\(-A = A^H\\)\n\n\n\n\n\n3.1 Notes\n\nSkew-hermitian operators are very close to hermitian: \\[\nA \\text{ skew-hermitian} \\iff iA \\text{ hermitian}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(A \\in \\mathbb{C}^{n \\times n}\\) be skew-hermitian, i.e., \\(-A = A^H\\). Let \\(B = iA\\). Then \\[\nB^H = (iA)^H = -iA^H = iA = B.\n\\]\nTherefore, \\(B\\) is hermitian.\n\n\n\n\n\nBy the property that hermitian operators have real spectrum, all skew-hermitian operators have purely imaginary spectrum.\n\\[\nA \\text{ skew-hermitian}\n\\iff \\begin{equation*}\n\\begin{cases} A \\text{ normal} &  \\\\ \\text{spectrum of }A \\subseteq i\\mathbb{R}. &  \\end{cases}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "posts/special-matrix/index.html#unitary-operator",
    "href": "posts/special-matrix/index.html#unitary-operator",
    "title": "Special Matrices 特殊矩阵与算子",
    "section": "4 Unitary Operator",
    "text": "4 Unitary Operator\n\n\n\n\n\n\n\nDefinition: Unitary Operator\n\n\n\n\nDefinition 4 \\(A \\in \\mathbb{C}^{n \\times n}\\) is called unitary iff \\(AA^H = I\\), denoted \\(A \\in U(n)\\)\n\n\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\nProposition 1 \\[\\begin{equation*}\n    \\begin{cases} A \\text{ square} &  \\\\ A^HA = I &  \\end{cases}\n\n    \\implies A A^H = I\n\\end{equation*}\\]\n\n\n\n\n\nProof. From \\(A^H A = I\\) we know that the columns of \\(A\\) are orthonormal\n\\(\\implies\\) \\(A\\) injective\nAlso \\(A\\) is square. Since injective automorphisms are epimorphisms, we have:\n\\(\\implies\\) \\(A\\) is an isomorphism\n\\(\\implies\\) \\(A\\) is invertible, and \\(A^H = A^{-1}\\)\nThis finished the proof.\n\n\n4.1 Notes\n\nHence for unitary operators: \\[\nA^H A = A A^H = I\n\\]\nNote \\[\n\\begin{aligned}\n&\\quad \\quad \\ \\begin{cases}\n     A \\text{ is normal} \\\\\n     \\text{Spectrum of } A \\subseteq \\mathbb{S}^1 \\subseteq \\mathbb{C}\n\\end{cases} \\\\\n&\\iff A^H = A^{-1} \\\\\n&\\iff\n\\begin{cases}\n     A \\text{ is square} \\\\\n     A^H A = I\n\\end{cases} \\\\\n&\\iff\n\\begin{cases}\n     A \\text{ is square} \\\\\n     A A^H = I\n\\end{cases}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/vector-derivation-relation/index.html",
    "href": "posts/vector-derivation-relation/index.html",
    "title": "Q&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?",
    "section": "",
    "text": "In differential geometry, we usually see a vector \\(v\\) is written as: \\[\nv = v^i \\frac{\\partial }{\\partial x^i} \\bigg\\rvert_p.\n\\]\nWhy does a vector naturally relates to partial derivatives?\n\n\n\n\n\n\n\nOne-line Solution\n\n\n\n\n\n\\[\nT_p (\\mathbb{R}^n) \\cong \\text{Der}_p (C^\\infty(\\mathbb{R}^n))\n\\]"
  },
  {
    "objectID": "posts/vector-derivation-relation/index.html#question",
    "href": "posts/vector-derivation-relation/index.html#question",
    "title": "Q&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?",
    "section": "",
    "text": "In differential geometry, we usually see a vector \\(v\\) is written as: \\[\nv = v^i \\frac{\\partial }{\\partial x^i} \\bigg\\rvert_p.\n\\]\nWhy does a vector naturally relates to partial derivatives?\n\n\n\n\n\n\n\nOne-line Solution\n\n\n\n\n\n\\[\nT_p (\\mathbb{R}^n) \\cong \\text{Der}_p (C^\\infty(\\mathbb{R}^n))\n\\]"
  },
  {
    "objectID": "posts/vector-derivation-relation/index.html#solution-from-derivative-to-derivation",
    "href": "posts/vector-derivation-relation/index.html#solution-from-derivative-to-derivation",
    "title": "Q&A: Basis vectors are exactly the same as partial derivative operator? 为什么向量等价于微分算子?",
    "section": "2 Solution – From Derivative to Derivation",
    "text": "2 Solution – From Derivative to Derivation\n\n2.1 Directional derivative\nWe know from multivariable calculus that in high dimensions, we could not say the “derivative”, but the directional derivative of a function1. The directional derivative is a measure of how quickly the function value vary when we step a tiny nudge along a vector \\(v\\). Imagine we are at \\(p\\) in \\(\\mathbb{R}^3\\) and temperature is different everywhere. We are curiously about how this temperature field \\(f\\) changes in different directions. we move a tiny proportion2 along \\(v\\) (say \\(\\epsilon = 0.01 \\%\\)) and we feel the temperature changes by \\(\\Delta f = f(p+\\epsilon v)-f(p)\\). So we define the directional derivative of \\(f\\) along \\(v\\) is \\[\nD_{v} f |_p := \\lim_{\\epsilon \\to 0} \\frac{\\Delta f}{\\epsilon}.\n\\]\n1 “Scalar field” in fancier term. A scalar field in \\(\\mathbb{R}^n\\) is a map from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\).2 This is important! We are NOT moving a tiny bit but a tiny proportion, which means the length of \\(v\\) matters. Because if we move \\(0.01 \\%\\) on \\(v\\) and \\(2 v\\), \\(f\\) will vary \\(\\Delta f\\) and \\(2\\Delta f\\) and therefore the directional derivative of \\(f\\) along \\(2v\\) would be doubled! In some books, you will see we force \\(v\\) to be unit length, so we will not have this problem. But for me it’s unnecessary.3 We use upper indices to represent coordinate components and lower indices to represent basis vectors, so Equation 1 in usually notation is just \\[\nD_v f = \\langle \\frac{\\partial f}{\\partial x} \\hat{\\imath} + \\frac{\\partial f}{\\partial y} \\hat{\\jmath} +\\frac{\\partial f}{\\partial z} \\hat{k}, v_1 \\hat{\\imath} + v_2 \\hat{\\jmath} + v_3 \\hat{k} \\rangle.\n\\]It turns out that there is an explicit formula for directional derivatives: \\[\nD_{v} f = \\langle\\nabla f, v\\rangle,\n\\] i.e., the inner product between the gradient of \\(f\\) and \\(v\\). The direction of the \\(\\nabla f\\) is the steepest ascend of \\(f\\) at \\(p\\). In \\(\\mathbb{R}^3\\), this can be written as3 \\[\n\\begin{aligned}\n    D_v f &= \\langle \\frac{\\partial f}{\\partial x^1} e_1 + \\frac{\\partial f}{\\partial x^2} e_2 +\\frac{\\partial f}{\\partial x^3} e_3, v^1 e_1 + v^2 e_2 + v^3 e_3 \\rangle \\\\\n    &= v^1 \\frac{\\partial f}{\\partial x^1} + v^2 \\frac{\\partial f}{\\partial x^2} + v^3 \\frac{\\partial f}{\\partial x^3} \\\\\n    &= \\sum_i v^i \\frac{\\partial f}{\\partial x^i} \\\\\n    &=: v^i \\frac{\\partial f}{\\partial x^i}.\n\\end{aligned}\n\\tag{1}\\]\nThe last step in Equation 1 where we drop the summation notation is a convention called Einstein notation.\nWe could view \\(D_v f\\) as \\(v\\) acts on \\(f\\). Some textbook uses \\(v[f]\\) to represent this action, i.e., \\[\nv[f] := D_v f.\n\\]\n\n\n2.2 Derivation\nWe know a normal derivative satisfy so-called chain rule: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x}(fg) = \\frac{\\mathrm{d}f}{\\mathrm{d}x} g + f \\frac{\\mathrm{d}g}{\\mathrm{d}x}.\n\\]\nWe extract this property and define abstractly the derivation operator on an algebra as follows:\n\n\n\n\n\n\n\nDerivation on an Algebra\n\n\n\n\nDefinition 1 Let \\(A\\) be an algebra over field \\(\\mathbb{F}\\), a derivation is a linear map \\(D: A \\to A\\) s.t., \\[\nD(ab) = D(a)b + aD(b).\n\\]\n\n\n\n\nIt’s obvious that every \\(v\\) induces such a derivation on the algebra \\(C^\\infty_p\\) by a map \\(\\phi: v \\mapsto D_v\\). The question is: Does every derivation necessarily induced by a vector?\n\n\n\n\n\n\n\nVectors are Derivations\n\n\n\n\nTheorem 1 The space of all vectors emanating at \\(p\\) is isomorphic to the space of all derivations \\[\nT_p (\\mathbb{R}^n) \\cong \\text{Der}_p (C^\\infty(\\mathbb{R}^n)).\n\\]\n\n\n\n\nIn other words, every possible derivations on the algebra \\(C^\\infty(\\mathbb{R}^n)\\) is some directional derivative along \\(v \\in T_p (\\mathbb{R}^n)\\). Under this isomorphism, the basis vectors \\(e_i\\) is mapped to the partial derivative operator \\(\\frac{\\partial }{\\partial x^i}\\)!\nIn a general manifold \\(M\\), we actually use derivations to define tangent vectors on a manifold4. Because the concept of derivations are just functions that satisfy certain property, which is easy to define. While vectors seem exclusively belongs to Euclidean space. So :\n\n4 Tu’s book is a very good book of differential geometry for beginners, check it out!\n\n\n\n\n\nTangent Vector in a manifold\n\n\n\n\nDefinition 2 A tangent vector at a point \\(p\\) in a manifold \\(M\\) is a derivation at \\(p\\).\n\n\n\n\nThis is common in mathematics. We call this “stereotyping”, ah sorry, “abstraction”. We find two similar concepts (e.g. vectors and directional derivatives) on some object (euclidean space). But one of them (directional derivative) can be easily generalized to another objects (“manifold”). So then Mathematicians use some of its properties back to define itself axiomatically and called it the same name just to confuse people (“tangent vectors”)5. Or invent another name (e.g. topological space) just to be intimidating. Anyway, you will feel comfortable once you get used to them.\n\n\n5 Other examples include topological spaces, groups, \\(\\sigma\\)-algebra, “measurable spaces”, etc. These are just abstraction of open sets, closed stuff, events, volumes, etc."
  },
  {
    "objectID": "posts/ccd-crash-course/index.html",
    "href": "posts/ccd-crash-course/index.html",
    "title": "CCD 通信电路速成笔记",
    "section": "",
    "text": "本笔记是本人电子科技大学格院大三《通信电路设计》课程的期末考试复习笔记, 遵循个人学习习惯与轨迹, 但我将尝试保留所有概念的动机和尽可能地解释原因, 不保证严谨性和绝对正确性."
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#sec-change-of-mind",
    "href": "posts/ccd-crash-course/index.html#sec-change-of-mind",
    "title": "CCD 通信电路速成笔记",
    "section": "1 Change of Mind 这个可能有用",
    "text": "1 Change of Mind 这个可能有用\n\n别当数学来学: 模电里面有很多 abuse of notation (AoN, 比如你看看「增益」这个词在 PLL 里面是怎么用的, dddd) 和抽象指标 (Carson’s rule 首当其冲, Modulation index 等) 和符号, 他们并不值得你品味和推导, 因为这里面人为的因素太多 (比如规定 \\(-3\\text{ dB}\\) 就很小了, Carson 频带内包含 \\(98\\%\\) 的能量等等), 这不是数学, 即使你理解了其推导也没有任何意义.\n不同问题情景下我们大脑中的模型是不一样的, 比如:\n\n普通电路分析中二极管就是单向导通; 但是 BJT 的 DC 分析中, 要用到 \\(0.7\\text{ V}\\) 的开启电压; 而 AC 分析中由于小信号, \\(0.7\\text{ V}\\) 这个模型就不能用了, 而要用更复杂的 Shockley 方程.\n放大电路中 \\(C, L\\) 都可以看作理想, 但是在 RLC 电路中 (Section 6) 就不行.\n适合的模型才是最好的, 不是越复杂越精确越好.\n\n关于「电压」和「电流」, 不要认为电压才是我们可以控制的, 然后再通过给电阻来控制电流. 有些元件 (BJT) 其实本质是通过操控电子的流动趋势来主动操控电流, 电流确定了以后, 再通过加电阻来获得电压.\n关于「背诵」的价值的问题, 有一些是纯 sb (比如某4046的参数, FM 的最大频偏), 还有一些 (IF 放大器比 RF 放大器更不容易发生自激振荡) 这种性质, 如果给足你时间进行分析和实验, 你是可以独立地发现这个结论的. 你不知道这个结论只是时间的问题, 所以你可以非常无负罪感地将它记下来, 因为你明白这不是死记硬背, 而是用前人的经验给自己节约时间1.\n考虑负载的习惯: 往往一个电路的性质 (滤波中心频率, 增益) 不仅取决于电路本身的参数, 还取决于你把它接在什么上面 (经典例子 Section 6.4). 你接在了抽象的东西上面, 它的作用也就变了. 这也是引入 「输入阻抗」和 「输出阻抗」 的动机.\n往往一个问题的解决方法的得出会经历若干个重要思想, 这些思想:\n\n的数量没你想象的多, 可能只有 4-5 个. 以下概念的引入可以称作「思想」:\n\n稳态解和瞬态解是完全不同的两个思维角度: 比如反馈实际上是需要时间的, 输出不断地被送回输入, 然后再改变输出, 再回来改变输入, 这个过程既耗实际时间, 也耗思维时间. 当反馈电路变得很复杂时这个过程几乎不可能被人类的大脑想清楚, 于是我们才用「假设量-列方程」的方法来将这个过程各个量权衡的结果需要满足的关系形式化出来, 我们解方程的过程就已经符号化地完成了整个分析过程并求出了最终的稳态解.\nPhasor: 正弦波与复数的同构, 接着引入阻抗, 颠覆了我们对信号的 mental picture (“信号是时域的”这种我们自己都无法察觉的直觉).\n信号是复值的: 傅立叶变换的基础思维, 这个思想都没有的话你就完全不理解 Fourier.\n传递函数及 \\(s\\) 域: Phasor 只是频谱里的一个频率点了, 频谱甚至可以外延到 \\(s\\) 域, 发现系统稳定性等可以在 \\(s\\) 域中很好地描述.\n信号流图: 给机械系统、电路系统甚至生物进化等等提供了统一的分析方法, 进而发展出控制论.\n\n不 trivial, 以至于通过这些思想堆叠出来的解决方法看起来非常不直观和莫名其妙. 但是其实他们就是基于这些思想逻辑推理的自然产物2!\n\n\n1 有些数学家对工程学嗤之以鼻的原因本质上是对「经验主义」的怀疑.2 这一点在纯数上尤为明显."
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#transistors-晶体管元件",
    "href": "posts/ccd-crash-course/index.html#transistors-晶体管元件",
    "title": "CCD 通信电路速成笔记",
    "section": "2 Transistors 晶体管元件",
    "text": "2 Transistors 晶体管元件\n\n2.1 Diode 二极管 ★☆☆☆☆\n以下是二极管的三种由简入繁的模型:\n\nForward/Reverse Bias 正/反向偏置 \nThreshold / Barrier potential 开启电压 \\(V_t\\) : PN 结导通所需克服的势垒电压.\n\nSilicon 硅管 \\(\\boxed{V_t = 0.7 \\text{ V}}\\) (默认这个值!)\nGermanium 锗管 \\(V_t = 0.3 \\text{ V}\\)\nBJT 的 BE 间在 DC 工作模式下也有这个开启电压.\n\n*Shockley 二极管方程: \\[ I_{\\text{D}} = I_{\\text{S}} \\left( e^{\\frac{V_{\\text{D}}}{V_{\\text{T}}}} - 1 \\right) \\tag{1}\\]\n\n\\(I_{\\text{S}}\\) 反向饱和电流 (非常小 \\(10^{-15}\\)) , \\(\\boxed{V_{\\text{T}} = 25 \\text{ mV}}\\) 热电压 是常数.\n“Diode Mixer”\n\n\n\n\n\n\n\n二极管的 Shockley 指数性质\n\n\n\n\n2.2 MOSFET 场效应管 ★☆☆☆☆\n\nJFET 可近似看作 depletion-mode MOSFET (Figure 1), 因此省略 JFET 的讨论.\n结构: Gate, Source, Drain 栅极, 源极, 漏极.\n\nS 源极指的是电子的源 而不是电流的源. D 漏极同理.\n计算题一般假设 \\(I_{\\text{S}} = I_{\\text{D}}\\).\nJFET 的 GS 在 \\(V_{\\text{GS}}\\) 下可能有极微弱的 leakage current3 \\(I_{\\text{GSS}}\\), 造成 input resistance 输入电阻 (很大): \\[R_{\\text{in}} := \\left\\vert \\frac{V_{\\text{GS}}}{I_{\\text{GSS}}} \\right\\vert\\]\nMOSFET 由于有氧化层隔离, GS 无电流.\n\n\n\n\nG 闸门, \\(V_{\\text{DS}}\\) 水压, \\(I_{\\text{D}}\\) 水流\n\n\n\n3 AoN, 应该记为 \\(I_{\\text{GSL}}\\) (gate-source leakage).\nMOSFET 分类:\n\n\n\n\n\n\nFigure 1: 箭头 in 代表 n-channel\n\n\n\n\nP/N channel 通道: 根据沟道类型 (不是 substrate 类型!), 空穴: P channel, 电子: N channel.\nDepletion/Enhancement mode 耗尽/增强型: 默认 (GSS, gate-source short) 闸门的状态, 导通: depletion, 不导通: enhancement (符号上断开).\n\\(V_{\\text{GS}}\\) 控制闸门的方法是不一样的 (Figure 1):\n\nP-channel D Mode: 正电压 -&gt; 吸引 n substrate 的电子 -&gt; 中和掉 channel 的空穴 -&gt; 截止\nN-channel D Mode: 负电压 -&gt; 吸引 p substrate 的空穴 -&gt; 中和掉 channel 的电子 -&gt; 截止\nP-channel E Mode: 负电压 -&gt; 排斥 n substrate 的电子 -&gt; 增加 channel 的空穴 -&gt; 导通\nN-channel E Mode: 正电压 -&gt; 排斥 p substrate 的空穴 -&gt; 增加 channel 的电子 -&gt; 导通\n\n\n特性:\n\n以 S 源极为基准, 我们关心 \\(V_{\\text{GS}}, V_{\\text{DS}}, I_{\\text{D}}\\) 三者的关系 (\\(I_{\\text{D}}\\) 曲面)\nQ-point (Quiescent)4: 静态工作点, Figure 2 上的每个点都是 Q-point.\n\n引入 Q-point 的原因是: 半导体元件在不同的输入变化频率下 即使输入的数值相同, 输出会有所差别. Q-point 代表这些点都是在输入变化很慢的情况下测出来的值, 即 DC 工作点、稳态值.\n所以模拟电路有必要区分 DC 和 AC 分析, 他们是不一样的.\n\n\n\n\n\n\n\n\nFigure 2: \\(I_{\\text{D}}\\) 曲面\n\n\n\n\nDrain curve 漏极特性曲线: 固定闸门 \\(V_{\\text{GS}}\\) 后的 水压与水流的关系.\n\nOhmic region 欧姆区: 相当于固定电阻. (改变闸门可以充当可变电阻)\nActive region 饱和区: 水流不变了.\nBreakdown region 击穿区: 水压太大, 水管坏了.\nPinch-off 对应的点是抛物线.\n\n\n\n\n\nDrain curve 漏极特性曲线\n\n\n\nTransfer curve 转移特性曲线: 水压足够大时水流和闸门的关系 (\\(I_{\\text{D}}\\) 曲面沿 \\(V_{\\text{DS}}\\) 轴的投影).\n\n抛物线 关系.\n默认饱和电流 \\(I_{\\text{DSS}}\\): Drain to Source current with gate Shorted.\n跨导 \\(g_m\\) (电阻的倒数): Drain curve 的斜率, 与闸门电压有关.\n\n\n\n\n\n\n\n\nFigure 3: Transfer curve 转移特性曲线\n\n\n\n\n4 这是一个典型的教科书讲不明白的问题.\n\n2.3 BJT 三极管 ★★★★☆\n\n大写表示 DC 工作模式 (e.g., \\(I_{\\text{E}}, \\beta_{\\text{DC}}\\)), 小写表示 AC 工作模式 (e.g., \\(I_{\\text{e}}, \\beta_{\\text{ac}}\\)). \\(\\tilde{I}\\): phasor, \\(i\\): 瞬时值.\n\n\n2.3.1 结构 ★☆☆☆☆\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n结构: Base, Collector, Emitter 基极, 集电极, 发射极.\n\nnpn, pnp 两种, 因 doping 浓度不同而不对称 ! npn 性能更好.\n\n模型及可观测参数\n\nr Parameters (resistance): \\(r'_e\\) (最重要★★★★★), \\(r'_b, r'_c, \\alpha_{\\text{ac}}, \\beta_{\\text{ac}}\\)\n\n\n\n\n\n\n\nFigure 4: 根据 \\(r'_b \\approx 0\\) 和 \\(r'_c \\approx \\infty\\) 来简化模型\n\n\n\n\nh Parameters (hybrid): (Omitted).\n\n\n\n\n2.3.2 性质 ★★★★★\n\n特征曲线\n\n一般都在 active region 工作区\n\\(I_C\\) 不一定是 \\(I_B\\) 的 \\(\\beta_{\\text{DC}}\\) 倍, 在饱和区就不行 (Figure 5).\n\n\n\n\n\n\n\n\n\n\nFigure 5: 饱和区没有放大\n\n\n\n\n\n\n抽象命名来了, 看看饱和区和工作区是不是你想的那样\n\n\n\nAC & DC 下都有的性质:\n\n箭头代表 总 (E) 电流 参考方向, 不一定是实际电流方向. 三者满足 KCL: \\[\\boxed{ I_{\\text{E}} - I_{\\text{C}} - I_{\\text{B}} = 0}  \\tag{2}\\]\n\nDC 工作模式性质:\n\n\\(\\alpha, \\beta\\) 参数:\\[ \\alpha_{\\text{DC}} := \\frac{I_{\\text{C}}}{I_{\\text{E}}} \\approx 0.99, \\quad h_{\\text{FE}} = \\boxed{\\beta_{\\text{DC}} := \\frac{I_{\\text{C}}}{I_{\\text{B}}}} \\approx 99\\]\nB, E 可看作导通的二极管, PN结之间存在开启电压: \\[\\boxed{V_{\\text{BE}} = V_t \\approx 0.7 \\text{ V}}\\]\n\nAC 工作模式性质:\n\n发射极动态电阻 \\[\\boxed{r'_e \\approx \\frac{V_T}{I_{\\text{E}}} = \\frac{25\\text{ mV}}{I_E}}\\]\n\\(\\beta\\) 参数: \\[ \\beta_{\\text{ac}} := \\frac{\\Delta I_{\\text{C}}}{\\Delta I_{\\text{B}}}\\]\n\n造成 \\(\\beta_{\\text{DC}} \\neq \\beta_{\\text{ac}}\\) 的原因是 \\(I_{\\text{C}}\\)-\\(I_{\\text{B}}\\) 关系是曲线:\n\n\n\n\nDC 模式相当于全局的流形, AC 模式相当于局部的切空间\n\n\n\n\n\n\n\n\n\n\n\n有关 \\(V_T\\) 和 \\(V_t\\)\n\n\n\n\n\nMental picture: 当我们将信号输入到 BJT 的 Base 时, 几乎就是 DC 信号, 波动幅度非常小!\nBJT 正常工作时, 将 BE 看作导通的二极管, 自然有电压 \\(V_t \\approx 0.7 \\text{ V}\\). \\(r'_e\\) 不能被 DC 信号感受到, 只有小信号才能感受到. 小信号需要用更精确的模型 Equation 1 来计算. Figure 6 在 \\(V_{BE} = 0.7\\text{ V}\\) 处的斜率就是 \\(r'_e\\) 的倒数: \\[\n\\text{slope}|_{V = 0.7\\text{ V}} = \\frac{I_S}{V_T} \\cdot e^{\\frac{V}{V_T}} = \\frac{1}{r'_e}\n\\]\n我们 利用 Equation 1 将 \\(e^{\\frac{V}{V_T}}\\) 用 \\(I_E\\) 表示: \\[\n\\text{slope}|_{I = I_E} = \\frac{I_S}{V_T} \\cdot \\left(\\frac{I}{I_S} + 1\\right) = \\frac{I}{V_T} + \\underbrace{\\cancel{\\frac{I_S}{V_T}}}_{\\to 0} = \\frac{1}{r'_e}\n\\]\n求得: \\[\nr'_e = \\frac{V_T}{I}\n\\]\n\\(I\\) 就是当前 E 上的直流电流大小!\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#network-analysis-网络分析",
    "href": "posts/ccd-crash-course/index.html#network-analysis-网络分析",
    "title": "CCD 通信电路速成笔记",
    "section": "3 Network Analysis 网络分析 ★★☆☆☆",
    "text": "3 Network Analysis 网络分析 ★★☆☆☆\n\n3.1 Thevenin & Norton 定理\n\n任何一个只有电压源、电流源和电阻的网络, 都可以等效为 一个有内阻的电压源 或 一个有内阻的电流源5.\n5 任何线性二端网络都可以这么等效, 原因是电源的 \\(V\\)-\\(I\\) graph 是直线! 而非理想的 电压源 和 电流源 也是直线, 二者自由度都是 \\(2\\), 因此相互等价.\n\\[\n\\boxed{\n\\begin{aligned}\nV_{\\text{TH}} &= V_{\\text{open}} \\\\\nI_{\\text{N}} &= I_{\\text{short}} \\\\\nR_{\\text{TH}} &= R_{\\text{N}} = \\frac{V_{\\text{open}}}{I_{\\text{short}}} \\\\\n\\end{aligned}\n}\n\\tag{3}\\]\n\n\n\nThevenin 等效 和 Norton 等效\n\n\n\n\n3.2 Input & Output Impedance 输入/输出阻抗\n\n一个线性网络作为负载时一定可以等效成一个电阻 (输入阻抗), 作为电源时一定可以等效成一个非理想电压源或电流源 (内阻就是输出阻抗).\n\n\n\\(R_{\\text{in}}\\) 就是该网络作为负载时两端的电压和电流之比.\n\n\n\n3.3 Conjugate Matching & Maximum Power Transfer 匹配负载与最大功率传输\n\n阻抗匹配是一个实际中要考虑的问题, 而不是在纯理论层面上的问题, 所以会觉得有点脱节的感觉.\n\n\n问题建模: Section 3.1 的结论对稳态交流电路同样适用 (见 Figure 7)! 任意复杂线性网络:\n\n从外面看都只是一个电流电压源 \\(\\tilde{V}_0\\) 和 一个内阻抗 \\(\\tilde{Z}_0\\) (相当于 Thevenin 等效).\n看外面都是一个阻抗为 \\(\\tilde{Z}_L\\) 的负载.\n问: 给定工作频率6 \\(\\omega_0\\) 和 \\(\\tilde{V}_0\\), \\(\\tilde{Z}_0, \\tilde{Z}_L\\) 满足什么的时候负载热功率最大?\n\n\n6 注意一个匹配电路只在一个工作频率下匹配, 而不是所有频率! 而我们会看到, 匹配后的频率恰好是协振频率 \\(\\omega_0\\), 所以用 \\(\\omega_0\\) 来表示工作频率.\n当内外阻抗互为共轭时, 称该电路阻抗匹配, 此时负载热功率7达到最大. \\[\\boxed{\\tilde{Z}_0 = \\tilde{Z}_L^*} \\tag{4}\\]\n\n\n\n阻抗匹配时相当于只有两个一样的纯电阻\n\n\n\n7 稳态电路功率有很多定义 (比如: 有功功率, 无功功率, 视在功率, 热功率等), 这里指热功率.\nReflection Coefficient 反射系数: 反映了负载阻抗与源阻抗的匹配程度, 定义为:\n\n\n\n\n\n\n\n\n阻抗匹配条件的证明\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: 问题建模情景\n\n\n\n设 \\(\\tilde{Z}_0 = R_0 + jX_0, \\tilde{Z}_L = R_L + jX_L\\), 负载的热功率: \\[\n\\begin{aligned}\nP_L &= I_{\\text{rms}}^2 R_L \\\\\n&= \\frac{1}{2} |\\tilde{I}|^2 R_L \\\\\n&= \\frac{1}{2} \\left| \\frac{\\tilde{V}_0}{\\tilde{Z}_0 + \\tilde{Z}_L} \\right|^2 R_L \\\\\n&= \\frac{1}{2} \\frac{|\\tilde{V}_0|^2}{(R_0 + R_L)^2 + (X_0 + X_L)^2} R_L \\\\\n\\end{aligned}\n\\]\n要 \\(P_L\\) 最大, 分母必须最小, 显然当 \\(X_0 + X_L = 0\\) 时即 Equation 4 时取最小."
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#amplifier-circuit-放大电路分析",
    "href": "posts/ccd-crash-course/index.html#amplifier-circuit-放大电路分析",
    "title": "CCD 通信电路速成笔记",
    "section": "4 Amplifier Circuit 放大电路分析 ★★★★☆",
    "text": "4 Amplifier Circuit 放大电路分析 ★★★★☆\n\n4.1 分析纲领 ★★★★★\n\n一个放大电路对 DC 和 AC 的作用是单独的, 我们先分开分析, 实际上的情况是两个分析结果的叠加!\n\n\n\n\n\n\n电容通交隔直\n\n\n\nDC 和 AC 的情况下, 放大电路的等效电路是不一样的.\n我们只需要考虑等效电路, 电容和电感在等效电路中是不存在的.\n\nDC 分析:\n\n电容开路, 电感短路8.\n画 Thevenin 等效电路\n\n\n\n\n\n\nFigure 8: DC等效电路 及其Thevenin等效\n\n\n\n\nAC 分析:\n\n电容短路, 电感开路.\n\\(V_{\\text{CC}}\\) 相当于接地 (任何不变的信号都相当于接地)\n电压、电流默认取 RMS \\(V_{\\text{rms}}\\) 计算 (峰值也行其实), \\(\\boxed{V_p = \\sqrt{2} V_{\\text{rms}}}\\)\nBypass Capacitor: 与某个电阻 \\(R\\) 并联的电容, 使得交流信号在至少 \\(f_{\\min}\\) 频率下看起来无阻抗, \\(C\\) 必须足够大 (\\(X_C\\) 必须足够小): \\[\\boxed{X_C \\le \\frac{R}{10}}\\]\n\n将等效电路中的 BJT 换成 Figure 4 (b) 中的模型, 然后当作正常电路分析即可.\n\n\n8 理想电容记为 \\(C_{\\infty}\\), 理想电感记为 \\(\\text{RFC}\\) (Radio Frequency Choke).\n最后 DC 和 AC 的结果叠加起来, 注意 这时候 \\(V_{\\text{CC}}\\) 不能接地了9, 所有元件都正常.\n\n\n9 特别是在分析为什么 Common Emitter 相位是 180 度的时候, \\(V_\\text{C} = V_{\\text{CC}} - I_\\text{C} R_\\text{C}\\), \\[V_B \\uparrow \\implies I_B \\uparrow \\implies I_C \\uparrow \\implies V_\\text{C} \\downarrow\\]\n\n\n\n\n\nEXAMPLE: Thevenin 等效电路\n\n\n\n\n\n\n\n\n求解 Figure 8\n\n\nFigure 8 中 \\(A\\) 点电压不能直接分压求解! 虽然 \\(I_{\\text{A}}\\) 很小: \\[ V_A \\neq \\frac{R_2}{R_1 + R_2} V_{\\text{CC}}\\]\n要用 Thevenin 等效电路求解!\n\n\n\n\n\n\n\n\n\n\n\nEXAMPLE: Bypass Capacitor 大小的计算\n\n\n\n\n\n\n\n\nBypass Capacitor 大小的计算"
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#mixer-circuit-混频电路",
    "href": "posts/ccd-crash-course/index.html#mixer-circuit-混频电路",
    "title": "CCD 通信电路速成笔记",
    "section": "5 Mixer Circuit 混频电路 ★★★☆☆",
    "text": "5 Mixer Circuit 混频电路 ★★★☆☆\n\n目的: 其实就是完成了调制和解调的时候将两个正弦信号 \\(v_1, v_2\\) 乘在一起的过程\n\n\\(v_1 = V_1 \\cos(\\omega_1 t), v_2 = V_2 \\cos(\\omega_2 t)\\)\n一般要结合滤波器保留需要的频率成分 (上边带 \\(\\omega_1 + \\omega_2\\) 或 下边带 \\(|\\omega_1 - \\omega_2|\\), 为了区分 Rejection 滤波, 姑且叫做 “Second” 滤波10).\n\n由于产生了新的频率成分, 需要引入二极管之类的非线性元件.\n\n10 我自己起的名字, 只是因为他在 Rejection 滤波器之后.\n5.1 Common Mixer 常见混频器\n\n5.1.1 Diode Mixer 二极管混频器\n\n\n\n\n\n\nFigure 9\n\n\n\n\nFigure 9 1 点电压 \\(V_1 = (v_1+v_2)/2\\), 也可以简单地串联 \\(v_1, v_2\\).\nDiode Mixer 要求 \\(v_1, v_2\\) 很小 (所以 \\(i_D\\) 也很小), 二极管要用 Shockley 方程 来建模!\n由 Taylor 展开 Shockley 方程, \\(i_D\\) 有 \\(v_D, v_D^2, v_D^3, \\cdots\\) 成分, 分别对应:\n\n一次谐波: \\(\\omega_1, \\omega_2\\)\n二次谐波: \\(2 \\omega_1, 2 \\omega_2, \\omega_1 + \\omega_2, |\\omega_1 - \\omega_2|\\)\n更高次谐波 …\n\n\n\n\n\n\n\n二极管混频器输出的频谱\n\n\n\n\n5.1.2 BJT & JFET Mixer 晶体管混频器\n\n\n\n\n\n\nFigure 10: 以 BJT Mixer 为例\n\n\n\n\nBJT 的 B, E 间其实是隐藏的二极管 (见 Section 2.3.2)\nBJT Mixer 自带放大效果 (输出的电流可以比较大).\nFigure 10 的 \\(L, C\\) 是带通滤波器 (“Second” 滤波)\n若换成 JFET Mixer, 不再满足 Shockley 方程, 而是 Transfer curve 的二次关系 (Figure 3), 没有高次谐波成分了, nice!\n\n\n\n5.1.3 Dual-Gate MOSFET Mixer\n(Omitted)\n\n\n5.1.4 Ring Modulator 环形调制器\n\n\n\n环形调制器输入 \\(e_i\\) 输出 DSB-SC 调制信号, 载波一般用矩形波\n\n\n\n\n\n5.2 Image Frequency 镜像频率\n\n混频器输出的二次谐波里的 \\(|\\omega_1 - \\omega_2|\\) 项不仅有希望的 \\(f_{\\text{RF}}\\) 频率, 还有可能存在的 镜像频率 \\(f_{\\text{image}}\\).\nImage frequency rejection: 当我们 mixing 后需要保留 \\(|\\omega_1 - \\omega_2|\\) 成分时, 我们不希望受到 \\(f_{\\text{image}}\\) 的干扰, 所以加一个 BPF 来滤掉它 (Figure 11)\n\n\n\n\n\n\n\n\n\nFigure 11: Rejection 滤波器例子\n\n\n\n\n\n\n\n\n\nFigure 12: 用带通滤波器滤掉可能存在的镜像频率, 定量分析见 Section 6"
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#sec-rlc-filter",
    "href": "posts/ccd-crash-course/index.html#sec-rlc-filter",
    "title": "CCD 通信电路速成笔记",
    "section": "6 RLC 电路作为滤波器",
    "text": "6 RLC 电路作为滤波器\n\n这门课中 含有 RLC 这个电路整体的地方包括: Image Rejection 滤波器, 振荡器的反馈电路, PLL 的 loop filter 低通滤波, 我们单独来看一看.\n\n\nHigh Frequency Effects/Parasitic Effects 高频/寄生效应: \\(R, L, C\\) 元件甚至导线在高频时表现不再理想, 都有了寄生电容和寄生电感. 但这超出了电路的集总 (lumped) 假设, 一般不讨论.\n\n\n\n\n\n\n\\(L\\) 在高频下变成 \\(RLC\\) 电路了\n\n\n\n6.1 Resonance 协振\n\nTank circuit: 并联 \\(LC\\) 电路中是有储存的能量的, 所以称为 Tank.\nFlywheel effect: \\(LC\\) 电路中 \\(L\\) 和 \\(C\\) 之间有能量交换 (用弹簧振子类比, 弹性势能和动能之间的交换).\n协振时 \\(LC\\) 串联电路相当于短路, \\(LC\\) 并联电路相当于断路. 即电路表现为纯电阻, 即: \\[\\boxed{\\Im{(Z)} = 0, \\quad \\text{or } \\Im{(Y)} = 0}\\]\n\n对 \\(LC\\) 电路, \\[Z = \\frac{1}{Cj\\omega_0} + Lj\\omega_0 = 0 \\implies \\boxed{\\omega_0 = \\sqrt{\\frac{1}{CL}}} \\tag{5}\\]\n对 \\(RLC\\) 电路 (Figure 13), 算 \\(Y\\) 更方便: \\[Y = \\frac{1}{R + Lj \\omega_0} + Cj \\omega_0 = 0 \\implies \\omega_0 = \\sqrt{\\frac{1}{CL}-\\frac{R^2}{L^2}}\\]\n\n\n\n\n\n\n\n\n\n\nFigure 13: 对于 \\(Q&gt;10\\) 的 \\(RLC\\) 电路可以忽略 \\(R\\)\n\n\n\n\n\n6.2 Q-factor 品质因数\n\nQ-factor 在一阶系统和二阶系统都有定义, 但不太一样. 先是在二阶系统上定义, 再形式化地迁移到了一阶系统上.\n\n\n6.2.1 二阶系统 (\\(C, L\\) 都存在)\n\nQ-factor 衡量了一个二阶电路的滤波品质. \\(Q\\) 越大, 滤波器的频率选择性越好.\n\n\n\n\n\n\n\\(Q\\) 越大, 滤波器的频率选择性越好\n\n\n\nDissipation \\(D := 1/Q.\\)\n等价定义(或推论):\n\n二阶系统 (如弹簧振子模型) 在共振(协振 resonance)条件下 (\\(\\omega_0\\)), 系统储存的能量与一个周期内消耗的能量 (在 \\(R\\) 上) 之比, 的 \\(2\\pi\\) 倍: \\[ Q := 2\\pi \\cdot \\frac{ \\text{Energy stored}}{\\text{Energy dissipated in one period}}\\]\n串联协振 (记忆: 串联电流主导, \\(R\\) 在分母, \\(R\\) 越大越耗能, \\(Q\\) 越小): \\[ \\boxed{Q = \\frac{\\omega_0 L}{R} = \\frac{1}{\\omega_0 C R}} \\tag{6}\\] 并联协振 (记忆: 并联电压主导, \\(R\\) 在分子, \\(R\\) 越大泻能越慢, \\(Q\\) 越大): \\[ \\boxed{Q = \\frac{R}{\\omega_0 L} = \\omega_0 C R} \\tag{7}\\]\n不管串/并联协振, \\(Q\\) 都可由 中心频率和 \\(-3\\text{ dB}\\) 带宽相同地表示 : \\[\\boxed{Q = \\frac{\\omega_0}{\\Delta \\omega}}, \\Delta \\omega = \\omega_2-\\omega_1, \\quad \\text{when } 20\\lg|H(\\omega)| = -3\\text{ dB} \\tag{8}\\]\n\\(\\omega_0\\) 是 \\(\\omega_1\\) 和 \\(\\omega_2\\) 的几何平均: \\[\\boxed{\\omega_0^2 = \\omega_1 \\cdot \\omega_2}\\] 但是实际上很接近算数平均, 所以可以这样算 \\(\\omega_1, \\omega_2\\): \\[ \\begin{cases} \\omega_1 &= \\omega_0 - \\frac{\\Delta \\omega}{2} \\\\ \\omega_2 &= \\omega_0 - \\frac{\\Delta \\omega}{2}\\end{cases}\\]\n\n\n\n\\(\\omega_0\\) 是 \\(\\omega_1\\) 和 \\(\\omega_2\\) 的几何平均\n\n\n\n\n\n\n「半功率」点 \\(-3\\text{ dB}\\) 对应的 \\(\\omega\\) 叫 截止频率, \\(|H|^2 = 0.5\\).\n\n\n6.2.2 一阶系统 (\\(RC\\) 或 \\(RL\\) 电路)\n\n直接将 Equation 6 和 Equation 7 的 \\(\\omega_0\\) 形式化地替换成当前的频率 \\(\\omega\\), 所以一阶电路的 \\(Q\\) 不再固定, 跟工作频率有关.\n\n\n\n\n\n\n\nFigure 14: 电容和电感可以进一步抽象为纯虚数的电抗 \\(X\\)\n\n\n\n\nSerial-Parallel Conversion\n\n动机: Figure 14 中两种电路的总体来说就是一个阻抗 \\(Z_s, Z_p\\), 给定频率 \\(\\omega\\), \\(R_s, R_p\\) 和 \\(X_s, X_p\\) 要满足什么条件他们可以完全等效, i.e., \\(Z_s = Z_p\\)?\n结论: 等效时, 并联的电阻和电抗都要大一些: \\[\n  \\boxed{\n  \\begin{aligned}\n  \\frac{R_p}{R_s} &= 1+Q^2 \\\\\n  \\frac{X_p}{X_s} &= 1+\\frac{1}{Q^2}\n  \\end{aligned}}\n   \\tag{9}\\]\n\n\n\n\n\n\n\n\n\nEquation 9 的推导\n\n\n\n\n\nFigure 14 中串联和并联的阻抗 \\(Z_s, Z_p\\) 为: \\[\n\\begin{aligned}\nZ_s &= R_s + j X_s \\\\\n\\frac{1}{Z_p} &= \\frac{1}{R_p} + \\frac{1}{j X_p}\n\\end{aligned}\n\\]\n令 \\(Z_s = Z_p\\), 实部与虚部分别相等, 再代入 Figure 14 中的公式即可: \\[\n\\begin{aligned}\n&\\frac{1}{R_s + j X_s} = \\frac{1}{R_p} + \\frac{1}{j X_p} \\\\\n&\\frac{R_s - jX_s}{R_s^2 + X_s^2} = \\frac{1}{R_p} - j \\frac{1}{X_p} \\\\\n\\implies &\\begin{cases} \\Re = \\frac{R_s}{R_s^2 + X_s^2} = \\frac{1}{R_p} \\implies \\frac{R_p}{R_s} = 1 + \\frac{X_s^2}{R_s^2} = 1 + Q^2 \\\\ \\Im = \\frac{X_s}{R_s^2 + X_s^2} = \\frac{1}{X_p} \\implies \\frac{X_p}{X_s} = 1 + \\frac{R_s^2}{X_s^2} = 1+\\frac{1}{Q^2} \\end{cases}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n6.3 RLC 电路作为 Image Rejection 滤波器\n将 Figure 11 作为一个整体, 其 admittance (导纳) \\(Y\\) 跟输入频率有关, 算出来是: \\[\n\\boxed{\n|Y| = Y_0 \\sqrt{1+(\\delta Q)^2}\n}\n\\] 其中 \\(Y_0\\) 是电路在协振 \\(\\omega_0\\) 时的导纳. \\(\\delta := \\frac{\\omega}{\\omega_0} - \\frac{\\omega_0}{\\omega}\\), \\(Q\\) 是品质因数.\n给 \\(a, b\\) 输入相同的电流 phasor \\(\\tilde{I}\\), \\(\\tilde{V}_{ab}\\) 的长度代表了滤波后的电压信号振幅, 由 \\(|\\tilde{V}_{ab}| = |\\tilde{I}| / |Y|\\) 知: 滤波器的输出反比于 \\(|Y|\\).\n以 \\(\\omega_0\\) 点为参考, 定义滤波器在 Figure 12 的 \\(\\omega\\) 点的 attenuation \\(|A_r|\\) 为 (一般还要化成 \\(\\text{dB}\\) 单位): \\[\n\\boxed{\n|A_r| := \\left|\\frac{\\tilde{V}_\\omega}{\\tilde{V}_{\\omega_0}}\\right| = \\frac{Y_0}{Y} = \\frac{1}{\\sqrt{1+(\\delta Q)^2}}\n}\n\\]\n\n\n\n\n\n\n\nEXAMPLE: RLC Rejection 滤波器 attenuation 的定量分析\n\n\n\n\n\n\n\n\n\n\n\n\n6.4 RLC 电路作为振荡器的反馈电路 ★★★★★ (See Section 7.2)\n\n6.4.1 Phase-shift oscillator 只有 \\(RC\\) 的反馈\n\nFigure 15 结构的振荡器输出的中心振荡频率 \\[\\boxed{\\omega_0 = \\frac{1}{\\sqrt{6}RC}} \\tag{10}\\]\n\n\n\n\n\n\n\nFigure 15: Phase-shift oscillator 的反馈电路结构\n\n\n\n\n\n\n\n\n\n\nEquation 10 推导过程\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Phase-shift oscillator 的中心频率的推导过程\n\n\n\n注意: 不能用三个 \\(RC\\) LPF 的连级来计算!, 因为每加一个 \\(RC\\) 就相当于加了负载 (见 Section 1), 会影响之前的电路的滤波性质:\n\n\n\n加 Buffer 才能用连级的思想\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXAMPLE: 用 Equation 10 来计算输出频率\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2 Tapped network 含 \\(RLC\\) 的反馈\n\nTapped network: 一个电路从中间某两个部分引出两条线11 (称为 “tap”) 分别作为输入和输出后形成的二端网络.\n11 比如滑动变阻器的滑片可称为 “tap”.\n\n\n\n\n\n\n\n\nFigure 17: Tapped network 例子\n\n\n\n\n根据所用的不同 Tapped network (Figure 17), 给振荡器起了不同名字 (仅仅是起个名字): Hartley, Colpitts, etc.\n这里题目一般只会叫你算中心频率 \\(\\omega_0\\), 只要将他给的电路进行 AC 等效, 然后与 Figure 19 进行对比找出 \\(\\beta(s)\\) 的部分, 这个部分含有一个 \\(LC\\) 协振电路, 是一个 BPF, 所以其协振频率就是振荡器的输出频率 \\(\\omega_0\\)!12\n\n\n12 这也是为什么协振频率和振荡器的输出频率是都用 \\(\\omega_0\\) 表示的原因.\n\n\n\n\n\nEXAMPLE: 找出下面振荡器的反馈部分 并算出其输出频率\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHartley: \\(\\omega_0 = 1/\\sqrt{(L_1+L_2)C}\\)\n\n\n\n\n\n\n\nHartley: 跟左边一样\n\n\n\n\n\n\n\n\nColpitts: \\(\\omega_0 = 1/\\sqrt{[C_1C_2/(C_1+C_2)]L_1}\\)\n\n\n\n\n\n\n\n\n\n6.5 \\(RC\\) 电路作为 PLL 的 Loop Filter\n\n\n\n\n\n\n\n\nFigure 18: \\(RC\\) 低通滤波器\n\n\n\n\nFigure 18 的传递函数为: \\[F(s) = \\frac{1}{1+RCs}\\] 其 截止频率 \\(\\omega_p\\) 来自「半功率」点: \\[|F(s)|^2 = \\frac{1}{1+(RC\\omega_p)^2} = \\frac{1}{2} \\implies \\boxed{\\omega_p = \\frac{1}{RC}}\\]"
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#oscillator-circuit-振荡器",
    "href": "posts/ccd-crash-course/index.html#oscillator-circuit-振荡器",
    "title": "CCD 通信电路速成笔记",
    "section": "7 Oscillator Circuit 振荡器 ★★★☆☆",
    "text": "7 Oscillator Circuit 振荡器 ★★★☆☆\n\n程序的执行需要时钟, 我们需要一个电路, 它可以自发持续稳定 (而不是像 \\(LC\\) 电路被动地) 地输出一个周期信号 (e.g., 正弦波), 称为振荡器. 如果频率还能调整的话就更好 (VCO, Voltage Controlled Oscillator).\n\n\n7.1 电路振荡要满足的条件\n\n\n\n\n\n\nFigure 19: 用正反馈来产生自发振荡\n\n\n\n\n为什么会自发震荡?\n\n噪声不可避免\n噪声中含有所有的频率, 通过线性系统有制造出任意想要的频率的潜力.\n我们希望小噪声在正反馈系统里不断被「放大」13 (用 \\(A(s)\\)), 而且能够筛选出我们想要的频率 \\(\\omega_0\\) (用 \\(\\beta(s)\\), \\(\\omega_0\\) 是振荡器的自发频率, 称为中心频率或工作频率, 是反馈 \\(LC\\) 电路中的协振频率).\n\nBarkhausen Stability Criterion 巴克豪森准则: Figure 19 要振荡, 必须满足 (但不保证振荡): \\[\n\\boxed{\n\\begin{cases} |A \\beta| \\ge 1 \\\\ \\angle (A \\beta) = 0 \\end{cases}\n}\n\\]\n\n\n13 在 Section 1 中我们提到过「信号是复值的」, 这意味着「放大」这个词不一定代表指数增长的信号, 也代表正弦信号, 指数信号和正弦信号的在复数看来本质是一样的.\n\n\n\n\n\nBarkhausen 准则的理解\n\n\n\n\n\n一个小噪声在 \\(\\omega\\) 频点每经过一次系统回来后都被「放大」了: \\[\nH(j \\omega) = A(j \\omega) \\beta(j \\omega)\n\\]\n那这个信号一遍又一遍经过系统出来以后被「放大」了: \\[\n1 + H(j \\omega) + H(j \\omega)^2 + H(j \\omega)^3 + \\cdots\n\\tag{11}\\]\n这是一个等比数列, 要想震荡, 这个值一定不能收敛, 也就是公比不能太小: \\[\n|H(j \\omega)| \\ge 1\n\\]\n另外回来的相位必须跟原来同向 (相位差是零, \\(\\angle(A \\beta) = 0\\)), 不然 Equation 11 的每一项就会均匀分布在复平面上, 会被平均掉.\n真实的过程就是 Equation 11 的求和是很快地一项一项求和 (因为反馈是需要时间的, 如果反馈不需要时间就不会震荡了, 如果宇宙是计算机的话, 直接 run time error), 如果发散的话就一直来回变化怎么也停不下来, 也就是在时间上震荡, 如果我们直接列方程求解的话是想一步到位求出稳态 (见 Section 1), 但是它收敛不了, 最后的结果就是无穷大.\n如果我们要输出固定的频率的话, 只需要在反馈的过程中让 \\(\\beta(s)\\) 筛选出我们想要的频率就行了 (BPF).\n\n\n\n\n\n\n7.2 设计正反馈电路\n\n现在要设计 Figure 19 中的反馈电路 \\(\\beta(s)\\), 目标是要跟 \\(A(s)\\) 一起满足 Barkhausen 准则, 而且要滤出所需震荡频率 \\(\\omega_0\\). 简单起见, 就用 \\(LC\\) 电路吧.\n\n\n这个正反馈电路可以只含有 \\(RC\\) 元件, 也可以利用 \\(RLC\\) 的协振, 所有内容见 Section 6.4.2.\n\n\n\n7.3 VCO (Voltage Controlled Oscillator) 电压控制振荡器\n\nVCO 接收一个电压14 \\(V_o\\), 输出一个频率与之 近似线性 的周期信号.\n14 Notation 开始变得抽象了, 用 \\(V_o\\) 是因为 PLL 里面 VCO 的输入是 LPF 的输出, 所以用 out.\n\n\n\n\n\n\n\n\nFigure 20: VCO 输入输出近似线性 (见 Equation 12)\n\n\n\n\nVaricap diode 可变电容: 这种电容的电容值 \\(C_D\\) 可通过外界输入的电压 \\(V_D\\) 来调节 (\\(C_0 \\approx 18\\text{ pF}\\) 是 \\(V_D\\) 等于零的电容值): \\[\\boxed{C_D = \\frac{C_0}{\\sqrt{1 + \\frac{|V_D|}{0.5}}}}\\]\n\nVCO 就是用了 varicap diode 的自发 oscillator, 通过输入电压, 改变电容值, 进而通过 Equation 5 改变振荡频率.\n\n\n\n\n7.4 PLL (Phase Locked Loops) 锁相环 ★★★★★\n\n锁相环可以看作 VCO 的应用, 它接收一个周期信号, 输出一个与之相位同步 (当然频率也要一样) 的周期信号 (正弦波或方波).\n\n\n\n\n\n\n\nFigure 21: PLL 都有的结构, LPF 也叫 Loop Filter\n\n\n\n\nPLL 工作的几个阶段 Stages:\n\nFree running: 没有输入, PLL 也会有输出, 频率是 VCO 的中心频率 \\(\\omega_0\\).\nCapture: 突然输入一个周期信号, 只有当这个信号的频率接近 \\(\\omega_0\\) 时 (具体来说, 要在 Capture range 捕获范围内), 输出才能锁定到输入.\nLock: 输出锁定到输入. 而且我们可以改变输入频率, 但是不能超过 Lock range 锁定范围 (大于捕获范围!), 不然失去锁定.\n\n\n\n\n\n\n\nPLL 的锁定范围大于捕获范围\n\n\n\nFigure 21 中模块的细节15:\n\nPhase Detector 鉴相器 (PD): 输入两个周期信号 (\\(v_i(t), v_{\\text{osc}}(t)\\)), 输出他们的某种运算后的信号 \\(v_{\\varphi}(t)\\) (一般是乘积 (用 Mixer), 或 \\(XOR\\))\n\nPhase Detector Gain 鉴相器增益16: \\[\\boxed{V_o = \\textcolor{green}{{K_D}} \\Delta \\varphi}\\]\n\\(K_D\\): 增益, \\(\\Delta \\varphi\\): PD 输入的两路信号的相位差.\n\nLoop Filter 低通滤波器: 对 \\(v_{\\varphi}(t)\\) 进行平滑 (滤波), 变成近似直流信号 \\(V_o\\), 这个电压反应了 \\(v_i(t)\\) 和 \\(v_{\\text{osc}}(t)\\) 的相位差. (具体电路见 Section 6.5)\nVCO\n\nFrequency sensitivity (gain) 频率灵敏度 (增益)17: \\[\\boxed{\\omega_{\\text{osc}} = \\omega_0 + \\textcolor{green}{K_O} V_o} \\tag{12}\\]\n\\(K_O\\): 频率灵敏度\n输入为 \\(0\\) 时也有频率 \\(\\omega_0\\) (Figure 20).\n\nLoop Gain 总增益: \\[\\boxed{\\textcolor{green}{{K_V}} := \\textcolor{green}{{K_D K_O}}}\\]\n\n\n15 抽象 Notation 来了, 大家坐稳.16 准确来说应该是 PD 和 LPF 的组合增益. 这个组合的输入「被隐性地认为」是相位差 (而不是输入信号本身), 输出就是正常的电压.17 开始 AoN 了, 如果 VCO 的输入是电压, 输出是正弦波, 增益应该是输出比输入, 但 \\(K_O\\) 的定义中输出好像变成了频率而不是信号值本身了. 这样的 AoN 在 PLL 中很多, 要注意说增益的时候输入、输出「被隐性地认为」是什么: 信号值, 频率还是相位!\n7.4.1 4046 PLL\n\n\n\n4046 PLL 是一种只能输入和输出方波的锁相环 (因为用了 \\(XOR\\) 做 PD)\n\n\n\nVCO 增益可以通过 \\(C_1, R_1, R_2\\) 来调节:\n\n为了正常工作, 必须满足18: \\(\\boxed{100\\text{ pF}\\le C_1 \\le 100\\text{ nF}, 10\\text{ k}\\Omega \\le R_{1,2} \\le 1\\text{ M}\\Omega}\\)\nFigure 20 中的 \\(\\omega_{\\min}\\) 和 \\(\\omega_{\\max}\\) 也可调19: \\[\n  \\boxed{\n  \\begin{aligned}\n  \\omega_{\\min} &= \\frac{1}{R_2(C_1 + 32\\text{ pF})} \\\\\n  \\omega_{\\max} - \\omega_{\\min} &= \\frac{1}{R_1(C_1 + 32\\text{ pF})}\n  \\end{aligned}\n  }\n  \\]\n\n\n18 这 sb 玩意儿都要背, 真是佛了.19 ppt 上写的是 \\(f\\) 而不是 \\(\\omega\\), 但用 \\(f\\) 明显单位不对啊, 牛魔的."
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#modulation-demodulation-调制解调",
    "href": "posts/ccd-crash-course/index.html#modulation-demodulation-调制解调",
    "title": "CCD 通信电路速成笔记",
    "section": "8 Modulation & Demodulation 调制解调 ★★☆☆☆",
    "text": "8 Modulation & Demodulation 调制解调 ★★☆☆☆\n\nModulation 调制是用某个高频 (e.g., \\(1\\text{ GHz}\\)) 的 carrier 载波信号 的某个特性 (幅度, 频率, 相位) 来表示一个低频 (\\(200\\sim 3\\text{ kHz}\\)) 的 intelligence / message 信息信号 的过程.\n\n\n不调制的问题:\n\n信号频率都相近, 信号间会 interference 干扰.\n低频信号传输需要几千米长的天线\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\n\n\nIntelligence 信息信号20: \\(e_i = E_i \\cos(\\omega_i t)\\) (若不是正弦信号, 则约定 \\(E_i := |\\min(e_i)|\\))\nCarrier 载波信号21: \\(e_c = \\cos(\\omega_c t)\\)\nModulated 调制信号: \\(e\\)\n\n\n\n\n\n21 这里用 normalized 归一化的载波信号, PPT 上是 \\(e_c = E_c \\cos(\\omega_c t)\\), 但是后面乘载波的时候又用的是 \\(\\cos(\\omega_c t)\\), 回字的四种写法, 研究这个没有意义.20 注意跟 modulated 调制信号区分开来.\n8.1 AM 调幅\n\nAM 调制信息在幅度里面. 而且有很多种 AM 的方法22: DSB-SC (omitted), DSB-WC, SSB, VSB, etc.\n22 Abbr. Double/Single/Vestigial (残留) Sideband- Suppressed/With Carrier.\n\n8.1.1 AM 方法1: DSB-SC\n\n\n\n\n\n\n\nDSB-SC 调制框图古早笔记\n\n\n\n\n\n\n\n\n\n\n\nFigure 22: DSB-SC 调制框图 (省略所有放大器)\n\n\n\n\n\n\n\n\nFigure 22 中的乘法器一般用 Ring Modulator 环形调制器 (见 Section 5.1.4).\n\n生成 SC 调制信号的调制器都叫 Balanced Modulator 平衡调制器, Ring Modulator 是 Balanced Modulator 的一种.\n\n\n\n\n8.1.2 AM 方法2: DSB-WC\n\n目标: 调制信号的 包络 要能还原原始信息信号的波形.\n\n\n\n\n\n\n\nFigure 23: DSB-WC 调制框图 (省略所有放大器)\n\n\n\n\nDSB-WC (Double Sideband With Carrier) 调制出来的信号: \\(e(t) = (e_i(t) + E_c) \\cdot e_c(t)\\)\nModulation Index 调制指数23: (见 Figure 23) \\[\\boxed{m_a := \\frac{E_i}{E_c}}\\]\n\nUnder-modulation: \\(m_a \\le 1\\), 有效的调制.\nOver-modulation: \\(m_a &gt; 1\\), 包络会被削顶, 无法还原原始信息信号的波形.\n\n\n23 这是一个典型的工程上一拍脑袋想出来的鸡肋度量, 三种调制都有这个概念, 但都不能用统一起来, 极不富有美感.\n\n\n\n不同的调制指数\n\n\n\n\nTransmitted Power 传输功率: 当 \\(e_i\\) 为正弦信号时我们有: \\[\\boxed{P_t = P_c \\left(1 + \\frac{m^2}{2} \\right)} \\tag{13}\\]\n\n载波信号的功率 \\(P_c = E_c^2/2\\)\n传输载波的功率至少占了 \\(2/3\\) (\\(m = 1\\) 时).\n若将 \\(e_c\\) 和 \\(e\\) 功率信号分别接到天线 (视为一个电阻 \\(R\\)) 上, 产生了电流 \\(I_c, I_t\\). 由 \\(P = I^2 R = V^2/R\\) 知, 功率大的信号 \\(e_c\\) 产生了更大的电流和电压 (正比于幅值 \\(E\\)): \\[\\boxed{I_t = I_c \\sqrt{1 + \\frac{m^2}{2}}, \\quad E_t = E_c \\sqrt{1 + \\frac{m^2}{2}}}\\]\n\n\n\n\n\n\n\n\n\nEquation 13 的推导\n\n\n\n\n\nFigure 23 中调制信号 \\[\n\\begin{aligned}\ne &= (e_i + E_c)e_c  \\\\\n&= E_c (m \\cos(\\omega_i t) + 1) \\cdot \\cos(\\omega_c t) \\\\\n&= \\underbrace{E_c \\cos(\\omega_c t)}_{P_1=\\frac{E_c^2}{2}} + \\underbrace{\\frac{E_c m}{2} \\left[\\cos((\\omega_c + \\omega_m)t) + \\cos((\\omega_c - \\omega_m)t)\\right]}_{P_2=\\left(\\frac{E_c m}{2}\\right)^2}\n\\end{aligned}\n\\]\n总传输功率 \\[\nP_t = P_1 + P_2 = \\frac{E_c^2}{2} \\left(1+\\frac{m^2}{2}\\right)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh / Low Level Modulation 高/低电平调制: 实际调制过程中 在输入和调制信号发射之间 肯定包含信号的放大 (Figure 23 省略了), 是先调制再放大 还是 先放大再调制呢? (都可以)\n\nHigh Level Modulation: 先放大再调制, 适合高功率要求24, 如radio broadcasting 广播塔台.\nLow Level Modulation: 先调制再放大, 适合功耗低、低功率要求, 如通信模块.\n\n\n24 为什么不能都用这种调制方法? 有一类放大器 (Class C) 对纯载波 \\(e_c\\) 的功率放大效果非常好, 但是是非线性的, 所以不能用于放大 \\(e\\).\n\n8.1.3 AM 方法3: SSB ★★★★★\n\n动机: 我们发现 (Figure 23) 在 DSB 中调制信号的频谱上有两份原始信息, 为了减小带宽资源, 可以用 SSB Generator25 只传输一份 (USB 或 LSB26) 就可以了, 这就是单边带调制.\n25 SSB Generator 的实现方法包括: phase shift, selective filtering (包括 One-step 和 Two-step 版本), Weaver’s method (“Third Method”), etc.26 Abbr. Upper/Lower Sideband.\n\n\n\n\n\n\nFigure 24: SSB 简化的调制框图 (省略所有放大器)\n\n\n\n\nSSB 与 SSB-SC27: SSB 不传输载波, 但 SSB-SC 传输一个微弱载波! 我没写反.\n\nSuppressed Carrier 又称 Pilot Carrier (导频载波), 可进行同步.\n\n\n27 这样命名是生怕老子学会吗, 不知道你佛不佛, 反正我是佛的.\nSSB 的优点:\n\n节省了传输载波和其中一个边带的功率\n接收端噪声能量少. 因为 SSB 的带宽小, 接收端滤波器带宽也会小, 而噪声功率 \\(\\propto\\) 带宽.\n抗选择性衰落 (Selective Fading). Figure 22 双边带解调端信号的重建用到两个边带的叠加, 不同频的边带从电离层 (ionosphere) 反射回来后折射角和路径不同 (选择性衰落), 使得两个边带不对称, 重建信号失真. 而 SSB 只用一个边带, 不受选择性衰落影响.\n\nSSB Selective Filtering:\n\n是 SSB 调制信号产生的其中一种电路实现.\n原始信号必须要有 Band Gap \\(\\Delta f\\). (比如人声音一般大于 \\(100\\text{ Hz}\\), 所以人声信号 \\(\\Delta f = 200\\text{ Hz}\\), 见 Figure 26)\n需要一个高 roll-off 的滤波器 (原因是 Band Gap 很小), 其 \\(Q\\)-factor 为: \\[\\boxed{Q = \\frac{f_c \\sqrt{|H|}}{4 \\Delta f}}\\] 其中 \\(|H|\\) 是该滤波器的传递函数在 Rejected Sideband 上的取值 (一般告诉你多少 \\(\\text{dB}\\)).\n(Two-step) SSB Transmitter 发射机:\n\n动机: Cut-off frequency 很高的滤波器很难有很高的 roll-off. 传输的频率一般要达到 \\(\\text{MHz}\\), 如果将 \\(e_i\\) 直接搬移到这个频率, 很难造出高 roll-off 的滤波器进行 SSB filtering. 但是低截止高 roll-off 的滤波器还是有的. 所以我们先将 \\(e_i\\) 用这个滤波器搬移到一个低的频段, 用低截止高 roll-off 的滤波器滤波 (相当于增大 Band Gap), 再用一个高截止低 roll-off 的滤波器进行真正的 SSB Filtering. 即先增大 Band Gap, 再 SSB Filtering.\n\n\n\n\n\n\nFigure 25: SSB “Two-step Method” 调制过程, 信号频谱变化 Figure 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXAMPLE: SSB Selective Filter 的 \\(Q\\) 计算\n\n\n\n\n\n\n\n\n\n\n\nFigure 26: \\(\\log^{-1} x = 10^x\\) 抽象符号\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-step SSB 调制框图 (Figure 25) 详细过程古早笔记\n\n\n\n\n\n\n\n\n\n\n\nFigure 27: Two-step SSB 调制信号图\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXAMPLE: Two-step 方法中第二个滤波器的 \\(Q\\)-factor 计算\n\n\n\n\n\nHINT: 用 Equation 8 来算 \\(Q\\). 说实话说这个中心频率应该取 \\(3.1\\text{ MHz}\\) 或 \\(2.9\\text{ MHz}\\), 这道题不好\n\n\n\n“In the previous slide” 指 Figure 25\n\n\n\n\n\n\n\n\n8.1.4 *AM 方法4: VSB {#sec-vsb}\n\nVSB 提供了针对 SSB 中高截止、高 roll-off 的滤波器难以实现的问题, VSB 滤波器不要求高 roll-off, 只需满足一些条件 (易于满足, 见 Figure 28) 即可. SSB 可看作 VSB 的特例.\n\n\n\n\n\n\n\n\nVSB 调制框图古早笔记\n\n\n\n\n\nVSB 很聪明地利用了解调的时候两边的频谱会合起来的特点, 不需要他们完全不重合, 只需要他们重合的部分加起来刚好是原来的频谱就行.\n\n\n\n\n\n\nFigure 28: VSB 调制框图 (省略所有放大器)\n\n\n\n\n\n\n\n\n\n\n8.2 PM & FM 角度调制\n\n我们会看到 (Equation 14, Equation 15) PM 和 FM 的本质是一样一样的, 这是因为 frequency 和 phase 是密切相关的, 频率是相位的导数, FM 调 \\(e_i\\) 相当于 PM 调 \\(e_i\\) 的积分; PM 调 \\(e_i\\) 相当于 FM 调 \\(e_i\\) 的微分, 所以它们统称为角度调制.\n\n\nLet \\(e(t) = \\cos (\\varphi(t))\\), (e.g., \\(\\varphi(t) = \\omega t + \\theta\\))\n\n称 \\(\\varphi(t)\\) 为 Phase 相位.\n称 \\(\\mathrm{d} \\varphi / \\mathrm{d} t\\) 为 Instantaneous frequency 瞬时频率.\n\nPM 目标: 让 \\(e(t)\\) 与 \\(e_c(t)\\) 的 相位差 正比于 \\(e_i(t)\\) 的幅度: \\[\\varphi(t) = \\omega_c t + \\underbrace{k_p e_t(t)}_{\\Delta \\varphi} \\tag{14}\\]\n\nModulation Index 调制指数: 调制信号 \\(e\\) 的最大相移: \\[\\boxed{m_p := (\\Delta \\varphi)_{\\max}}\\]\n\nFM 目标: 让 \\(e(t)\\) 与 \\(e_c(t)\\) 的 瞬时频率差 正比于 \\(e_i(t)\\) 的幅度: \\[\n  \\begin{aligned}\n  \\frac{\\mathrm{d} \\varphi(t)}{\\mathrm{d} t} &= \\omega_c + \\underbrace{k_f e_t(t)}_{\\Delta \\omega} \\\\\n  \\implies \\varphi(t) &= \\omega_c t + \\underbrace{k_f \\int_{-\\infty}^{t} e_t(\\tau) \\mathrm{d} \\tau}_{\\Delta \\varphi}\n  \\end{aligned}\n   \\tag{15}\\]\n\nModulation Index 调制指数: 信息信号 \\(e_i\\) 最大的频率 反映到调制信号 \\(e\\) 的最大频偏上 缩小了多少: \\[\\boxed{m_f := \\frac{(\\Delta f_e)_{\\max}}{(f_i)_{\\max}}}\\]\n广播 FM 最大频偏默认值: \\[\\boxed{(\\Delta f_e)_{\\max} = 75\\text{ kHz}}\\]\n\nCarson’s Rule 卡森规则 ★★★★★\n\n引入动机: \\(e(t)\\) 的瞬时频率范围 \\(\\neq\\) \\(e(t)\\) 的频谱带宽!!! (\\(e(t)\\) 的带宽比 瞬时频率范围 要宽)\n对于 FM, \\(e\\) 的 非负频率带宽 大致为 \\(m_f\\) 的分子分母加起来 (的两倍): \\[B = 2 \\left((\\Delta f_e)_{\\max} + (f_i)_{\\max} \\right)\\]\n对于 PM, 相当于对 \\(e_i\\) 的导数进行 FM 调制.\nBessel 函数: 可用于定量计算单音信号 \\(e_i = \\cos(\\omega_i t)\\) 的频谱 (注意只是单音信号). 由计算28可知单音信号的 FM 调制波的频谱是离散的, 第 \\(n\\) 个 频点上的幅度正比于 第一类 Bessel 函数的系数 \\(J_n(\\beta)\\).\n\n\n28 Jacobi–Anger 展开: \\[\\cos\\left(\\omega_c t + \\beta \\sin \\omega_m t\\right) = \\sum_{n=-\\infty}^{\\infty} J_n(\\beta) \\cdot \\cos\\left[(\\omega_c + n\\omega_m)\\,t\\right]\\]\n\n8.3 RF 接收机与解调"
  },
  {
    "objectID": "posts/ccd-crash-course/index.html#a-d-模数转换",
    "href": "posts/ccd-crash-course/index.html#a-d-模数转换",
    "title": "CCD 通信电路速成笔记",
    "section": "9 A & D 模数转换 ★★★★★",
    "text": "9 A & D 模数转换 ★★★★★\n\n9.1 Op-amp 运算放大器\n\n运算放大器用了反馈电路来实现放大, 微分, 积分等运算. 这些你都不需要会, 这些反馈最后的结果就是下面的模型, 用他们对付模数转换足够. (思想见 Section 1)\n\n\\[\n\\boxed{\n\\begin{aligned}\nI_{R_1} &= I_{R_2} \\\\\n\\implies \\frac{V_{\\text{in}}}{R_1} &= -\\frac{V_{\\text{out}}}{R_2}\n\\end{aligned}\n}\n\\]\n\n\n\n用跷跷板来思考 Op-amp, \\(+\\) 接地相当于 \\(-\\) 接地\n\n\n\n\n9.2 DAC29\n\n为什么需要 DAC? (数字信号的局限性30)\n\n我们 percept 到的信号是模拟的 (比如声音)\n世界是模拟的, 只有模拟信号才能跟他们 interact (DAC as a bridge)\n数字化的过程中有信息损失\n\n\n30 资产阶级的局限性 (幻视\n电路实现\n\nWeighted Resistor DAC\n\n\n\n\n\n\nFigure 29: Weighted Resistor DAC: 通过调整 \\(R_1\\) 来实现 DAC, 用导纳思考更好, 一般情况下 \\(R_f = R\\)\n\n\n\n\nMotivation: 一个很自然的想法是利用 Op-amp, 通过数字输入 \\(D = [D_3:D_0]\\) 来控制接入电阻的数量, 进而控制 \\(R_1\\) 的大小, 我们期待 \\(D\\) 越大, \\(R_1\\) 越小, 这样 \\(V_{\\text{out}}\\) 就会越大, 即 \\(R_1\\) 的导纳满足: \\[Y_1 \\propto D_0+2D_1+4D_2+8D_3\\]\n\\(Y_1\\) 和 \\(D\\) 的关系: \\[\\boxed{Y_1 = \\frac{1}{R}\\left(\\frac{D_0}{8} + \\frac{D_1}{4} + \\frac{D_2}{2} + D_3\\right)}\\]\n局限性: 电阻随比特数指数级增大, 需要精确的电阻\n\nR-2R Ladder DAC31\n\n\n\nR-2R Ladder DAC: 用电流思考而不是等效电阻 \\(R_1\\), 一般情况下 \\(R_f = R\\)\n\n\n\n这个结构要论证叠加的话别看等效电阻 \\(R_1\\), 看电流, 电流满足叠加定理.\n推导可用 Figure 30 树结构思考. 先只考虑单独接通一个节点 (注意没接通的节点是接地而不是浮空!!) 上面的节点接通电流最大, 从上到下指数减小, 因此上面节点是 \\(D\\) 的高位.\n\\(Y_1\\) 和 \\(D\\) 的关系: \\[\\boxed{Y_1 = \\frac{1}{R}\\left(\\frac{D_0}{16} + \\frac{D_1}{8} + \\frac{D_2}{4} + \\frac{D_3}{2}\\right)}\\]\n\n\n\n31 这个结构真是太精妙了, 很难说它是怎么想出来的.\n\n\n\n\n\n\nFigure 30: 每根线段代表 \\(R\\), 电阻树的简化涉及电阻等效和电源等效, 两个节点同时接入电流满足叠加定理\n\n\n\n\n\n\n\n\n29 我只想说 PPT 这一章可真够啰嗦的."
  }
]